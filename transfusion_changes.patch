diff --git a/.gitignore b/.gitignore
index 912d430..d61ecc8 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,6 +3,8 @@ __pycache__/
 *.py[cod]
 *$py.class
 *.ipynb
+mmdetection-2.11.0/
+mmcv-1.3.10/
 
 # C extensions
 *.so
diff --git a/configs/transfusion_nusc_voxel_LC_adv.py b/configs/transfusion_nusc_voxel_LC_adv.py
new file mode 100644
index 0000000..40d4632
--- /dev/null
+++ b/configs/transfusion_nusc_voxel_LC_adv.py
@@ -0,0 +1,288 @@
+point_cloud_range = [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
+class_names = [
+    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
+    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
+]
+voxel_size = [0.075, 0.075, 0.2]
+out_size_factor = 8
+evaluation = dict(interval=1)
+dataset_type = 'NuScenesDataset'
+data_root = 'data/nuscenes/'
+input_modality = dict(
+    use_lidar=True,
+    use_camera=True,
+    use_radar=False,
+    use_map=False,
+    use_external=False)
+img_scale = (800, 448)
+num_views = 6
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+train_pipeline = [
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='LoadMultiViewImageFromFiles'),
+    # dict(
+    #     type='GlobalRotScaleTrans',
+    #     rot_range=[-0.3925 * 2, 0.3925 * 2],
+    #     scale_ratio_range=[0.9, 1.1],
+    #     translation_std=[0.5, 0.5, 0.5]),
+    # dict(
+    #     type='RandomFlip3D',
+    #     sync_2d=True,
+    #     flip_ratio_bev_horizontal=0.5,
+    #     flip_ratio_bev_vertical=0.5),
+    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectNameFilter', classes=class_names),
+    dict(type='PointShuffle'),
+    dict(type='MyResize', img_scale=img_scale, keep_ratio=True),
+    dict(type='MyNormalize', **img_norm_cfg),
+    dict(type='MyPad', size_divisor=32),
+    dict(type='DefaultFormatBundle3D', class_names=class_names),
+    dict(type='Collect3D', keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
+]
+test_pipeline = [
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='LoadMultiViewImageFromFiles'),
+    dict(
+        type='MultiScaleFlipAug3D',
+        img_scale=img_scale,
+        pts_scale_ratio=1,
+        flip=False,
+        transforms=[
+            dict(
+                type='GlobalRotScaleTrans',
+                rot_range=[0, 0],
+                scale_ratio_range=[1.0, 1.0],
+                translation_std=[0, 0, 0]),
+            dict(type='RandomFlip3D'),
+            dict(type='MyResize', img_scale=img_scale, keep_ratio=True),
+            dict(type='MyNormalize', **img_norm_cfg),
+            dict(type='MyPad', size_divisor=32),
+            dict(
+                type='DefaultFormatBundle3D',
+                class_names=class_names,
+                with_label=False),
+            # dict(type='Collect3D', keys=['points', 'img'])
+            # dict(type='Collect3D', keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d', 'instance_tokens'])
+            dict(type='Collect3D', keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'], 
+                 meta_keys=('filename', 'ori_shape', 'img_shape', 'lidar2img', 'lidar2cam', 'intrinsic',
+                            'pad_shape', 'scale_factor', 'flip',
+                            'pcd_horizontal_flip', 'pcd_vertical_flip',
+                            'box_mode_3d', 'box_type_3d', 'img_norm_cfg',
+                            'rect', 'Trv2c', 'P2', 'pcd_trans', 'sample_idx',
+                            'pcd_scale_factor', 'pcd_rotation', 'pts_filename',
+                            'transformation_3d_flow', 'instance_tokens', 'img_org',
+                            'lidar2ego_rotation',
+                            'lidar2ego_translation',
+                            'ego2global_rotation',
+                            'ego2global_translation',
+                            )
+                 )
+        ])
+]
+data = dict(
+    samples_per_gpu=2,
+    workers_per_gpu=6,
+    train=dict(
+        type='CBGSDataset',
+        dataset=dict(
+            type=dataset_type,
+            data_root=data_root,
+            num_views=num_views,
+            ann_file=data_root + '/nuscenes_infos_train.pkl',
+            load_interval=1,
+            pipeline=train_pipeline,
+            classes=class_names,
+            modality=input_modality,
+            test_mode=False,
+            box_type_3d='LiDAR')),
+    val=dict(
+        type=dataset_type,
+        data_root=data_root,
+        num_views=num_views,
+        ann_file=data_root + '/nuscenes_infos_val.pkl',
+        load_interval=1,
+        pipeline=test_pipeline,
+        classes=class_names,
+        modality=input_modality,
+        test_mode=True,
+        box_type_3d='LiDAR'),
+    test=dict(
+        type=dataset_type,
+        data_root=data_root,
+        num_views=num_views,
+        ann_file=data_root + '/nuscenes_infos_val.pkl',
+        load_interval=1,
+        pipeline=test_pipeline,
+        classes=class_names,
+        modality=input_modality,
+        test_mode=True,
+        box_type_3d='LiDAR'))
+model = dict(
+    type='TransFusionDetector',
+    freeze_img=True,
+    # img_backbone=dict(
+    #     type='DLASeg',
+    #     num_layers=34,
+    #     heads={},
+    #     head_convs=-1,
+    #     ),
+    img_backbone=dict(
+        type='ResNet',
+        depth=50,
+        num_stages=4,
+        out_indices=(0, 1, 2, 3),
+        frozen_stages=1,
+        norm_cfg=dict(type='BN', requires_grad=True),
+        norm_eval=True,
+        style='pytorch'),
+    img_neck=dict(
+        type='FPN',
+        in_channels=[256, 512, 1024, 2048],
+        out_channels=256,
+        num_outs=5),
+    pts_voxel_layer=dict(
+        max_num_points=10,
+        voxel_size=voxel_size,
+        max_voxels=(120000, 160000),
+        point_cloud_range=point_cloud_range),
+    pts_voxel_encoder=dict(
+        type='HardSimpleVFE',
+        num_features=5,
+    ),
+    pts_middle_encoder=dict(
+        type='SparseEncoder',
+        in_channels=5,
+        sparse_shape=[41, 1440, 1440],
+        output_channels=128,
+        order=('conv', 'norm', 'act'),
+        encoder_channels=((16, 16, 32), (32, 32, 64), (64, 64, 128), (128, 128)),
+        encoder_paddings=((0, 0, 1), (0, 0, 1), (0, 0, [0, 1, 1]), (0, 0)),
+        block_type='basicblock'),
+    pts_backbone=dict(
+        type='SECOND',
+        in_channels=256,
+        out_channels=[128, 256],
+        layer_nums=[5, 5],
+        layer_strides=[1, 2],
+        norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
+        conv_cfg=dict(type='Conv2d', bias=False)),
+    pts_neck=dict(
+        type='SECONDFPN',
+        in_channels=[128, 256],
+        out_channels=[256, 256],
+        upsample_strides=[1, 2],
+        norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
+        upsample_cfg=dict(type='deconv', bias=False),
+        use_conv_for_no_stride=True),
+    pts_bbox_head=dict(
+        type='TransFusionHead',
+        fuse_img=True,
+        num_views=num_views,
+        in_channels_img=256,
+        out_size_factor_img=4,
+        num_proposals=200,
+        auxiliary=True,
+        in_channels=256 * 2,
+        hidden_channel=128,
+        num_classes=len(class_names),
+        num_decoder_layers=1,
+        num_heads=8,
+        learnable_query_pos=False,
+        initialize_by_heatmap=True,
+        nms_kernel_size=3,
+        ffn_channel=256,
+        dropout=0.1,
+        bn_momentum=0.1,
+        activation='relu',
+        common_heads=dict(center=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
+        bbox_coder=dict(
+            type='TransFusionBBoxCoder',
+            pc_range=point_cloud_range[:2],
+            voxel_size=voxel_size[:2],
+            out_size_factor=out_size_factor,
+            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
+            score_threshold=0.0,
+            code_size=10,
+        ),
+        loss_cls=dict(type='FocalLoss', use_sigmoid=True, gamma=2, alpha=0.25, reduction='mean', loss_weight=1.0),
+        # loss_iou=dict(type='CrossEntropyLoss', use_sigmoid=True, reduction='mean', loss_weight=0.0),
+        loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
+        loss_heatmap=dict(type='GaussianFocalLoss', reduction='mean', loss_weight=1.0),
+    ),
+    train_cfg=dict(
+        pts=dict(
+            dataset='nuScenes',
+            assigner=dict(
+                type='HungarianAssigner3D',
+                iou_calculator=dict(type='BboxOverlaps3D', coordinate='lidar'),
+                cls_cost=dict(type='FocalLossCost', gamma=2, alpha=0.25, weight=0.15),
+                reg_cost=dict(type='BBoxBEVL1Cost', weight=0.25),
+                iou_cost=dict(type='IoU3DCost', weight=0.25)
+            ),
+            pos_weight=-1,
+            gaussian_overlap=0.1,
+            min_radius=2,
+            grid_size=[1440, 1440, 40],  # [x_len, y_len, 1]
+            voxel_size=voxel_size,
+            out_size_factor=out_size_factor,
+            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
+            point_cloud_range=point_cloud_range)),
+    test_cfg=dict(
+        pts=dict(
+            dataset='nuScenes',
+            grid_size=[1440, 1440, 40],
+            out_size_factor=out_size_factor,
+            pc_range=point_cloud_range[0:2],
+            voxel_size=voxel_size[:2],
+            nms_type=None,
+        )))
+optimizer = dict(type='AdamW', lr=0.0001, weight_decay=0.01)  # for 8gpu * 2sample_per_gpu
+optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))
+lr_config = dict(
+    policy='cyclic',
+    target_ratio=(10, 0.0001),
+    cyclic_times=1,
+    step_ratio_up=0.4)
+momentum_config = dict(
+    policy='cyclic',
+    target_ratio=(0.8947368421052632, 1),
+    cyclic_times=1,
+    step_ratio_up=0.4)
+total_epochs = 6
+checkpoint_config = dict(interval=1)
+log_config = dict(
+    interval=50,
+    hooks=[dict(type='TextLoggerHook'),
+           dict(type='TensorboardLoggerHook')])
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = None
+load_from = 'checkpoints/fusion_voxel0075_R50.pth'
+resume_from = None
+workflow = [('train', 1)]
+gpu_ids = range(0, 8)
+freeze_lidar_components = True
+find_unused_parameters = True
diff --git a/configs/transfusion_nusc_voxel_LC_instancetoken.py b/configs/transfusion_nusc_voxel_LC_instancetoken.py
new file mode 100644
index 0000000..a571e96
--- /dev/null
+++ b/configs/transfusion_nusc_voxel_LC_instancetoken.py
@@ -0,0 +1,288 @@
+point_cloud_range = [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
+class_names = [
+    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
+    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
+]
+voxel_size = [0.075, 0.075, 0.2]
+out_size_factor = 8
+evaluation = dict(interval=1)
+dataset_type = 'NuScenesDatasetInstanceToken'
+data_root = 'data/nuscenes/'
+input_modality = dict(
+    use_lidar=True,
+    use_camera=True,
+    use_radar=False,
+    use_map=False,
+    use_external=False)
+img_scale = (800, 448)
+num_views = 6
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
+train_pipeline = [
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='LoadMultiViewImageFromFiles'),
+    # dict(
+    #     type='GlobalRotScaleTrans',
+    #     rot_range=[-0.3925 * 2, 0.3925 * 2],
+    #     scale_ratio_range=[0.9, 1.1],
+    #     translation_std=[0.5, 0.5, 0.5]),
+    # dict(
+    #     type='RandomFlip3D',
+    #     sync_2d=True,
+    #     flip_ratio_bev_horizontal=0.5,
+    #     flip_ratio_bev_vertical=0.5),
+    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectNameFilter', classes=class_names),
+    dict(type='PointShuffle'),
+    dict(type='MyResize', img_scale=img_scale, keep_ratio=True),
+    dict(type='MyNormalize', **img_norm_cfg),
+    dict(type='MyPad', size_divisor=32),
+    dict(type='DefaultFormatBundle3D', class_names=class_names),
+    dict(type='Collect3D', keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
+]
+test_pipeline = [
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        use_dim=[0, 1, 2, 3, 4],
+    ),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='LoadMultiViewImageFromFiles'),
+    dict(
+        type='MultiScaleFlipAug3D',
+        img_scale=img_scale,
+        pts_scale_ratio=1,
+        flip=False,
+        transforms=[
+            dict(
+                type='GlobalRotScaleTrans',
+                rot_range=[0, 0],
+                scale_ratio_range=[1.0, 1.0],
+                translation_std=[0, 0, 0]),
+            dict(type='RandomFlip3D'),
+            dict(type='MyResize', img_scale=img_scale, keep_ratio=True),
+            dict(type='MyNormalize', **img_norm_cfg),
+            dict(type='MyPad', size_divisor=32),
+            dict(
+                type='DefaultFormatBundle3D',
+                class_names=class_names,
+                with_label=False),
+            # dict(type='Collect3D', keys=['points', 'img'])
+            # dict(type='Collect3D', keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d', 'instance_tokens'])
+            dict(type='Collect3D', keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'], 
+                 meta_keys=('filename', 'ori_shape', 'img_shape', 'lidar2img', 'lidar2cam', 'intrinsic',
+                            'pad_shape', 'scale_factor', 'flip',
+                            'pcd_horizontal_flip', 'pcd_vertical_flip',
+                            'box_mode_3d', 'box_type_3d', 'img_norm_cfg',
+                            'rect', 'Trv2c', 'P2', 'pcd_trans', 'sample_idx',
+                            'pcd_scale_factor', 'pcd_rotation', 'pts_filename',
+                            'transformation_3d_flow', 'instance_tokens', 'img_org',
+                            'lidar2ego_rotation',
+                            'lidar2ego_translation',
+                            'ego2global_rotation',
+                            'ego2global_translation',
+                            )
+                 )
+        ])
+]
+data = dict(
+    samples_per_gpu=2,
+    workers_per_gpu=6,
+    train=dict(
+        type='CBGSDataset',
+        dataset=dict(
+            type=dataset_type,
+            data_root=data_root,
+            num_views=num_views,
+            ann_file=data_root + '/nuscenes_infos_train.pkl',
+            load_interval=1,
+            pipeline=train_pipeline,
+            classes=class_names,
+            modality=input_modality,
+            test_mode=False,
+            box_type_3d='LiDAR')),
+    val=dict(
+        type=dataset_type,
+        data_root=data_root,
+        num_views=num_views,
+        ann_file=data_root + '/nuscenes_infos_val.pkl',
+        load_interval=1,
+        pipeline=test_pipeline,
+        classes=class_names,
+        modality=input_modality,
+        test_mode=True,
+        box_type_3d='LiDAR'),
+    test=dict(
+        type=dataset_type,
+        data_root=data_root,
+        num_views=num_views,
+        ann_file=data_root + '/nuscenes_infos_val.pkl',
+        load_interval=1,
+        pipeline=test_pipeline,
+        classes=class_names,
+        modality=input_modality,
+        test_mode=True,
+        box_type_3d='LiDAR'))
+model = dict(
+    type='TransFusionDetector',
+    freeze_img=True,
+    # img_backbone=dict(
+    #     type='DLASeg',
+    #     num_layers=34,
+    #     heads={},
+    #     head_convs=-1,
+    #     ),
+    img_backbone=dict(
+        type='ResNet',
+        depth=50,
+        num_stages=4,
+        out_indices=(0, 1, 2, 3),
+        frozen_stages=1,
+        norm_cfg=dict(type='BN', requires_grad=True),
+        norm_eval=True,
+        style='pytorch'),
+    img_neck=dict(
+        type='FPN',
+        in_channels=[256, 512, 1024, 2048],
+        out_channels=256,
+        num_outs=5),
+    pts_voxel_layer=dict(
+        max_num_points=10,
+        voxel_size=voxel_size,
+        max_voxels=(120000, 160000),
+        point_cloud_range=point_cloud_range),
+    pts_voxel_encoder=dict(
+        type='HardSimpleVFE',
+        num_features=5,
+    ),
+    pts_middle_encoder=dict(
+        type='SparseEncoder',
+        in_channels=5,
+        sparse_shape=[41, 1440, 1440],
+        output_channels=128,
+        order=('conv', 'norm', 'act'),
+        encoder_channels=((16, 16, 32), (32, 32, 64), (64, 64, 128), (128, 128)),
+        encoder_paddings=((0, 0, 1), (0, 0, 1), (0, 0, [0, 1, 1]), (0, 0)),
+        block_type='basicblock'),
+    pts_backbone=dict(
+        type='SECOND',
+        in_channels=256,
+        out_channels=[128, 256],
+        layer_nums=[5, 5],
+        layer_strides=[1, 2],
+        norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
+        conv_cfg=dict(type='Conv2d', bias=False)),
+    pts_neck=dict(
+        type='SECONDFPN',
+        in_channels=[128, 256],
+        out_channels=[256, 256],
+        upsample_strides=[1, 2],
+        norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
+        upsample_cfg=dict(type='deconv', bias=False),
+        use_conv_for_no_stride=True),
+    pts_bbox_head=dict(
+        type='TransFusionHead',
+        fuse_img=True,
+        num_views=num_views,
+        in_channels_img=256,
+        out_size_factor_img=4,
+        num_proposals=200,
+        auxiliary=True,
+        in_channels=256 * 2,
+        hidden_channel=128,
+        num_classes=len(class_names),
+        num_decoder_layers=1,
+        num_heads=8,
+        learnable_query_pos=False,
+        initialize_by_heatmap=True,
+        nms_kernel_size=3,
+        ffn_channel=256,
+        dropout=0.1,
+        bn_momentum=0.1,
+        activation='relu',
+        common_heads=dict(center=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
+        bbox_coder=dict(
+            type='TransFusionBBoxCoder',
+            pc_range=point_cloud_range[:2],
+            voxel_size=voxel_size[:2],
+            out_size_factor=out_size_factor,
+            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
+            score_threshold=0.0,
+            code_size=10,
+        ),
+        loss_cls=dict(type='FocalLoss', use_sigmoid=True, gamma=2, alpha=0.25, reduction='mean', loss_weight=1.0),
+        # loss_iou=dict(type='CrossEntropyLoss', use_sigmoid=True, reduction='mean', loss_weight=0.0),
+        loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
+        loss_heatmap=dict(type='GaussianFocalLoss', reduction='mean', loss_weight=1.0),
+    ),
+    train_cfg=dict(
+        pts=dict(
+            dataset='nuScenes',
+            assigner=dict(
+                type='HungarianAssigner3D',
+                iou_calculator=dict(type='BboxOverlaps3D', coordinate='lidar'),
+                cls_cost=dict(type='FocalLossCost', gamma=2, alpha=0.25, weight=0.15),
+                reg_cost=dict(type='BBoxBEVL1Cost', weight=0.25),
+                iou_cost=dict(type='IoU3DCost', weight=0.25)
+            ),
+            pos_weight=-1,
+            gaussian_overlap=0.1,
+            min_radius=2,
+            grid_size=[1440, 1440, 40],  # [x_len, y_len, 1]
+            voxel_size=voxel_size,
+            out_size_factor=out_size_factor,
+            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
+            point_cloud_range=point_cloud_range)),
+    test_cfg=dict(
+        pts=dict(
+            dataset='nuScenes',
+            grid_size=[1440, 1440, 40],
+            out_size_factor=out_size_factor,
+            pc_range=point_cloud_range[0:2],
+            voxel_size=voxel_size[:2],
+            nms_type=None,
+        )))
+optimizer = dict(type='AdamW', lr=0.0001, weight_decay=0.01)  # for 8gpu * 2sample_per_gpu
+optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))
+lr_config = dict(
+    policy='cyclic',
+    target_ratio=(10, 0.0001),
+    cyclic_times=1,
+    step_ratio_up=0.4)
+momentum_config = dict(
+    policy='cyclic',
+    target_ratio=(0.8947368421052632, 1),
+    cyclic_times=1,
+    step_ratio_up=0.4)
+total_epochs = 6
+checkpoint_config = dict(interval=1)
+log_config = dict(
+    interval=50,
+    hooks=[dict(type='TextLoggerHook'),
+           dict(type='TensorboardLoggerHook')])
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = None
+load_from = 'checkpoints/fusion_voxel0075_R50.pth'
+resume_from = None
+workflow = [('train', 1)]
+gpu_ids = range(0, 8)
+freeze_lidar_components = True
+find_unused_parameters = True
diff --git a/extend/__init__.py b/extend/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/extend/custom_func.py b/extend/custom_func.py
new file mode 100644
index 0000000..9d22daa
--- /dev/null
+++ b/extend/custom_func.py
@@ -0,0 +1,95 @@
+# for transfusion
+import torch
+import torchvision
+import torchvision.transforms as transforms
+import torch.nn.functional as F
+from torchvision.utils import save_image
+import copy
+
+def custom_data_preprocess(data):
+    for _key in data:
+        data[_key] = data[_key][0]
+    return data
+
+def custom_data_postprocess_eval(data):
+    data.pop('gt_bboxes_3d', None)
+    data.pop('gt_labels_3d', None)
+    for _key in data:
+        data[_key] = [data[_key]]
+    return data
+
+def custom_data_work(data):
+    metas = data['img_metas']._data[0][0]
+    img_path_list = metas['filename']
+    img_org_np = metas['img_org']
+    img_processed = data['img']._data[0].clone()
+    gt_labels_3d = data['gt_labels_3d']._data[0][0]
+    return metas, img_path_list, img_org_np, img_processed, gt_labels_3d
+
+def custom_data_work_point(data):
+    metas = data['img_metas']._data[0][0]
+    img_path_list = metas['filename']
+    img_org_np = metas['img_org']
+    img_processed = data['img']._data[0].clone()
+    gt_labels_3d = data['gt_labels_3d']._data[0][0]
+    points_tensor = data['points']._data[0][0].clone()
+    return metas, img_path_list, img_org_np, img_processed, gt_labels_3d, points_tensor
+
+def custom_result_postprocess(result):
+    return result
+    
+
+def custom_img_read_from_img_org(img_org_np, device):
+    img_org_np_255_bgr_hwcn_uint8 = img_org_np # mmcv 读取 BGR 转 numpy
+    img_org_tensor_bgr_255_hwcn = torch.from_numpy(img_org_np_255_bgr_hwcn_uint8).float()
+    img_org_tensor_bgr_255 = img_org_tensor_bgr_255_hwcn.permute(3,2,0,1)
+    img_org_tensor_bgr = (img_org_tensor_bgr_255/255.).to(device) # 6chw
+    img_org_tensor_rgb = img_org_tensor_bgr[:,[2,1,0]]
+    img_tensor_rgb_6chw_0to1 = img_org_tensor_rgb
+    return img_tensor_rgb_6chw_0to1
+
+
+def custom_differentiable_transform(img_tensor_rgb_6chw_0to1, img_metas):
+    """Alternative Data Preparation for Original Model
+
+    Args:
+        img_tensor (torch.tensor): (6xCxHxW), tensors of original imgs 
+    """
+
+    assert len(img_tensor_rgb_6chw_0to1.shape) == 4
+    assert img_tensor_rgb_6chw_0to1.shape[0] == 6
+    assert img_tensor_rgb_6chw_0to1.shape[1] == 3
+    assert img_tensor_rgb_6chw_0to1.max() <= 1.
+    assert img_tensor_rgb_6chw_0to1.min() >= 0.
+    assert img_tensor_rgb_6chw_0to1.dtype == torch.float32
+    assert img_tensor_rgb_6chw_0to1.is_cuda
+
+    img_tensor_255 = img_tensor_rgb_6chw_0to1 * 255.
+
+    device = img_tensor_rgb_6chw_0to1.device
+    mean=[123.675, 116.28, 103.53]
+    std=[58.395, 57.12, 57.375]
+    mean = torch.tensor(mean).to(device)[None,None,:,None,None]
+    std = torch.tensor(std).to(device)[None,None,:,None,None]
+
+    ############ resize norm pad
+    ######## resize
+    img_tensor_255_resize = F.interpolate(img_tensor_255, (448, 796), mode='bilinear', align_corners=False)
+    ######## norm
+    img_tensor_norm = (img_tensor_255_resize - mean)/std
+    ######## pad
+    img_tensor_pad = img_tensor_norm.new_zeros(1,6,3,448,800)
+    img_tensor_pad[:,:,:,:,:img_tensor_norm.shape[4]] = img_tensor_norm
+
+    return img_tensor_pad
+
+def custom_image_data_give(data, image_ready):
+    data_copy = copy.deepcopy(data)
+    data_copy['img']._data[0] = image_ready
+    return data_copy
+
+def custom_image_data_give_point(data, image_ready, points_ready):
+    data_copy = copy.deepcopy(data)
+    data_copy['img']._data[0] = image_ready
+    data_copy['points']._data[0][0] = points_ready
+    return data_copy
diff --git a/extend/func_for_patchinfo.py b/extend/func_for_patchinfo.py
new file mode 100644
index 0000000..07be482
--- /dev/null
+++ b/extend/func_for_patchinfo.py
@@ -0,0 +1,1127 @@
+# useful function
+import torch
+import numpy as np
+import cv2
+from tqdm import tqdm
+
+def in_image_by_size(coor_uvd, img_org_shape):
+    coor_u = coor_uvd[:,0]
+    coor_v = coor_uvd[:,1]
+    h, w = img_org_shape
+    return (coor_u > 0) * (coor_u < w) * (coor_v > 0) * (coor_v < h)
+
+def in_image_by_depth(coor_uvd, img_org_shape):
+    return coor_uvd[:,2] > 0 
+
+
+def get_on_which_image(obj_idx, on_the_image):
+    i = on_the_image.t()[obj_idx].nonzero()[:,0]
+    return i
+
+
+
+
+# cross x1y2-x2y1
+def cross_mul(a, b):
+    # a N*2
+    # b M*2
+    if len(a.shape) == 1:
+        a = a[None]
+    if len(b.shape) == 1:
+        b = b[None]
+    # out N*M
+    # a > N*1*2
+    # b > 1*M*2
+    a = a[:,None]
+    b = b[None]
+    return a[...,0]*b[...,1] - b[...,0]*a[...,1]
+
+def judge_inout_triangle(
+        left_end,
+        right_end,
+        points,
+    ):
+    
+    left_vector = torch.Tensor(left_end)
+    right_vector = torch.Tensor(right_end)
+    
+    
+    right_crossmult_points = cross_mul(right_vector, points)[0]
+    points_crossmult_left  = cross_mul(points,  left_vector)[:,0]
+    
+    points_in_flag = (right_crossmult_points>0) * (points_crossmult_left>0)
+        
+    return points_in_flag
+
+
+def point_in_line_segment(point, lineseg):
+    vec_0 = point - lineseg[0]
+    vec_1 = point - lineseg[1]
+    if (vec_0*vec_1).sum()<0:
+        return True
+    else:
+        return False
+
+def get_ABC(xyxy):
+    # Ax + By + C = 0
+    x1 = xyxy[0][0]
+    y1 = xyxy[0][1]
+    x2 = xyxy[1][0]
+    y2 = xyxy[1][1]
+    A = y2-y1
+    B = x1-x2
+    C = x2*y1-x1*y2
+    return A, B, C
+
+# 给定2*2个点，算相直线交点位置
+def lines_intersection_by_2x2points(
+    line1_xyxy,
+    line2_xyxy,
+):
+    line1_xyxy = torch.Tensor(line1_xyxy)
+    line2_xyxy = torch.Tensor(line2_xyxy)
+    A1, B1, C1 = get_ABC(line1_xyxy)
+    A2, B2, C2 = get_ABC(line2_xyxy)
+    '''
+        X_ = A_.inverse() @ B_
+        A_ @ X_ = B_
+        [ A1 B1 ] [x] = [ -C1 ]
+        [ A2 B2 ] [y] = [ -C2 ]
+    '''
+    A_ = torch.Tensor([
+        [A1,B1],
+        [A2,B2],
+    ])
+    B_ = torch.Tensor([
+        [-C1],
+        [-C2],
+    ])
+    
+    
+    valid_intersection = False
+    
+    try:
+        X_ = A_.inverse() @ B_
+        line_parallel = False
+    except:
+        # no intersection
+        line_parallel = True
+        valid_intersection = False
+        X_ = -1
+    
+    if not line_parallel:
+        # 判断 交点 是否在4个点的2个线段之内，而不是在延长线上。
+        in_seg1 = point_in_line_segment(X_, line1_xyxy)
+        in_seg2 = point_in_line_segment(X_, line2_xyxy)
+        valid_intersection = in_seg1*in_seg2
+
+    return X_, valid_intersection
+
+def get_neighbor_index(in_index, total_num):
+    assert total_num > 3
+    assert in_index < total_num
+    if in_index == 0:
+        return torch.IntTensor([1, total_num-1]) # first+1 then-1
+    elif in_index == total_num-1:
+        return torch.IntTensor([0, total_num-2])
+    else:
+        return torch.IntTensor([in_index+1, in_index-1])
+
+
+
+def get_inrate_by_cv2_depth(
+        fake_depth,
+        cv2_depth,
+        image_corners_4d_cam,
+        box_4corner,
+        ):
+    map_fov = np.zeros((cv2_depth, 2*cv2_depth), np.uint8)
+    left_end_index  = [image_corners_4d_cam[0,0]*cv2_depth/fake_depth + cv2_depth, image_corners_4d_cam[0,2]*cv2_depth/fake_depth]
+    right_end_index = [image_corners_4d_cam[2,0]*cv2_depth/fake_depth + cv2_depth, image_corners_4d_cam[2,2]*cv2_depth/fake_depth]
+    map_fov_3corner = np.array([[cv2_depth, 0], left_end_index, right_end_index], np.int32)
+    cv2.fillPoly(map_fov, [map_fov_3corner], (255), cv2.LINE_AA)
+    map_box = np.zeros((cv2_depth, 2*cv2_depth), np.uint8)
+    box_4corner_index = [
+        [box_4corner[0,0]*cv2_depth/fake_depth+cv2_depth, box_4corner[0,1]*cv2_depth/fake_depth],
+        [box_4corner[1,0]*cv2_depth/fake_depth+cv2_depth, box_4corner[1,1]*cv2_depth/fake_depth],
+        [box_4corner[2,0]*cv2_depth/fake_depth+cv2_depth, box_4corner[2,1]*cv2_depth/fake_depth],
+        [box_4corner[3,0]*cv2_depth/fake_depth+cv2_depth, box_4corner[3,1]*cv2_depth/fake_depth],
+        ]
+    map_box_4corner = np.array(box_4corner_index, np.int32)
+    cv2.fillPoly(map_box, [map_box_4corner], (255), cv2.LINE_AA)
+    # 判断 obj 是否在相机正面（否则压根不画图）
+    # 判断 obj 在相机fov内的，以及在内的比例
+    map_fov = map_fov.astype(np.float32) / 255.
+    map_box = map_box.astype(np.float32) / 255.
+    map_in_overlap = map_box * map_fov
+    map_out_overlap = map_box * (1 - map_fov)
+    in_pix = map_in_overlap.sum()
+    out_pix = map_out_overlap.sum()
+    show_img = ((map_box+map_fov)/2*255).astype(np.uint8)[::-1]
+    
+    if in_pix + out_pix == 0:
+        return 0., show_img
+    else:
+        return in_pix / (in_pix + out_pix), show_img
+
+
+def get_fov_intersection_rate(points_3d_batch, lidar2img, lidar2cam, intrinsic, img_org_shape):
+    points_4d_batch = torch.cat([points_3d_batch, torch.ones_like(points_3d_batch[:,0:1])], dim=-1)
+    # points_2d_batch_1 = points_4d_batch @ lidar2img.t()
+    # points_2d_batch_2 = points_4d_batch @ lidar2cam @ intrinsic.t()
+    points_4d_cam_batch = points_4d_batch @ lidar2cam
+    
+    points_depth = points_4d_cam_batch[:,2]
+    
+    # 函数目的，计算box在当前镜头fov内的比例。
+    
+    if (points_depth<0).all():
+        # 所有点都在相机后面，必然为0
+        return 0.
+        
+    # 搞实验，图像画面左上右下角，分别对应哪里呢？
+    
+    image_corners = torch.Tensor([
+        [0, 0],
+        [0, img_org_shape[0]],
+        [img_org_shape[1], 0],
+        [img_org_shape[1], img_org_shape[0]],
+        ])
+    
+    fake_depth = 100
+    image_corners_uvd = torch.cat([image_corners*fake_depth, torch.zeros_like(image_corners[:,0:1]).fill_(fake_depth)], dim=1)
+    image_corners_uvd1_4d = torch.cat([image_corners_uvd, torch.ones_like(image_corners_uvd[:,0:1])], dim=-1)
+    image_corners_4d_cam = image_corners_uvd1_4d @ torch.linalg.inv(intrinsic.t())
+    '''
+    camera coordinate (BEV)
+      left      ^ z     right
+        --------+---------
+        \       |       /
+         \      |      /
+          \     |     /
+           \    |    /
+            \   |   /
+             \  |  /
+              \ | /
+              o +--------------> x
+    '''
+    left_end = [image_corners_4d_cam[0,0], image_corners_4d_cam[0,2]]
+    right_end = [image_corners_4d_cam[2,0], image_corners_4d_cam[2,2]]
+    
+    '''
+        use map area to compute the overlap area,
+        resolution = 3000 / 60m
+        resolution = 2000 / 80m
+        这个方法可以高低分辨率搭配，这样效率高还准确
+        
+        低分辨率：
+        500 / 100m
+        高分辨率
+        5000 / 100m
+    '''
+    
+    # '''
+    # planA: 使用点坐标进行比例计算
+    #     首先约定如何表示点和线，
+    #         点坐标就用xoz坐标就好
+    #         线：参考torch的表示方式
+    #             A*x + B*y + C = 0
+    #             A= y2 - y1, B= x1 - x2
+    #             C = x2*y1 - x1*y2
+        
+    #     2.python求两个直线交点其实就是解二元一次方程组
+    #         A * X = B
+    #     torch.linalg.solve(A, B, *, left=True, out=None)
+    #     或者直接使用 X = A.inverse() @ B
+    
+    # '''
+    
+    # box_4corner = points_4d_cam_batch[[0,3,7,4]][:,[0,2]]
+    # # 判断点在直线的哪一侧？使用叉乘！
+    # # 因为直线过原点且有方向
+    
+    # points_in_flag = judge_inout_triangle(
+    #     left_end,
+    #     right_end,
+    #     box_4corner,
+    # )
+    
+    # points_in_num = points_in_flag.sum()
+    
+    # if points_in_num == 0:
+    #     return 0.
+    # elif points_in_num == 1:
+    #     # 1个点在内部，
+    #     point_index = torch.nonzero(points_in_flag)[:,0]
+    #     neighbor_index = get_neighbor_index(point_index, len(points_in_flag))
+    #     neighbor_points = box_4corner[neighbor_index]
+        
+        
+    #     fov_line_left_points  = [[0,0], left_end]
+    #     fov_line_right_points = [[0,0], right_end]
+
+        
+    
+    #     # 注意求交点可能求不出来，if 直线平行
+    #     intersection_xy_l0, valid_l0 = lines_intersection_by_2x2points(fov_line_left_points, neighbor_points[0])
+    #     intersection_xy_l1, valid_l1 = lines_intersection_by_2x2points(fov_line_left_points, neighbor_points[1])
+    
+    #     intersection_xy_r0, valid_r0 = lines_intersection_by_2x2points(fov_line_right_points, neighbor_points[0])
+    #     intersection_xy_r1, valid_r1 = lines_intersection_by_2x2points(fov_line_right_points, neighbor_points[1])
+
+
+    #     if 
+    '''
+    planB: 使用面积进行比例计算，过于慢了
+    '''
+    cv2_depth_coarse = 500
+    cv2_depth_fine   = 2500
+    box_4corner = points_4d_cam_batch[[0,3,7,4]][:,[0,2]]
+    in_div_all_coarse, show1 = get_inrate_by_cv2_depth(
+        fake_depth,
+        cv2_depth_coarse,
+        image_corners_4d_cam,
+        box_4corner
+        )
+    
+    if in_div_all_coarse != 1 and in_div_all_coarse != 0:
+        in_div_all_fine, show2 = get_inrate_by_cv2_depth(
+            fake_depth,
+            cv2_depth_fine,
+            image_corners_4d_cam,
+            box_4corner
+            )
+
+        in_div_all_final = float(in_div_all_fine)
+        # if in_div_all_final > 0.9 or in_div_all_final < 0.1:
+        #     cv2.imwrite('1.png', show1)
+        #     cv2.imwrite('2.png', show2)
+
+    else:
+        in_div_all_final = float(in_div_all_coarse)
+    
+    return in_div_all_final
+
+
+def proj_3d_to_2d(points_3d_batch, lidar2img, img_org_shape):
+    points_4d_batch = torch.cat([points_3d_batch, torch.ones_like(points_3d_batch[:,0:1])], dim=-1)
+    points_2d_batch = points_4d_batch @ lidar2img.t()
+    points_2d_batch_depth = points_2d_batch[:,2]
+    
+    points_2d_batch_depth_no_zero = torch.where(
+        points_2d_batch_depth!=0,
+        points_2d_batch_depth,
+        torch.ones_like(points_2d_batch_depth).fill_(1e-5)
+    )
+    
+    points_2d_batch[:,0] /= points_2d_batch_depth_no_zero
+    points_2d_batch[:,1] /= points_2d_batch_depth_no_zero
+    on_the_image_depth = points_2d_batch_depth > 0
+    coor_x, coor_y = points_2d_batch[:,0], points_2d_batch[:,1]
+    h, w = img_org_shape
+    on_the_image_size = (coor_x > 0) * (coor_x < w) * (coor_y > 0) * (coor_y < h)
+    return points_2d_batch[:,:3], on_the_image_depth, on_the_image_size
+
+
+
+# 额外函数
+# 计算2dbbox大小 裁切边缘
+def get_bbox_2d_area_cutonedge(gt_bboxes_corners_j, lidar2img_i, img_org_shape):
+    gt_bboxes_corners_2d_uvd_ij, _, _ = proj_3d_to_2d(gt_bboxes_corners_j, lidar2img_i, img_org_shape)
+    h, w = img_org_shape
+    gt_bboxes_corners_2d_ij_x_max = gt_bboxes_corners_2d_uvd_ij[:,0].max().clamp(0,w)
+    gt_bboxes_corners_2d_ij_x_min = gt_bboxes_corners_2d_uvd_ij[:,0].min().clamp(0,w)
+    gt_bboxes_corners_2d_ij_y_max = gt_bboxes_corners_2d_uvd_ij[:,1].max().clamp(0,h)
+    gt_bboxes_corners_2d_ij_y_min = gt_bboxes_corners_2d_uvd_ij[:,1].min().clamp(0,h)
+    bbox_2d_area = (gt_bboxes_corners_2d_ij_x_max - gt_bboxes_corners_2d_ij_x_min) \
+        * (gt_bboxes_corners_2d_ij_y_max - gt_bboxes_corners_2d_ij_y_min)
+    return bbox_2d_area
+
+# 计算2dbbox大小 无裁切
+def get_bbox_2d_area_nocut(gt_bboxes_corners_j, lidar2img_i, img_org_shape):
+    gt_bboxes_corners_2d_uvd_ij, _, _ = proj_3d_to_2d(gt_bboxes_corners_j, lidar2img_i, img_org_shape)
+    gt_bboxes_corners_2d_ij_x_max = gt_bboxes_corners_2d_uvd_ij[:,0].max()
+    gt_bboxes_corners_2d_ij_x_min = gt_bboxes_corners_2d_uvd_ij[:,0].min()
+    gt_bboxes_corners_2d_ij_y_max = gt_bboxes_corners_2d_uvd_ij[:,1].max()
+    gt_bboxes_corners_2d_ij_y_min = gt_bboxes_corners_2d_uvd_ij[:,1].min()
+    bbox_2d_area = (gt_bboxes_corners_2d_ij_x_max - gt_bboxes_corners_2d_ij_x_min) \
+        * (gt_bboxes_corners_2d_ij_y_max - gt_bboxes_corners_2d_ij_y_min)
+    return bbox_2d_area
+
+
+# 函数，根据四个顶点计算patch大小
+def get_area_from_4corner(ordered_points):
+    # 计算任意多边形的面积，顶点按照顺时针或者逆时针方向排列
+    return compute_polygon_area(ordered_points)
+
+# 函数，根据逆时针或者顺时针排列的顶点，计算多边形的面积
+def compute_polygon_area(points):
+    # from  https://blog.csdn.net/u012939880/article/details/88193763
+    point_num = len(points)
+    if(point_num < 3):
+        return 0.0
+    s = points[0][1] * (points[point_num-1][0] - points[1][0])
+    # for i in range(point_num): # (int i = 1 i < point_num ++i):
+    for i in range(1, point_num):  # 有小伙伴发现一个bug，这里做了修改，但是没有测试，需要使用的亲请测试下，以免结果不正确。
+        s += points[i][1] * (points[i-1][0] - points[(i+1) % point_num][0])
+    return abs(s/2.0)
+
+# 计算垂直向量
+def get_orthogonal(array):
+    return torch.tensor([-array[1], array[0]])
+
+def get_area_from_gt_center_array_and_patch_size_3d(
+    orthogonal_norm,
+    patch_size_3d,
+    gt_bboxes_center_array_shorter,
+    lidar2img_i,
+    img_org_shape,
+    ):    
+    virtul_points_2d = get_virtul_points_2d_from_gt_center_array_and_patch_size_3d(
+        orthogonal_norm,
+        patch_size_3d,
+        gt_bboxes_center_array_shorter,
+        lidar2img_i,
+        img_org_shape,
+    )
+    now_area = get_area_from_4corner(virtul_points_2d)
+    return now_area
+
+def get_virtul_points_2d_from_gt_center_array_and_patch_size_3d(
+    orthogonal_norm,
+    patch_size_3d,
+    gt_bboxes_center_array_shorter,
+    lidar2img_i,
+    img_org_shape
+    ):
+    offset_horizonal = torch.cat([orthogonal_norm, orthogonal_norm.new_zeros(1)]) \
+                        * patch_size_3d / 2
+    # offset_horizonal 向左
+    offset_vertical = torch.Tensor([0, 0, 1]) * patch_size_3d / 2
+    virtul_points = torch.stack([
+        gt_bboxes_center_array_shorter + offset_horizonal + offset_vertical,  # 左上
+        gt_bboxes_center_array_shorter - offset_horizonal + offset_vertical,  #             右上
+        gt_bboxes_center_array_shorter - offset_horizonal - offset_vertical,  #             右下
+        gt_bboxes_center_array_shorter + offset_horizonal - offset_vertical,  # 左下
+    ])
+    virtul_points_2d_uvd, _, _ = proj_3d_to_2d(virtul_points, lidar2img_i, img_org_shape)
+    return virtul_points_2d_uvd
+
+# 函数，从2d面积，试出来3d的空间patch大小，得出4角点
+def get_patch3d_corners2d_by_area2d(gt_bboxes_center_array_shorter, lidar2img_i, img_org_shape, patch_area):
+    orthogonal = get_orthogonal(gt_bboxes_center_array_shorter[:2])
+    orthogonal_norm = orthogonal / orthogonal.norm()
+    
+    target_area = patch_area
+    
+    # 粗粒度加细粒度吧
+    # 粗：0.3m为间隔，从0.到 3.0，一旦上一个小于，这一个大于target数值，则，进入细粒度，否则一直到最大
+    # 但是这有个问题，万一物体不在图像中呢？
+    
+    # 改一下在图像的判读逻辑：
+    #   分两层，一个是在相机的前还是后，一个是在画面里还是画面外
+    #   对于相机后的，直接不管，对于相机前画面外的，再处理
+    # 注意，可能会有物体的多个角点，分布在相机的前后，这种情况，一般是一定看不到的，所以也不管
+    
+    
+    
+    # 不要两头节省了，直接写一个能够反复执行的函数，不断获取更准确的分级就好了
+    # 粗粒度
+    
+    
+    step_level_num = 4
+    step_num = 10
+    init_step_size = torch.Tensor(1).fill_(0.5)
+    
+
+    next_area_low_bound = torch.zeros(1)
+    next_size_3d_low_bound = torch.zeros(1)
+    
+    for step_level in range(step_level_num):
+        begin_area = next_area_low_bound
+        begin_size = next_size_3d_low_bound
+        search_step_num = step_num
+        search_step_size = init_step_size / step_num**(step_level)
+        
+        next_area_low_bound,next_area_high_bound,\
+            next_size_3d_low_bound,next_size_3d_high_bound = search_one_level(
+            begin_area,
+            begin_size,
+            search_step_num,
+            search_step_size,
+            orthogonal_norm,
+            gt_bboxes_center_array_shorter,
+            target_area,
+            lidar2img_i,
+            img_org_shape,
+        )
+            
+        if next_size_3d_low_bound == next_size_3d_high_bound:
+            break
+            
+    
+
+        
+    # print(next_area_low_bound-target_area)
+    # 判断 now_area 是否贴近 bbox_2d_area
+        
+    return next_size_3d_high_bound
+
+
+def search_one_level(
+        begin_area,
+        begin_size,
+        search_step_num,
+        search_step_size,
+        orthogonal_norm,
+        gt_bboxes_center_array_shorter,
+        target_area,
+        lidar2img_i,
+        img_org_shape,
+    ):
+    now_2d_area = begin_area
+    patch_size_3d = begin_size
+    for i_size in range(1, search_step_num+1):
+        last_2d_area = now_2d_area.clone()
+        last_patch_size_3d = patch_size_3d.clone()
+        patch_size_3d = begin_size + i_size * search_step_size
+        now_2d_area = get_area_from_gt_center_array_and_patch_size_3d(
+            orthogonal_norm,
+            patch_size_3d,
+            gt_bboxes_center_array_shorter,
+            lidar2img_i,
+            img_org_shape,
+            )
+        # print(now_2d_area, last_2d_area)
+        if last_2d_area < target_area and now_2d_area >= target_area:
+            # print(
+            #     'target_area',
+            #     round(float(target_area),7),
+            #     'i_size',
+            #     i_size,
+            #     'patch_size_3d should be in between', 
+            #     round(float(last_2d_area),7),
+            #     round(float(now_2d_area),7),
+            # )
+            patch_size_3d_low_bound = last_patch_size_3d
+            patch_size_3d_high_bound = patch_size_3d
+            patch_area_low_bound = last_2d_area
+            patch_area_high_bound = now_2d_area
+            break
+        elif i_size == search_step_num and now_2d_area < target_area:
+            patch_size_3d_low_bound  = patch_size_3d
+            patch_size_3d_high_bound = patch_size_3d
+            patch_area_low_bound  = now_2d_area
+            patch_area_high_bound = now_2d_area
+            
+    next_area_low_bound = patch_area_low_bound
+    next_area_high_bound = patch_area_high_bound
+    next_size_3d_low_bound = patch_size_3d_low_bound
+    next_size_3d_high_bound = patch_size_3d_high_bound
+    
+    return next_area_low_bound,next_area_high_bound,next_size_3d_low_bound,next_size_3d_high_bound
+
+            
+            
+
+def get_2d_patch(
+    gt_bboxes_center_j, 
+    gt_bboxes_corners_j, 
+    lidar2img_i,
+    img_org_shape, 
+    area_rate):
+    
+    
+    
+    
+    # 1，计算2dpatch中心
+    # lidar2img_i = lidar2img[cams_i]
+    gt_bboxes_center_2d_uvd_ij, _, _ = proj_3d_to_2d(gt_bboxes_center_j[None], lidar2img_i, img_org_shape)
+    gt_bboxes_center_2d_uvd_ij = gt_bboxes_center_2d_uvd_ij[0]
+    
+    # 2，计算2dbbox大小 裁切边缘
+    bbox_2d_area = get_bbox_2d_area_cutonedge(gt_bboxes_corners_j, lidar2img_i, img_org_shape)
+    
+    # 3，计算2dpatch大小（按比例）
+    patch_area = bbox_2d_area * area_rate
+    
+    # 4，计算patch角点（这个理论上可以在图像外，因为是绑定center的）
+    patch_size_2d = patch_area.sqrt()
+    patch_size_2d = patch_size_2d.clamp(max=450)
+    center_u = gt_bboxes_center_2d_uvd_ij[0]
+    center_v = gt_bboxes_center_2d_uvd_ij[1]
+    center_d = gt_bboxes_center_2d_uvd_ij[2]
+    return torch.Tensor([
+        [center_u - patch_size_2d/2, center_v - patch_size_2d/2, center_d],
+        [center_u + patch_size_2d/2, center_v - patch_size_2d/2, center_d],
+        [center_u + patch_size_2d/2, center_v + patch_size_2d/2, center_d],
+        [center_u - patch_size_2d/2, center_v + patch_size_2d/2, center_d],
+    ])
+    
+    
+    
+
+
+def get_3d_patch_nooverlap(
+    gt_bboxes_center_j, 
+    gt_bboxes_corners_j, 
+    lidar2img_i, 
+    img_org_shape, 
+    area_rate):
+    
+    # 1，计算3dpatch的中心，绑定空间位置
+    patch_objcenter_distance = 0.5
+    gt_bboxes_center_array = gt_bboxes_center_j
+    gt_bboxes_center_array_len = gt_bboxes_center_array.norm()
+    gt_bboxes_center_array_shorter = gt_bboxes_center_array / gt_bboxes_center_array_len \
+        * (gt_bboxes_center_array_len-patch_objcenter_distance).clamp(min=0.2)
+    
+    # 2，计算2dbbox大小 在1个图像上
+    bbox_2d_area = get_bbox_2d_area_nocut(gt_bboxes_corners_j, lidar2img_i, img_org_shape)
+    
+    # 3，计算 patch_3d 大小（按比例），需要多次试验
+    patch_area = area_rate * bbox_2d_area
+    patch_size_3d = get_patch3d_corners2d_by_area2d(gt_bboxes_center_array_shorter, lidar2img_i, img_org_shape, patch_area)
+    
+    # 4，计算patch角点
+    orthogonal = get_orthogonal(gt_bboxes_center_array_shorter[:2])
+    orthogonal_norm = orthogonal / orthogonal.norm()
+    virtul_points_2d = get_virtul_points_2d_from_gt_center_array_and_patch_size_3d(
+        orthogonal_norm,
+        patch_size_3d,
+        gt_bboxes_center_array_shorter,
+        lidar2img_i,
+        img_org_shape
+        )
+    # print(patch_area, virtul_points_2d)
+    return virtul_points_2d
+
+def get_3d_patch_overlap(
+        gt_bboxes_center_j, 
+        gt_bboxes_corners_j, 
+        lidar2img_i_1,
+        lidar2img_i_2,
+        lidar2img_i,
+        img_org_shape, 
+        area_rate
+    ):
+    # 1，计算3dpatch的中心，绑定空间位置
+    patch_objcenter_distance = 0.5
+    gt_bboxes_center_array = gt_bboxes_center_j
+    gt_bboxes_center_array_len = gt_bboxes_center_array.norm()
+    gt_bboxes_center_array_shorter = gt_bboxes_center_array / gt_bboxes_center_array_len \
+        * (gt_bboxes_center_array_len-patch_objcenter_distance).clamp(min=0.2)
+    
+    
+    # 2，计算2dbbox大小 在2个图像上，取平均大小
+    bbox_2d_area_0 = get_bbox_2d_area_nocut(gt_bboxes_corners_j, lidar2img_i_1, img_org_shape)
+    bbox_2d_area_1 = get_bbox_2d_area_nocut(gt_bboxes_corners_j, lidar2img_i_2, img_org_shape)
+    bbox_2d_area = (bbox_2d_area_0 + bbox_2d_area_1) / 2.
+    
+    # 3，计算 patch_3d 大小（按比例），需要多次试验
+    patch_area = area_rate * bbox_2d_area
+    patch_size_3d = get_patch3d_corners2d_by_area2d(gt_bboxes_center_array_shorter, lidar2img_i, img_org_shape, patch_area)
+    
+    # 4，计算patch角点
+    orthogonal = get_orthogonal(gt_bboxes_center_array_shorter[:2])
+    orthogonal_norm = orthogonal / orthogonal.norm()
+    virtul_points_2d = get_virtul_points_2d_from_gt_center_array_and_patch_size_3d(
+        orthogonal_norm,
+        patch_size_3d,
+        gt_bboxes_center_array_shorter,
+        lidar2img_i,
+        img_org_shape,
+        )
+    # print(patch_area, virtul_points_2d)
+    return virtul_points_2d
+
+
+
+
+def get_3d_patch_onbox(
+        gt_bboxes_center_j, 
+        gt_bboxes_corners_j, 
+        lidar2img_i,
+        img_org_shape, 
+        area_rate
+    ):
+    # 1，计算Bbox面的可见性
+    
+    
+    """torch.Tensor: Coordinates of corners of all the boxes
+        in shape (N, 8, 3).
+
+        Convert the boxes to corners in clockwise order, in form of
+        ``(x0y0z0, x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0)``
+
+        .. code-block:: none
+
+            (                              up z
+            (               front x           ^
+            (                    /            |
+            (                   /             |
+            (     (x1, y0, z1) + -----------  + (x1, y1, z1)
+            (                 /|            / |
+            (                / |           /  |
+            (   (x0,y0, z1) + ----------- +   + (x1, y1, z0)
+            (               |  /      .   |  /
+            (               | / oriign    | /
+            left y<-------- + ----------- + (x0, y1, z0)
+                (x0, y0, z0)
+    """
+    # 关键1： 判断面的可见不可见
+    # 底面的顺序是 0 4 7 3
+    # 定义4个面的名字为 04 47 73 30
+    
+    gt_bboxes_corners_j  # torch.Size([8, 3])
+    corners_bottom_ordered_xy = gt_bboxes_corners_j[[0,4,7,3],:2]
+    corners_bottom_ordered_theta = torch.atan2(corners_bottom_ordered_xy[:,1],corners_bottom_ordered_xy[:,0])
+
+    # theta need adjust 因为有可能相差接近360度，
+    # 校准的目标是，最大角度和最小角度，相差超过180
+    for adjust_step in range(10):
+        if corners_bottom_ordered_theta.max() - corners_bottom_ordered_theta.min() > np.pi:
+            corners_bottom_ordered_theta[corners_bottom_ordered_theta.argmin()] += 2 * np.pi
+        else:
+            break
+    if adjust_step > 3:
+        print('error bbox may cover xyz-axis origin')
+        assert adjust_step <= 3
+        
+    max_theta_idx = corners_bottom_ordered_theta.argmax()
+    min_theta_idx = corners_bottom_ordered_theta.argmin()
+    # 判断这两个极值theta点是否相邻，相邻则，我们只能看到box的一个面
+    # 所以 theta是 按顺时针或者逆时针有序，就非常重要
+    idx_minus = max_theta_idx - min_theta_idx
+    if idx_minus.abs()==1 or idx_minus.abs()==3:
+        only_one_face_saw = True
+    else:
+        only_one_face_saw = False
+        
+        
+    # 判断最近的corner 这个一定能被看到
+    nearest_corner_idx = corners_bottom_ordered_xy.norm(dim=1).argmin()
+    
+    # corners_bottom_ordered_theta 遵从
+    # face_04_47_73_30_saw
+    # [0,4,7,3] 的顺序
+    #  0,1,2,3
+    
+    corner_0_1_2_3_saw = torch.zeros(4)
+    corner_0_1_2_3_saw[max_theta_idx] = 1
+    corner_0_1_2_3_saw[min_theta_idx] = 1
+    corner_0_1_2_3_saw[nearest_corner_idx] = 1
+    corner_0_1_2_3_0_saw = torch.cat([corner_0_1_2_3_saw, corner_0_1_2_3_saw[:1]])
+    face_01_12_23_30_saw = (corner_0_1_2_3_0_saw[:-1] + corner_0_1_2_3_0_saw[1:])==2
+    face_01_12_23_30_0473_saw = face_01_12_23_30_saw
+    
+
+    
+    # 2，计算3dpatch的中心，绑定bbox面上
+    # boxside_patch
+    
+    
+    side_2_corneridx = torch.LongTensor([
+        [0,4,5,1],
+        [4,7,6,5],
+        [7,3,2,6],
+        [3,0,1,2],
+    ])
+    
+    
+    boxside_patch_corners_0473_list = []
+    
+    boxside_patch_wh_real_0473_list = []
+    
+    
+    for boxside_i in range(4):
+        boxside_saw = face_01_12_23_30_0473_saw[boxside_i]
+        corneridx_i = side_2_corneridx[boxside_i]
+        corners_i = gt_bboxes_corners_j[corneridx_i]
+        boxside_patch_center = corners_i.mean(0)
+        
+        # 3，计算3dpatch大小（按box面的比例），在bbox面上
+        boxside_w = (corners_i[1]-corners_i[0])
+        boxside_w_length = boxside_w.norm()
+        boxside_h = (corners_i[2]-corners_i[1])
+        boxside_h_length = boxside_h.norm()
+        boxside_area = boxside_w_length * boxside_h_length
+        boxside_wh_ratio = boxside_w_length / boxside_h_length
+        # boxside_patch_h * (boxside_patch_h * boxside_wh_ratio) = boxside_patch_area
+        # boxside_patch_h = (boxside_patch_area/boxside_wh_ratio).sqrt()
+        boxside_patch_area = area_rate * boxside_area
+        boxside_patch_h = (boxside_patch_area/boxside_wh_ratio).sqrt()
+        boxside_patch_w = boxside_wh_ratio * boxside_patch_h
+        
+        boxside_w_norm = boxside_w / boxside_w_length
+        boxside_h_norm = boxside_h / boxside_h_length
+        
+        
+        w_offset = boxside_w_norm * boxside_patch_w / 2
+        h_offset = boxside_h_norm * boxside_patch_h / 2
+        
+        offset_horizonal = w_offset
+        offset_vertical = h_offset
+        
+        boxside_patch_corners = torch.stack([
+            boxside_patch_center + offset_horizonal + offset_vertical,  # A上
+            boxside_patch_center - offset_horizonal + offset_vertical,  #             B上
+            boxside_patch_center - offset_horizonal - offset_vertical,  #             B下
+            boxside_patch_center + offset_horizonal - offset_vertical,  # A下
+        ])
+        
+        # (boxside_patch_corner[2]-boxside_patch_corner[1])@ (boxside_patch_corner[1]-boxside_patch_corner[0])
+        # (boxside_patch_corner[2]-boxside_patch_corner[1]).norm() * (boxside_patch_corner[1]-boxside_patch_corner[0]).norm()
+        boxside_patch_corners_0473_list.append(boxside_patch_corners)
+        boxside_patch_wh_real_0473_list.append(torch.tensor([boxside_patch_w, boxside_patch_h]))
+        
+        
+        # 看到看不到都输出，最后输出一下判断就好，保证输出4个patch。
+        
+    
+    
+    
+    
+    # 4，计算patch角点
+    boxside_patch_corners_0473_uvd_list = []
+    for boxside_i in range(4):
+        boxside_patch_corners_xyz = boxside_patch_corners_0473_list[boxside_i]
+        boxside_patch_corners_uvd, _, _ = proj_3d_to_2d(boxside_patch_corners_xyz, lidar2img_i, img_org_shape)
+        boxside_patch_corners_0473_uvd_list.append(boxside_patch_corners_uvd)
+    
+    
+    return boxside_patch_corners_0473_uvd_list, face_01_12_23_30_0473_saw, boxside_patch_wh_real_0473_list
+                        
+
+
+
+
+def get_3d_patch_onbox_area_list(
+        gt_bboxes_center_j, 
+        gt_bboxes_corners_j, 
+        lidar2img_i,
+        img_org_shape, 
+        area_rate_list
+    ):
+    # 1，计算Bbox面的可见性
+    
+    
+    """torch.Tensor: Coordinates of corners of all the boxes
+        in shape (N, 8, 3).
+
+        Convert the boxes to corners in clockwise order, in form of
+        ``(x0y0z0, x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0)``
+
+        .. code-block:: none
+
+            (                              up z
+            (               front x           ^
+            (                    /            |
+            (                   /             |
+            (     (x1, y0, z1) + -----------  + (x1, y1, z1)
+            (                 /|            / |
+            (                / |           /  |
+            (   (x0,y0, z1) + ----------- +   + (x1, y1, z0)
+            (               |  /      .   |  /
+            (               | / oriign    | /
+            left y<-------- + ----------- + (x0, y1, z0)
+                (x0, y0, z0)
+    """
+    # 关键1： 判断面的可见不可见
+    # 底面的顺序是 0 4 7 3
+    # 定义4个面的名字为 04 47 73 30
+    
+    gt_bboxes_corners_j  # torch.Size([8, 3])
+    corners_bottom_ordered_xy = gt_bboxes_corners_j[[0,4,7,3],:2]
+    corners_bottom_ordered_theta = torch.atan2(corners_bottom_ordered_xy[:,1],corners_bottom_ordered_xy[:,0])
+
+    # theta need adjust 因为有可能相差接近360度，
+    # 校准的目标是，最大角度和最小角度，相差超过180
+    for adjust_step in range(10):
+        if corners_bottom_ordered_theta.max() - corners_bottom_ordered_theta.min() > np.pi:
+            corners_bottom_ordered_theta[corners_bottom_ordered_theta.argmin()] += 2 * np.pi
+        else:
+            break
+    if adjust_step > 3:
+        print('error bbox may cover xyz-axis origin')
+        assert adjust_step <= 3
+        
+    max_theta_idx = corners_bottom_ordered_theta.argmax()
+    min_theta_idx = corners_bottom_ordered_theta.argmin()
+    # 判断这两个极值theta点是否相邻，相邻则，我们只能看到box的一个面
+    # 所以 theta是 按顺时针或者逆时针有序，就非常重要
+    idx_minus = max_theta_idx - min_theta_idx
+    if idx_minus.abs()==1 or idx_minus.abs()==3:
+        only_one_face_saw = True
+    else:
+        only_one_face_saw = False
+        
+        
+    # 判断最近的corner 这个一定能被看到
+    nearest_corner_idx = corners_bottom_ordered_xy.norm(dim=1).argmin()
+    
+    # corners_bottom_ordered_theta 遵从
+    # face_04_47_73_30_saw
+    # [0,4,7,3] 的顺序
+    #  0,1,2,3
+    
+    corner_0_1_2_3_saw = torch.zeros(4)
+    corner_0_1_2_3_saw[max_theta_idx] = 1
+    corner_0_1_2_3_saw[min_theta_idx] = 1
+    corner_0_1_2_3_saw[nearest_corner_idx] = 1
+    corner_0_1_2_3_0_saw = torch.cat([corner_0_1_2_3_saw, corner_0_1_2_3_saw[:1]])
+    face_01_12_23_30_saw = (corner_0_1_2_3_0_saw[:-1] + corner_0_1_2_3_0_saw[1:])==2
+    face_01_12_23_30_0473_saw = face_01_12_23_30_saw
+    
+
+    
+    # 2，计算3dpatch的中心，绑定bbox面上
+    # boxside_patch
+    
+    
+    side_2_corneridx = torch.LongTensor([
+        [0,4,5,1],
+        [4,7,6,5],
+        [7,3,2,6],
+        [3,0,1,2],
+    ])
+    
+    
+
+
+    boxside_patch_corners_0473_dict = {}
+    boxside_patch_wh_real_0473_dict = {}
+    
+    
+    for area_rate in area_rate_list:
+        boxside_patch_corners_0473_list = []
+        boxside_patch_wh_real_0473_list = []
+
+        for boxside_i in range(4):
+            boxside_saw = face_01_12_23_30_0473_saw[boxside_i]
+            corneridx_i = side_2_corneridx[boxside_i]
+            corners_i = gt_bboxes_corners_j[corneridx_i]
+            boxside_patch_center = corners_i.mean(0)
+            
+            # 3，计算3dpatch大小（按box面的比例），在bbox面上
+            boxside_w = (corners_i[1]-corners_i[0])
+            boxside_w_length = boxside_w.norm()
+            boxside_h = (corners_i[2]-corners_i[1])
+            boxside_h_length = boxside_h.norm()
+            boxside_area = boxside_w_length * boxside_h_length
+            boxside_wh_ratio = boxside_w_length / boxside_h_length
+            # boxside_patch_h * (boxside_patch_h * boxside_wh_ratio) = boxside_patch_area
+            # boxside_patch_h = (boxside_patch_area/boxside_wh_ratio).sqrt()
+            boxside_patch_area = area_rate * boxside_area
+            boxside_patch_h = (boxside_patch_area/boxside_wh_ratio).sqrt()
+            boxside_patch_w = boxside_wh_ratio * boxside_patch_h
+            
+            boxside_w_norm = boxside_w / boxside_w_length
+            boxside_h_norm = boxside_h / boxside_h_length
+            
+            
+            w_offset = boxside_w_norm * boxside_patch_w / 2
+            h_offset = boxside_h_norm * boxside_patch_h / 2
+            
+            offset_horizonal = w_offset
+            offset_vertical = h_offset
+            
+            boxside_patch_corners = torch.stack([
+                boxside_patch_center + offset_horizonal + offset_vertical,  # A上
+                boxside_patch_center - offset_horizonal + offset_vertical,  #             B上
+                boxside_patch_center - offset_horizonal - offset_vertical,  #             B下
+                boxside_patch_center + offset_horizonal - offset_vertical,  # A下
+            ])
+
+            boxside_patch_corners_0473_list.append(boxside_patch_corners)
+            boxside_patch_wh_real_0473_list.append(torch.tensor([boxside_patch_w, boxside_patch_h]))
+            
+        boxside_patch_corners_0473_dict[str(area_rate)] = boxside_patch_corners_0473_list
+        boxside_patch_wh_real_0473_dict[str(area_rate)] = boxside_patch_wh_real_0473_list
+        # 看到看不到都输出，最后输出一下判断就好，保证输出4个patch。
+        
+    
+    
+    
+    
+    # 4，计算patch角点
+    boxside_patch_corners_0473_uvd_dict = {}
+    for area_rate in area_rate_list:
+        boxside_patch_corners_0473_uvd_list = []
+        for boxside_i in range(4):
+            boxside_patch_corners_xyz = boxside_patch_corners_0473_dict[str(area_rate)][boxside_i]
+            boxside_patch_corners_uvd, _, _ = proj_3d_to_2d(boxside_patch_corners_xyz, lidar2img_i, img_org_shape)
+            boxside_patch_corners_0473_uvd_list.append(boxside_patch_corners_uvd)
+        boxside_patch_corners_0473_uvd_dict[str(area_rate)] = boxside_patch_corners_0473_uvd_list
+    
+    
+    return boxside_patch_corners_0473_uvd_dict, face_01_12_23_30_0473_saw, boxside_patch_wh_real_0473_dict
+                        
+
+
+
+
+def get_3d_patch_square_onbox_area_list(
+        gt_bboxes_center_j, 
+        gt_bboxes_corners_j, 
+        lidar2img_i,
+        img_org_shape, 
+        area_rate_list
+    ):
+    # 1，计算Bbox面的可见性
+    
+    
+    """torch.Tensor: Coordinates of corners of all the boxes
+        in shape (N, 8, 3).
+
+        Convert the boxes to corners in clockwise order, in form of
+        ``(x0y0z0, x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0)``
+
+        .. code-block:: none
+
+            (                              up z
+            (               front x           ^
+            (                    /            |
+            (                   /             |
+            (     (x1, y0, z1) + -----------  + (x1, y1, z1)
+            (                 /|            / |
+            (                / |           /  |
+            (   (x0,y0, z1) + ----------- +   + (x1, y1, z0)
+            (               |  /      .   |  /
+            (               | / oriign    | /
+            left y<-------- + ----------- + (x0, y1, z0)
+                (x0, y0, z0)
+    """
+    # 关键1： 判断面的可见不可见
+    # 底面的顺序是 0 4 7 3
+    # 定义4个面的名字为 04 47 73 30
+    
+    gt_bboxes_corners_j  # torch.Size([8, 3])
+    corners_bottom_ordered_xy = gt_bboxes_corners_j[[0,4,7,3],:2]
+    corners_bottom_ordered_theta = torch.atan2(corners_bottom_ordered_xy[:,1],corners_bottom_ordered_xy[:,0])
+
+    # theta need adjust 因为有可能相差接近360度，
+    # 校准的目标是，最大角度和最小角度，相差超过180
+    for adjust_step in range(10):
+        if corners_bottom_ordered_theta.max() - corners_bottom_ordered_theta.min() > np.pi:
+            corners_bottom_ordered_theta[corners_bottom_ordered_theta.argmin()] += 2 * np.pi
+        else:
+            break
+    if adjust_step > 3:
+        print('error bbox may cover xyz-axis origin')
+        assert adjust_step <= 3
+        
+    max_theta_idx = corners_bottom_ordered_theta.argmax()
+    min_theta_idx = corners_bottom_ordered_theta.argmin()
+    # 判断这两个极值theta点是否相邻，相邻则，我们只能看到box的一个面
+    # 所以 theta是 按顺时针或者逆时针有序，就非常重要
+    idx_minus = max_theta_idx - min_theta_idx
+    if idx_minus.abs()==1 or idx_minus.abs()==3:
+        only_one_face_saw = True
+    else:
+        only_one_face_saw = False
+        
+        
+    # 判断最近的corner 这个一定能被看到
+    nearest_corner_idx = corners_bottom_ordered_xy.norm(dim=1).argmin()
+    
+    # corners_bottom_ordered_theta 遵从
+    # face_04_47_73_30_saw
+    # [0,4,7,3] 的顺序
+    #  0,1,2,3
+    
+    corner_0_1_2_3_saw = torch.zeros(4)
+    corner_0_1_2_3_saw[max_theta_idx] = 1
+    corner_0_1_2_3_saw[min_theta_idx] = 1
+    corner_0_1_2_3_saw[nearest_corner_idx] = 1
+    corner_0_1_2_3_0_saw = torch.cat([corner_0_1_2_3_saw, corner_0_1_2_3_saw[:1]])
+    face_01_12_23_30_saw = (corner_0_1_2_3_0_saw[:-1] + corner_0_1_2_3_0_saw[1:])==2
+    face_01_12_23_30_0473_saw = face_01_12_23_30_saw
+    
+
+    
+    # 2，计算3dpatch的中心，绑定bbox面上
+    # boxside_patch
+    
+    
+    side_2_corneridx = torch.LongTensor([
+        [0,4,5,1],
+        [4,7,6,5],
+        [7,3,2,6],
+        [3,0,1,2],
+    ])
+    
+    
+
+
+    boxside_patch_corners_0473_dict = {}
+    boxside_patch_wh_real_0473_dict = {}
+    
+    
+    for area_rate in area_rate_list:
+        boxside_patch_corners_0473_list = []
+        boxside_patch_wh_real_0473_list = []
+
+        for boxside_i in range(4):
+            boxside_saw = face_01_12_23_30_0473_saw[boxside_i]
+            corneridx_i = side_2_corneridx[boxside_i]
+            corners_i = gt_bboxes_corners_j[corneridx_i]
+            boxside_patch_center = corners_i.mean(0)
+            
+            # 3，计算3dpatch大小（按box面的比例），在bbox面上
+            boxside_w = (corners_i[1]-corners_i[0])
+            boxside_w_length = boxside_w.norm()
+            boxside_h = (corners_i[2]-corners_i[1])
+            boxside_h_length = boxside_h.norm()
+            boxside_area = boxside_w_length * boxside_h_length
+
+            # boxside_wh_ratio = boxside_w_length / boxside_h_length
+            boxside_wh_ratio = 1.0
+
+            boxside_patch_area = area_rate * boxside_area
+            boxside_patch_h = (boxside_patch_area/boxside_wh_ratio).sqrt()
+            boxside_patch_w = boxside_wh_ratio * boxside_patch_h
+            
+            boxside_w_norm = boxside_w / boxside_w_length
+            boxside_h_norm = boxside_h / boxside_h_length
+            
+            
+            w_offset = boxside_w_norm * boxside_patch_w / 2
+            h_offset = boxside_h_norm * boxside_patch_h / 2
+            
+            offset_horizonal = w_offset
+            offset_vertical = h_offset
+            
+            boxside_patch_corners = torch.stack([
+                boxside_patch_center + offset_horizonal + offset_vertical,  # A上
+                boxside_patch_center - offset_horizonal + offset_vertical,  #             B上
+                boxside_patch_center - offset_horizonal - offset_vertical,  #             B下
+                boxside_patch_center + offset_horizonal - offset_vertical,  # A下
+            ])
+
+            boxside_patch_corners_0473_list.append(boxside_patch_corners)
+            boxside_patch_wh_real_0473_list.append(torch.tensor([boxside_patch_w, boxside_patch_h]))
+            
+        boxside_patch_corners_0473_dict[str(area_rate)] = boxside_patch_corners_0473_list
+        boxside_patch_wh_real_0473_dict[str(area_rate)] = boxside_patch_wh_real_0473_list
+        # 看到看不到都输出，最后输出一下判断就好，保证输出4个patch。
+        
+    
+    
+    
+    
+    # 4，计算patch角点
+    boxside_patch_corners_0473_uvd_dict = {}
+    for area_rate in area_rate_list:
+        boxside_patch_corners_0473_uvd_list = []
+        for boxside_i in range(4):
+            boxside_patch_corners_xyz = boxside_patch_corners_0473_dict[str(area_rate)][boxside_i]
+            boxside_patch_corners_uvd, _, _ = proj_3d_to_2d(boxside_patch_corners_xyz, lidar2img_i, img_org_shape)
+            boxside_patch_corners_0473_uvd_list.append(boxside_patch_corners_uvd)
+        boxside_patch_corners_0473_uvd_dict[str(area_rate)] = boxside_patch_corners_0473_uvd_list
+    
+    
+    return boxside_patch_corners_0473_uvd_dict, face_01_12_23_30_0473_saw, boxside_patch_wh_real_0473_dict
+                        
+
+
diff --git a/extend_common b/extend_common
new file mode 120000
index 0000000..0633dd9
--- /dev/null
+++ b/extend_common
@@ -0,0 +1 @@
+../extend_common/
\ No newline at end of file
diff --git a/mmdet3d/apis/test_save_instance_patch_mask.py b/mmdet3d/apis/test_save_instance_patch_mask.py
new file mode 100644
index 0000000..5def44b
--- /dev/null
+++ b/mmdet3d/apis/test_save_instance_patch_mask.py
@@ -0,0 +1,146 @@
+import mmcv
+import torch
+import numpy as np
+import os
+import PIL.Image as Image
+import torchvision.transforms as transforms
+import torch.nn.functional as F
+import torchvision
+from torchvision.utils import save_image
+import cv2
+from extend.patch_apply import proj_3d_to_2d
+
+
+def single_gpu_test(model, data_loader, show=False, out_dir=None, mask_save_dir=None):
+    """Test model with single gpu.
+
+    This method tests model with single gpu and gives the 'show' option.
+    By setting ``show=True``, it saves the visualization results under
+    ``out_dir``.
+
+    Args:
+        model (nn.Module): Model to be tested.
+        data_loader (nn.Dataloader): Pytorch data loader.
+        show (bool): Whether to save viualization results.
+            Default: True.
+        out_dir (str): The path to save visualization results.
+            Default: None.
+
+    Returns:
+        list[dict]: The prediction results.
+    """
+    results = []
+    dataset = data_loader.dataset
+    prog_bar = mmcv.ProgressBar(len(dataset))
+    
+    patch_instance_mask_dir = mask_save_dir
+    area_rate_2dbbox_list = [
+        0.01,
+        0.02,
+        0.05,
+        0.1,
+        0.2,
+        0.4,
+    ]
+    
+    
+    # skip_data_num = 470
+    
+    for data_i, data in enumerate(data_loader):
+        # if data_i < skip_data_num:
+        #     continue
+        for area_rate_2dbbox in area_rate_2dbbox_list:
+            area_rate_2dbbox_save_str = str(int(100*area_rate_2dbbox)).zfill(3)
+
+            input_image = data['img'][0]._data[0] # torch.Size([1, 6, 3, 448, 800])
+            device = input_image.device
+            
+            gt_bboxes_3d = data['gt_bboxes_3d']
+            gt_labels_3d = data['gt_labels_3d']
+
+            
+            img_metas = data['img_metas'][0]._data[0][0]
+            img_path_list = img_metas['filename']
+            lidar2img = img_metas['lidar2img']
+            lidar2img = torch.tensor(lidar2img).float()
+
+            img_tensor_list = []
+            cam_num = len(img_path_list)
+            
+            save_path_list = []
+            for cams_i in range(cam_num):
+                img_path = img_path_list[cams_i]
+                img_path_core = img_path.split('nuscenes/')[1]
+                save_path = os.path.join(patch_instance_mask_dir, area_rate_2dbbox_save_str, img_path_core)
+                save_path_list.append(save_path)
+            
+
+            img_org_shape = (900,1600)
+
+            
+            gt_bboxes_3d_tensor = gt_bboxes_3d[0]._data[0][0]
+            gt_labels_3d_tensor = gt_labels_3d[0]._data[0][0]
+            
+            if gt_bboxes_3d_tensor.center.shape[0] == 0:
+                no_gt_bbox = True
+            else:
+                no_gt_bbox = False
+                # get gtbbox info
+                
+                gt_bboxes_bottom_center = gt_bboxes_3d_tensor.bottom_center
+                gt_bboxes_height = gt_bboxes_3d_tensor.height
+                gt_bboxes_3d_center = gt_bboxes_bottom_center.clone()
+                gt_bboxes_3d_center[:,2] += gt_bboxes_height *0.5
+                gt_bboxes_center = gt_bboxes_3d_center
+                gt_bboxes_corners = gt_bboxes_3d_tensor.corners
+                gt_bboxes_dims = gt_bboxes_3d_tensor.dims
+                obj_num = gt_bboxes_center.shape[0]
+            
+            
+            for cams_i in range(cam_num):
+                mask_np = np.zeros(img_org_shape).astype(np.uint8)
+                if not no_gt_bbox: # 有物体
+                    for objs_j in range(obj_num):
+                        
+                        # 1  获取物体中心，以及各个角点在图像上的位置
+                        gt_bboxes_center_j = gt_bboxes_center[objs_j]
+                        gt_bboxes_corners_j = gt_bboxes_corners[objs_j]
+                        lidar2img_i = lidar2img[cams_i]
+                        gt_bboxes_center_2d_ij, on_the_image = proj_3d_to_2d(gt_bboxes_center_j[None], lidar2img_i, img_org_shape)
+                        gt_bboxes_center_2d_ij = gt_bboxes_center_2d_ij[0]
+
+                        if not on_the_image:
+                            continue
+                        
+                        # 2  获取patch在3D空间中的大小
+                        gt_bboxes_corners_2d_ij, on_the_image = proj_3d_to_2d(gt_bboxes_corners_j, lidar2img_i, img_org_shape)
+                        h, w = img_org_shape
+                        gt_bboxes_corners_2d_ij_x_max = gt_bboxes_corners_2d_ij[:,0].max().clamp(0,w)
+                        gt_bboxes_corners_2d_ij_x_min = gt_bboxes_corners_2d_ij[:,0].min().clamp(0,w)
+                        gt_bboxes_corners_2d_ij_y_max = gt_bboxes_corners_2d_ij[:,1].max().clamp(0,h)
+                        gt_bboxes_corners_2d_ij_y_min = gt_bboxes_corners_2d_ij[:,1].min().clamp(0,h)
+                        
+                        patch_area = (gt_bboxes_corners_2d_ij_x_max - gt_bboxes_corners_2d_ij_x_min) \
+                            * (gt_bboxes_corners_2d_ij_y_max - gt_bboxes_corners_2d_ij_y_min) * area_rate_2dbbox
+                        patch_size_2d = patch_area.sqrt()
+                        patch_size_2d = patch_size_2d.clamp(max=450)
+
+                        # 3. 粘贴 patch mask
+                        pt1_x1y1 = (gt_bboxes_center_2d_ij - patch_size_2d/2).int().tolist()
+                        pt1_x2y2 = (gt_bboxes_center_2d_ij + patch_size_2d/2).int().tolist()
+                        cv2.rectangle(
+                            img=mask_np, 
+                            pt1=pt1_x1y1,
+                            pt2=pt1_x2y2, 
+                            color=1, 
+                            thickness=-1)
+                else:
+                    pass
+                    
+                save_path = save_path_list[cams_i]
+                save_path_father_dir = os.path.dirname(save_path)
+                os.makedirs(save_path_father_dir, exist_ok=True)
+                cv2.imwrite(save_path, (mask_np*255))
+                    
+        prog_bar.update()
+    return results
diff --git a/mmdet3d/apis/test_save_patch_info.py b/mmdet3d/apis/test_save_patch_info.py
new file mode 100644
index 0000000..eed40b3
--- /dev/null
+++ b/mmdet3d/apis/test_save_patch_info.py
@@ -0,0 +1,534 @@
+import mmcv
+import torch
+import numpy as np
+import os
+import PIL.Image as Image
+import torchvision.transforms as transforms
+import torch.nn.functional as F
+import torchvision
+from torchvision.utils import save_image
+import cv2
+import pickle
+
+from extend.func_for_patchinfo import *
+from extend_common.get_scene_start_idx import get_scene_start_idx
+
+
+
+def single_gpu_test(model, data_loader, patch_info_2d3d_dir=None, show=False, out_dir=None):
+    """Test model with single gpu.
+
+    This method tests model with single gpu and gives the 'show' option.
+    By setting ``show=True``, it saves the visualization results under
+    ``out_dir``.
+
+    Args:
+        model (nn.Module): Model to be tested.
+        data_loader (nn.Dataloader): Pytorch data loader.
+        show (bool): Whether to save viualization results.
+            Default: True.
+        out_dir (str): The path to save visualization results.
+            Default: None.
+
+    Returns:
+        list[dict]: The prediction results.
+    """
+    # model.eval()
+    results = []
+    dataset = data_loader.dataset
+
+
+    prog_bar = mmcv.ProgressBar(len(dataset))
+    
+
+    area_rate_2dbbox_save_str = 'all'
+    
+
+    scene_start_idx_list = get_scene_start_idx()
+    scene_start_idx_list.append(6019)
+
+    
+    # 为每一个类别定义一个patch
+    class_names_list = [
+        'car', 'truck', 'construction_vehicle', 
+        'bus', 'trailer', 'barrier',
+        'motorcycle', 'bicycle', 'pedestrian', 
+        'traffic_cone'
+    ]
+    
+    
+    # skip_data_num = 470
+    
+    nusc = data_loader.dataset.nusc
+    
+    
+    
+    
+    for data_i, data in enumerate(data_loader):
+        
+    #     break
+        
+    # for iii2 in range(1000):
+        # if data_i < skip_data_num:
+        #     continue
+        input_image = data['img'][0]._data[0] # torch.Size([1, 6, 3, 448, 800])
+        device = input_image.device
+        
+        gt_bboxes_3d = data['gt_bboxes_3d']
+        gt_labels_3d = data['gt_labels_3d']
+        instance_tokens = data['img_metas'][0]._data[0][0]['instance_tokens'] #object token
+        # del data['gt_bboxes_3d']
+        # del data['gt_labels_3d']
+        
+        # result = model(return_loss=False, rescale=True, **data)
+        
+        img_metas = data['img_metas'][0]._data[0][0]
+        img_path_list = img_metas['filename']
+        lidar2img = img_metas['lidar2img']
+        lidar2img = torch.tensor(lidar2img).float()
+        
+        
+
+        img_tensor_list = []
+        cam_num = len(img_path_list)
+        
+        save_path_list = []
+        for cams_i in range(cam_num):
+            img_path = img_path_list[cams_i]
+            img_path_core = img_path.split('nuscenes/')[1]
+            img_path_core = img_path_core.replace('.jpg','.pkl')
+            save_path = os.path.join(patch_info_2d3d_dir, area_rate_2dbbox_save_str, img_path_core)
+            save_path_list.append(save_path)
+        
+        
+        # 逻辑： 全部都存在，才能跳过
+        exist_list = []
+        for save_path in save_path_list:
+            exist_list.append(os.path.exists(save_path))
+        exist_tensor = torch.tensor(exist_list)
+        
+        if exist_tensor.all():
+            prog_bar.update()
+            continue
+            
+        
+        
+        
+        # for cam_i in range(6):
+        #     # ====================== 保存 pkl ====================
+        #     save_path_i = save_path_list[cam_i]
+        #     info_in_image_i = []
+        #     save_path_father_dir = os.path.dirname(save_path_i)
+        #     os.makedirs(save_path_father_dir, exist_ok=True)
+        #     with open(save_path_i, 'wb') as f:
+        #         pickle.dump(info_in_image_i, f)
+        
+        
+
+        img_org_shape = (900,1600)
+        img_h, img_w = img_org_shape
+
+        # img_tensor_ncam_noise = torch.zeros_like(img_tensor_ncam)
+        
+        # full image
+        # mask = torch.ones_like(img_tensor_ncam)
+        
+        gt_bboxes_3d_tensor = gt_bboxes_3d[0]._data[0][0]
+        gt_labels_3d_tensor = gt_labels_3d[0]._data[0][0]
+        
+        
+        
+        
+        # 当前场景没有gt
+        
+        if gt_bboxes_3d_tensor.center.shape[0] == 0:
+            no_gt_bbox = True
+        else:
+            no_gt_bbox = False
+            # get gtbbox info
+            
+            gt_bboxes_bottom_center = gt_bboxes_3d_tensor.bottom_center
+            gt_bboxes_height = gt_bboxes_3d_tensor.height
+            gt_bboxes_3d_center = gt_bboxes_bottom_center.clone()
+            gt_bboxes_3d_center[:,2] += gt_bboxes_height *0.5
+            gt_bboxes_center = gt_bboxes_3d_center
+            gt_bboxes_corners = gt_bboxes_3d_tensor.corners
+            gt_bboxes_dims = gt_bboxes_3d_tensor.dims
+            obj_num = gt_bboxes_center.shape[0]
+        
+        
+        
+
+        if not no_gt_bbox:
+            
+            # 错了，最好还是不要并行化！并行化难以做定制的处理，
+            # 并且后面还是要for循环去做，不如直接就for循环
+            # center
+            gt_bboxes_center_4d = torch.cat([gt_bboxes_center, torch.ones_like(gt_bboxes_center[:,0:1])], dim=-1)
+            # o obj num
+            # c cam num
+            gt_bboxes_center__c_o_4_4 = gt_bboxes_center_4d[None, :, None, :]
+            lidar2img__c_o_4_4 = lidar2img[:,None,:,:]
+            gt_bboxes_center_2d = gt_bboxes_center__c_o_4_4 @ lidar2img__c_o_4_4.transpose(2,3)
+            gt_bboxes_center_2d = gt_bboxes_center_2d.squeeze(2)
+            gt_bboxes_center_2d_depth = gt_bboxes_center_2d[..., 2]
+            gt_bboxes_center_2d_depth_no_zero = torch.where(
+                    gt_bboxes_center_2d_depth!=0,
+                    gt_bboxes_center_2d_depth,
+                    torch.ones_like(gt_bboxes_center_2d_depth).fill_(1e-5)
+                )
+
+            gt_bboxes_center_2d[..., 0] /= gt_bboxes_center_2d_depth_no_zero #?
+            gt_bboxes_center_2d[..., 1] /= gt_bboxes_center_2d_depth_no_zero #?
+            on_the_image_depth = gt_bboxes_center_2d_depth > 0
+            coor_x, coor_y = gt_bboxes_center_2d[...,0], gt_bboxes_center_2d[...,1]
+            h, w = img_org_shape
+            on_the_image_size = (coor_x > 0) * (coor_x < w) * (coor_y > 0) * (coor_y < h) #bbox中心是否在image内
+
+            # 更新v1.0：用1.5倍的size
+            # on_the_image_size_bigger = (coor_x > -0.5*w) * (coor_x < 1.5*w) * (coor_y > -0.5*h) * (coor_y < 1.5*h)
+
+            # 更新v2.0：用8个角点 的 size + depth
+
+
+
+            ##########################################
+            gt_bboxes_corners_line = gt_bboxes_corners.reshape(-1,3)
+            # corner
+            gt_bboxes_corners_4d = torch.cat([gt_bboxes_corners_line, torch.ones_like(gt_bboxes_corners_line[:,0:1])], dim=-1)
+            # o obj num
+            # c cam num
+            gt_bboxes_corners__c_o_4_4 = gt_bboxes_corners_4d[None, :, None, :]
+            lidar2img__c_o_4_4 = lidar2img[:,None,:,:]
+            gt_bboxes_corners_2d = gt_bboxes_corners__c_o_4_4 @ lidar2img__c_o_4_4.transpose(2,3)
+            gt_bboxes_corners_2d = gt_bboxes_corners_2d.squeeze(2)
+            gt_bboxes_corners_2d_depth = gt_bboxes_corners_2d[..., 2]
+            gt_bboxes_corners_2d_depth_no_zero = torch.where(
+                    gt_bboxes_corners_2d_depth!=0,
+                    gt_bboxes_corners_2d_depth,
+                    torch.ones_like(gt_bboxes_corners_2d_depth).fill_(1e-5)
+                )
+
+            gt_bboxes_corners_2d[..., 0] /= gt_bboxes_corners_2d_depth_no_zero
+            gt_bboxes_corners_2d[..., 1] /= gt_bboxes_corners_2d_depth_no_zero
+            corners_on_the_image_depth = gt_bboxes_corners_2d_depth > 0
+            corners_on_the_image_depth = corners_on_the_image_depth.reshape(6, obj_num, 8)
+            coor_x, coor_y = gt_bboxes_corners_2d[...,0], gt_bboxes_corners_2d[...,1]
+            h, w = img_org_shape
+            corners_on_the_image_size = (coor_x > 0) * (coor_x < w) * (coor_y > 0) * (coor_y < h)
+            corners_on_the_image_size = corners_on_the_image_size.reshape(6, obj_num, 8)
+            # 必须8个角都 depth>0  
+            corners_on_the_image_depth = corners_on_the_image_depth.all(2)
+            # 至少有 1 个角 size-in 即可
+            corners_on_the_image_size = corners_on_the_image_size.any(2)
+            # 两方面 都满足
+            on_the_image_size_bigger = corners_on_the_image_depth * corners_on_the_image_size
+            #######################################
+
+
+            on_the_image = on_the_image_depth * on_the_image_size
+            on_the_image_bigger = on_the_image_depth * on_the_image_size_bigger
+            on_two_image = on_the_image.sum(0) > 1
+
+
+            
+            
+            ##################################################################################################
+            #  patch2d(for class patch)  patch3d(for overlap patch)  patch3d_temporal(for temporal patch)
+            ##################################################################################################
+            
+            
+            area_list = [
+                0.01,
+                0.02,
+                0.05,
+                0.1,
+                0.2,
+                0.4,
+                1.0
+            ]
+            
+            info_in_image_6cam = []
+            
+            # for 6 cameras 对6个相机
+            for cam_i in range(cam_num):
+                lidar2img_i = lidar2img[cam_i]
+                
+                
+                
+                # =================== get patch info!(获取patch 信息) ======================
+                
+                # ==================== patch 2d ==========================
+                points_2d_uvd_patch2d_dict_list = []
+                for j in range(obj_num):
+                    
+                    if on_the_image_bigger[cam_i, j]:
+                        points_2d_uvd_patch2d_dict = {}
+                        lidar2img_i = lidar2img[cam_i]
+                        gt_bboxes_center_j = gt_bboxes_center[j]
+                        gt_bboxes_corners_j = gt_bboxes_corners[j]
+
+                        for area_rate in area_list: # Loop over multiple areas 对多个面积进行循环
+                            points_2d_uvd_patch2d = get_2d_patch(
+                                    gt_bboxes_center_j, 
+                                    gt_bboxes_corners_j, 
+                                    lidar2img_i,
+                                    img_org_shape, 
+                                    area_rate
+                                )
+                            points_2d_uvd_patch2d_dict[str(area_rate)] = points_2d_uvd_patch2d
+                    else:
+                        points_2d_uvd_patch2d_dict = None
+                    points_2d_uvd_patch2d_dict_list.append(points_2d_uvd_patch2d_dict)
+
+
+                # ==================== patch 3d overlap ==========================
+
+                points_2d_uvd_patch3d_list_ovlp_noovlp_dict_list = []
+                for j in range(obj_num):
+
+                    if on_the_image_bigger[cam_i, j]:
+                    
+                        points_2d_uvd_patch3d_list_ovlp_noovlp_dict = {}
+                        for area_rate in area_list: # Loop over multiple areas 对多个面积进行循环
+                            if on_two_image[j]:
+                                i1, i2 = get_on_which_image(j, on_the_image)
+                                # 有可能这两个都不是当前的摄像机
+                                # It's possible that neither of these are current cameras
+                                lidar2img_i1 = lidar2img[i1]
+                                lidar2img_i2 = lidar2img[i2]
+                                lidar2img_i = lidar2img[cam_i]
+                                gt_bboxes_center_j = gt_bboxes_center[j]
+                                gt_bboxes_corners_j = gt_bboxes_corners[j]
+                                points_2d_uvd_patch3d = get_3d_patch_overlap(
+                                        gt_bboxes_center_j, 
+                                        gt_bboxes_corners_j, 
+                                        lidar2img_i1, # 这两个用于算面积 These two are used to calculate the area
+                                        lidar2img_i2, # 这两个用于算面积 These two are used to calculate the area
+                                        lidar2img_i,  # 实际用于 算2d位置 的参数 Parameters actually used to calculate 2d position
+                                        img_org_shape, 
+                                        area_rate
+                                    )
+                            else:
+                                lidar2img_i = lidar2img[cam_i]
+                                gt_bboxes_center_j = gt_bboxes_center[j]
+                                gt_bboxes_corners_j = gt_bboxes_corners[j]
+                                points_2d_uvd_patch3d = get_3d_patch_nooverlap(
+                                        gt_bboxes_center_j, 
+                                        gt_bboxes_corners_j, 
+                                        lidar2img_i,
+                                        img_org_shape, 
+                                        area_rate
+                                    )
+                            points_2d_uvd_patch3d_list_ovlp_noovlp_dict[str(area_rate)] = points_2d_uvd_patch3d
+                    else:
+                        points_2d_uvd_patch3d_list_ovlp_noovlp_dict = None
+                        
+                    points_2d_uvd_patch3d_list_ovlp_noovlp_dict_list.append(points_2d_uvd_patch3d_list_ovlp_noovlp_dict)
+            
+
+
+                # ================== patch on box 3d  ===================
+
+                points_2d_uvd_onbox_patches_3d_list_dict_list = []
+                points_2d_uvd_onbox_patches_3d_boxside_saw_dict_list = []
+                patch_onbox_3d_wh_real_dict_list = []
+                for j in range(obj_num):
+
+                    if on_the_image_bigger[cam_i, j]:
+                    
+                        points_2d_uvd_onbox_patches_3d_list_dict = {}
+                        points_2d_uvd_onbox_patches_3d_boxside_saw_dict = {}
+                        patch_onbox_3d_wh_real_dict = {}
+                        
+                        lidar2img_i = lidar2img[cam_i]
+                        gt_bboxes_center_j = gt_bboxes_center[j]
+                        gt_bboxes_corners_j = gt_bboxes_corners[j]
+                        points_2d_uvd_onbox_patches_3d_list, saw, patch_wh_real_0473_list = get_3d_patch_square_onbox_area_list(
+                                gt_bboxes_center_j, 
+                                gt_bboxes_corners_j, 
+                                lidar2img_i,
+                                img_org_shape, 
+                                area_list
+                            )
+                        points_2d_uvd_onbox_patches_3d_list_dict = points_2d_uvd_onbox_patches_3d_list
+                        points_2d_uvd_onbox_patches_3d_boxside_saw_dict = saw
+                        patch_onbox_3d_wh_real_dict = patch_wh_real_0473_list
+                    else:
+                        points_2d_uvd_onbox_patches_3d_list_dict = None
+                        points_2d_uvd_onbox_patches_3d_boxside_saw_dict = None
+                        patch_onbox_3d_wh_real_dict = None
+
+
+                    points_2d_uvd_onbox_patches_3d_list_dict_list.append(points_2d_uvd_onbox_patches_3d_list_dict)
+                    points_2d_uvd_onbox_patches_3d_boxside_saw_dict_list.append(points_2d_uvd_onbox_patches_3d_boxside_saw_dict)
+                    patch_onbox_3d_wh_real_dict_list.append(patch_onbox_3d_wh_real_dict)
+
+
+
+                
+                # ==================== 获取 patch 之外的 信息 ==========================
+                # ==================== Get information other than patch ==========================
+                
+
+                # scene info
+                # instance > anno > sample > scene
+                instance_token = instance_tokens[0]
+                instance = nusc.get('instance', instance_token)
+                first_annotation_token = instance['first_annotation_token']
+                anno = nusc.get('sample_annotation', first_annotation_token) 
+                sample = nusc.get('sample', anno['sample_token'])
+                scene = nusc.get('scene', sample['scene_token'])
+                scene_name = scene['name']
+            
+            
+                info_in_image = {}
+
+                # 一个图像保存的信息文件，里面应该有：
+                # 场景（视频段）
+                # 帧（当前帧）
+                # 物体（所有物体）（注意把可见性写明白）
+                # patch属于物体的哪个面
+
+                # An saved information file for one image, which should contain:
+                # Scene (video segment)
+                # Frame (current frame)
+                # Objects (all objects) (note add visibility clearly)
+                # Which side of the object does the patch belong to
+
+
+                # data_i 神教  data_i is very useful
+                
+                # now_start_flag
+                if data_i in scene_start_idx_list:
+                    now_start_flag = True
+                else:
+                    now_start_flag = False
+                
+                # scene_start_idx
+                if now_start_flag:
+                    scene_start_idx = data_i
+                else:
+                    scene_start_idx = None
+                    for i_ in range(150):
+                        if scene_start_idx_list[i_] < data_i and data_i < scene_start_idx_list[i_+1]:
+                            scene_start_idx = scene_start_idx_list[i_]
+                            break
+                    if scene_start_idx == None:
+                        print('ERROR!!!!!!!!!!!!!!!!!!!')
+                    
+                # scene_now_idx
+                if now_start_flag:
+                    scene_now_idx = 0
+                else:
+                    scene_now_idx = data_i - scene_start_idx
+                
+                
+                scene_info = dict(
+                    # 当前帧的场景名
+                    scene_name=scene_name,
+                    # 当前帧的场景的初始帧idx
+                    scene_idx_start=scene_start_idx,
+                    # 当前帧是场景的第几帧idx
+                    scene_idx_now=scene_now_idx,
+                    scene_start_here_flag=now_start_flag
+                )
+
+                class_names_list_here = []
+                num_obj = len(gt_labels_3d_tensor)
+                for j_obj in range(num_obj):
+                    class_names_list_here.append(class_names_list[gt_labels_3d_tensor[j_obj]])
+
+                uvd_center, _, _ = proj_3d_to_2d(gt_bboxes_3d_center, lidar2img_i, img_org_shape)
+                gt_bboxes_corners_line = gt_bboxes_corners.view(-1,3)
+                uvd_corners_line, _, _ = proj_3d_to_2d(gt_bboxes_corners_line, lidar2img_i, img_org_shape)
+                uvd_corners = uvd_corners_line.view(-1,8,3)
+
+
+
+
+                objects_info = dict(
+                    # 物体类别, 类别名字
+                    class_idx=gt_labels_3d_tensor,
+                    class_name=class_names_list_here,
+                    # 物体 8顶点的2d_uvd位置  1中心的2d_uvd位置
+                    instance_tokens=instance_tokens,
+                    uvd_center=uvd_center,
+                    uvd_corners=uvd_corners,
+                    is_overlap=on_two_image,
+                )
+                
+                
+        
+                patch_info = dict(
+                    scene_info=scene_info,
+                    objects_info=objects_info,
+                    patch_visible_bigger=on_the_image_bigger[cam_i],
+                    patch_2d=dict( 
+                        # 给 class patch 用的
+                        # 1个patch
+                        # 4角规则
+                        # 单张图决定patch大小
+                        # 允许不完整patch粘贴，
+                        patch_vertices=points_2d_uvd_patch2d_dict_list,
+                        # 可见性，逐个点的depth确定，size不再重要，可能一半在图中一半在图外，
+                        # 不再与instance level一致
+                    ),
+                    
+                    patch_3d=dict( 
+                        # 给 overlap patch 用的
+                        # 1个patch
+                        # 4角不规则
+                        patch_vertices=points_2d_uvd_patch3d_list_ovlp_noovlp_dict_list,
+                        # 是否overlap由物体中心判断
+                        #       no overlap: 单张图决定patch大小 
+                        #          overlap: 2 张图决定patch大小 
+                        # 保存时，noovlp用自己的4角，ovlp的用新的重新计算的4角
+                        # 允许不完整patch粘贴
+                        # 可见性，逐个点的depth确定，size不再重要，可能一般在图中一半在图外，
+                        # 不再与instance level一致
+                    ),
+                    
+                    
+
+                    
+                    patch_3d_temporal=dict( 
+                        # 给 temporal patch 用的
+                        # 4个patch
+                        # 4角不规则
+                        patch_vertices=points_2d_uvd_onbox_patches_3d_list_dict_list,
+                        patch_3d_wh=patch_onbox_3d_wh_real_dict_list,
+                        # bbox side 大小 决定patch大小
+                        # 允许不完整patch粘贴
+                        # 可见性，逐个点的depth确定，
+                        # size不再重要，可能一半在图中一半在图外，
+                        # 不再与instance level一致
+                        boxside_visiblie=points_2d_uvd_onbox_patches_3d_boxside_saw_dict_list,
+                    ),
+                )
+                info_in_image_6cam.append(patch_info)
+
+                
+            for cam_i in range(6):
+                # ====================== save pkl ====================
+                save_path_i = save_path_list[cam_i]
+                info_in_image_i = info_in_image_6cam[cam_i]
+                save_path_father_dir = os.path.dirname(save_path_i)
+                os.makedirs(save_path_father_dir, exist_ok=True)
+                with open(save_path_i, 'wb') as f:
+                    pickle.dump(info_in_image_i, f)
+
+        else:
+            # no gt
+            # ====================== save pkl ====================
+            
+            for cam_i in range(6):
+                save_path_i = save_path_list[cam_i]
+                info_in_image_i = 'no gt in these 6 images no gt in these 6 images no gt in these 6 images no gt in these 6 images'
+                save_path_father_dir = os.path.dirname(save_path_i)
+                os.makedirs(save_path_father_dir, exist_ok=True)
+                with open(save_path_i, 'wb') as f:
+                    pickle.dump(info_in_image_i, f)
+            pass
+        prog_bar.update()
+    return results
diff --git a/mmdet3d/apis_common b/mmdet3d/apis_common
new file mode 120000
index 0000000..b44e193
--- /dev/null
+++ b/mmdet3d/apis_common
@@ -0,0 +1 @@
+../../apis_common/
\ No newline at end of file
diff --git a/mmdet3d/datasets/__init__.py b/mmdet3d/datasets/__init__.py
index 61b34ec..1478d13 100644
--- a/mmdet3d/datasets/__init__.py
+++ b/mmdet3d/datasets/__init__.py
@@ -15,6 +15,8 @@ from .semantickitti_dataset import SemanticKITTIDataset
 from .sunrgbd_dataset import SUNRGBDDataset
 from .waymo_dataset import WaymoDataset
 
+from .nuscenes_dataset_with_instancetoken import NuScenesDatasetInstanceToken
+
 __all__ = [
     'KittiDataset', 'GroupSampler', 'DistributedGroupSampler',
     'build_dataloader', 'RepeatFactorDataset', 'DATASETS', 'build_dataset',
diff --git a/mmdet3d/datasets/custom_3d.py b/mmdet3d/datasets/custom_3d.py
index fbd488f..b130669 100644
--- a/mmdet3d/datasets/custom_3d.py
+++ b/mmdet3d/datasets/custom_3d.py
@@ -107,6 +107,10 @@ class Custom3DDataset(Dataset):
             input_dict['ann_info'] = annos
             if self.filter_empty_gt and ~(annos['gt_labels_3d'] != -1).any():
                 return None
+        # zzj : get gt_info when testing
+        else:
+            annos = self.get_ann_info(index)
+            input_dict['ann_info'] = annos
         return input_dict
 
     def pre_pipeline(self, results):
diff --git a/mmdet3d/datasets/nuscenes_dataset.py b/mmdet3d/datasets/nuscenes_dataset.py
index 1ab8e59..eac577a 100644
--- a/mmdet3d/datasets/nuscenes_dataset.py
+++ b/mmdet3d/datasets/nuscenes_dataset.py
@@ -9,7 +9,9 @@ from mmdet.datasets import DATASETS
 from ..core import show_result
 from ..core.bbox import Box3DMode, Coord3DMode, LiDARInstance3DBoxes
 from .custom_3d import Custom3DDataset
-
+import orjson
+import json
+import time
 
 @DATASETS.register_module()
 class NuScenesDataset(Custom3DDataset):
@@ -210,10 +212,22 @@ class NuScenesDataset(Custom3DDataset):
             timestamp=info['timestamp'] / 1e6,
         )
 
+        input_dict.update(
+            dict(
+                lidar2ego_rotation=info['lidar2ego_rotation'],
+                lidar2ego_translation=info['lidar2ego_translation'],
+                ego2global_rotation=info['ego2global_rotation'],
+                ego2global_translation=info['ego2global_translation'],
+            ))
+
+
+
         cam_orders = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT']
         if self.modality['use_camera']:
             image_paths = []
             lidar2img_rts = []
+            lidar2cam_rts = []
+            intrinsics = []
             # for cam_type, cam_info in info['cams'].items():
             for cam_type in cam_orders:
                 cam_info = info['cams'][cam_type]
@@ -230,16 +244,24 @@ class NuScenesDataset(Custom3DDataset):
                 viewpad[:intrinsic.shape[0], :intrinsic.shape[1]] = intrinsic
                 lidar2img_rt = (viewpad @ lidar2cam_rt.T)
                 lidar2img_rts.append(lidar2img_rt)
+                lidar2cam_rts.append(lidar2cam_rt)
+                intrinsics.append(viewpad)
 
             input_dict.update(
                 dict(
                     img_filename=image_paths,
                     lidar2img=lidar2img_rts,
+                    lidar2cam=lidar2cam_rts,
+                    intrinsic=intrinsics
                 ))
 
         if not self.test_mode:
             annos = self.get_ann_info(index)
             input_dict['ann_info'] = annos
+        # zzj : get gt_info when testing
+        else:
+            annos = self.get_ann_info(index)
+            input_dict['ann_info'] = annos
 
         return input_dict
 
@@ -286,10 +308,17 @@ class NuScenesDataset(Custom3DDataset):
             box_dim=gt_bboxes_3d.shape[-1],
             origin=(0.5, 0.5, 0.5)).convert_to(self.box_mode_3d)
 
+        # zzj api add
+        # anns_results = dict(
+        #     gt_bboxes_3d=gt_bboxes_3d,
+        #     gt_labels_3d=gt_labels_3d,
+        #     gt_names=gt_names_3d)
         anns_results = dict(
             gt_bboxes_3d=gt_bboxes_3d,
             gt_labels_3d=gt_labels_3d,
-            gt_names=gt_names_3d)
+            gt_names=gt_names_3d,
+            instance_tokens=[]
+            )
         return anns_results
 
     def _format_bbox(self, results, jsonfile_prefix=None):
@@ -357,8 +386,26 @@ class NuScenesDataset(Custom3DDataset):
 
         mmcv.mkdir_or_exist(jsonfile_prefix)
         res_path = osp.join(jsonfile_prefix, 'results_nusc.json')
-        print('Results writes to', res_path)
-        mmcv.dump(nusc_submissions, res_path)
+        print('Results writes to', res_path,'by orjson')
+        
+        
+        
+        # #生成json文件
+        # start = time.time()
+        # with open(res_path,"w") as f:
+        #     json.dump(nusc_submissions, f)
+        #     print("by json", time.time()-start)
+ 
+
+        start = time.time()
+        # 您通常可以通过write()函数轻松地将字节内容保存到文件中。 以二进制的方式写入：
+        with open(res_path, "wb") as f:    
+            f.write(orjson.dumps(nusc_submissions))
+            print("by orjson in", time.time()-start,'s')
+        
+        
+        
+        # mmcv.dump(nusc_submissions, res_path)
         return res_path
 
     def _evaluate_single(self,
@@ -384,7 +431,7 @@ class NuScenesDataset(Custom3DDataset):
 
         output_dir = osp.join(*osp.split(result_path)[:-1])
         nusc = NuScenes(
-            version=self.version, dataroot=self.data_root, verbose=False)
+            version=self.version, dataroot=self.data_root, verbose=True)
         eval_set_map = {
             'v1.0-mini': 'mini_val',
             'v1.0-trainval': 'val',
diff --git a/mmdet3d/datasets/nuscenes_dataset_backup.py b/mmdet3d/datasets/nuscenes_dataset_backup.py
new file mode 100644
index 0000000..1ab8e59
--- /dev/null
+++ b/mmdet3d/datasets/nuscenes_dataset_backup.py
@@ -0,0 +1,603 @@
+import mmcv
+import numpy as np
+import pyquaternion
+import tempfile
+from nuscenes.utils.data_classes import Box as NuScenesBox
+from os import path as osp
+
+from mmdet.datasets import DATASETS
+from ..core import show_result
+from ..core.bbox import Box3DMode, Coord3DMode, LiDARInstance3DBoxes
+from .custom_3d import Custom3DDataset
+
+
+@DATASETS.register_module()
+class NuScenesDataset(Custom3DDataset):
+    r"""NuScenes Dataset.
+
+    This class serves as the API for experiments on the NuScenes Dataset.
+
+    Please refer to `NuScenes Dataset <https://www.nuscenes.org/download>`_
+    for data downloading.
+
+    Args:
+        ann_file (str): Path of annotation file.
+        pipeline (list[dict], optional): Pipeline used for data processing.
+            Defaults to None.
+        data_root (str): Path of dataset root.
+        classes (tuple[str], optional): Classes used in the dataset.
+            Defaults to None.
+        load_interval (int, optional): Interval of loading the dataset. It is
+            used to uniformly sample the dataset. Defaults to 1.
+        with_velocity (bool, optional): Whether include velocity prediction
+            into the experiments. Defaults to True.
+        modality (dict, optional): Modality to specify the sensor data used
+            as input. Defaults to None.
+        box_type_3d (str, optional): Type of 3D box of this dataset.
+            Based on the `box_type_3d`, the dataset will encapsulate the box
+            to its original format then converted them to `box_type_3d`.
+            Defaults to 'LiDAR' in this dataset. Available options includes.
+            - 'LiDAR': Box in LiDAR coordinates.
+            - 'Depth': Box in depth coordinates, usually for indoor dataset.
+            - 'Camera': Box in camera coordinates.
+        filter_empty_gt (bool, optional): Whether to filter empty GT.
+            Defaults to True.
+        test_mode (bool, optional): Whether the dataset is in test mode.
+            Defaults to False.
+        eval_version (bool, optional): Configuration version of evaluation.
+            Defaults to  'detection_cvpr_2019'.
+        use_valid_flag (bool): Whether to use `use_valid_flag` key in the info
+            file as mask to filter gt_boxes and gt_names. Defaults to False.
+    """
+    NameMapping = {
+        'movable_object.barrier': 'barrier',
+        'vehicle.bicycle': 'bicycle',
+        'vehicle.bus.bendy': 'bus',
+        'vehicle.bus.rigid': 'bus',
+        'vehicle.car': 'car',
+        'vehicle.construction': 'construction_vehicle',
+        'vehicle.motorcycle': 'motorcycle',
+        'human.pedestrian.adult': 'pedestrian',
+        'human.pedestrian.child': 'pedestrian',
+        'human.pedestrian.construction_worker': 'pedestrian',
+        'human.pedestrian.police_officer': 'pedestrian',
+        'movable_object.trafficcone': 'traffic_cone',
+        'vehicle.trailer': 'trailer',
+        'vehicle.truck': 'truck'
+    }
+    DefaultAttribute = {
+        'car': 'vehicle.parked',
+        'pedestrian': 'pedestrian.moving',
+        'trailer': 'vehicle.parked',
+        'truck': 'vehicle.parked',
+        'bus': 'vehicle.moving',
+        'motorcycle': 'cycle.without_rider',
+        'construction_vehicle': 'vehicle.parked',
+        'bicycle': 'cycle.without_rider',
+        'barrier': '',
+        'traffic_cone': '',
+    }
+    AttrMapping = {
+        'cycle.with_rider': 0,
+        'cycle.without_rider': 1,
+        'pedestrian.moving': 2,
+        'pedestrian.standing': 3,
+        'pedestrian.sitting_lying_down': 4,
+        'vehicle.moving': 5,
+        'vehicle.parked': 6,
+        'vehicle.stopped': 7,
+    }
+    AttrMapping_rev = [
+        'cycle.with_rider',
+        'cycle.without_rider',
+        'pedestrian.moving',
+        'pedestrian.standing',
+        'pedestrian.sitting_lying_down',
+        'vehicle.moving',
+        'vehicle.parked',
+        'vehicle.stopped',
+    ]
+    CLASSES = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',
+               'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
+               'barrier')
+
+    def __init__(self,
+                 ann_file,
+                 num_views=6,
+                 pipeline=None,
+                 data_root=None,
+                 classes=None,
+                 load_interval=1,
+                 with_velocity=True,
+                 modality=None,
+                 box_type_3d='LiDAR',
+                 filter_empty_gt=True,
+                 test_mode=False,
+                 eval_version='detection_cvpr_2019',
+                 use_valid_flag=False):
+        self.load_interval = load_interval
+        self.use_valid_flag = use_valid_flag
+        super().__init__(
+            data_root=data_root,
+            ann_file=ann_file,
+            pipeline=pipeline,
+            classes=classes,
+            modality=modality,
+            box_type_3d=box_type_3d,
+            filter_empty_gt=filter_empty_gt,
+            test_mode=test_mode)
+
+        self.num_views = num_views
+        assert self.num_views <= 6
+        self.with_velocity = with_velocity
+        self.eval_version = eval_version
+        from nuscenes.eval.detection.config import config_factory
+        self.eval_detection_configs = config_factory(self.eval_version)
+        if self.modality is None:
+            self.modality = dict(
+                use_camera=False,
+                use_lidar=True,
+                use_radar=False,
+                use_map=False,
+                use_external=False,
+            )
+
+    def get_cat_ids(self, idx):
+        """Get category distribution of single scene.
+
+        Args:
+            idx (int): Index of the data_info.
+
+        Returns:
+            dict[list]: for each category, if the current scene
+                contains such boxes, store a list containing idx,
+                otherwise, store empty list.
+        """
+        info = self.data_infos[idx]
+        if self.use_valid_flag:
+            mask = info['valid_flag']
+            gt_names = set(info['gt_names'][mask])
+        else:
+            gt_names = set(info['gt_names'])
+
+        cat_ids = []
+        for name in gt_names:
+            if name in self.CLASSES:
+                cat_ids.append(self.cat2id[name])
+        return cat_ids
+
+    def load_annotations(self, ann_file):
+        """Load annotations from ann_file.
+
+        Args:
+            ann_file (str): Path of the annotation file.
+
+        Returns:
+            list[dict]: List of annotations sorted by timestamps.
+        """
+        data = mmcv.load(ann_file)
+        data_infos = list(sorted(data['infos'], key=lambda e: e['timestamp']))
+        data_infos = data_infos[::self.load_interval]
+        self.metadata = data['metadata']
+        self.version = self.metadata['version']
+        return data_infos
+
+    def get_data_info(self, index):
+        """Get data info according to the given index.
+
+        Args:
+            index (int): Index of the sample data to get.
+
+        Returns:
+            dict: Data information that will be passed to the data \
+                preprocessing pipelines. It includes the following keys:
+
+                - sample_idx (str): Sample index.
+                - pts_filename (str): Filename of point clouds.
+                - sweeps (list[dict]): Infos of sweeps.
+                - timestamp (float): Sample timestamp.
+                - img_filename (str, optional): Image filename.
+                - lidar2img (list[np.ndarray], optional): Transformations \
+                    from lidar to different cameras.
+                - ann_info (dict): Annotation info.
+        """
+        info = self.data_infos[index]
+        # standard protocal modified from SECOND.Pytorch
+        input_dict = dict(
+            sample_idx=info['token'],
+            pts_filename=info['lidar_path'],
+            sweeps=info['sweeps'],
+            timestamp=info['timestamp'] / 1e6,
+        )
+
+        cam_orders = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT']
+        if self.modality['use_camera']:
+            image_paths = []
+            lidar2img_rts = []
+            # for cam_type, cam_info in info['cams'].items():
+            for cam_type in cam_orders:
+                cam_info = info['cams'][cam_type]
+                image_paths.append(cam_info['data_path'])
+                # obtain lidar to image transformation matrix
+                lidar2cam_r = np.linalg.inv(cam_info['sensor2lidar_rotation'])
+                lidar2cam_t = cam_info[
+                    'sensor2lidar_translation'] @ lidar2cam_r.T
+                lidar2cam_rt = np.eye(4)
+                lidar2cam_rt[:3, :3] = lidar2cam_r.T
+                lidar2cam_rt[3, :3] = -lidar2cam_t
+                intrinsic = cam_info['cam_intrinsic']
+                viewpad = np.eye(4)
+                viewpad[:intrinsic.shape[0], :intrinsic.shape[1]] = intrinsic
+                lidar2img_rt = (viewpad @ lidar2cam_rt.T)
+                lidar2img_rts.append(lidar2img_rt)
+
+            input_dict.update(
+                dict(
+                    img_filename=image_paths,
+                    lidar2img=lidar2img_rts,
+                ))
+
+        if not self.test_mode:
+            annos = self.get_ann_info(index)
+            input_dict['ann_info'] = annos
+
+        return input_dict
+
+    def get_ann_info(self, index):
+        """Get annotation info according to the given index.
+
+        Args:
+            index (int): Index of the annotation data to get.
+
+        Returns:
+            dict: Annotation information consists of the following keys:
+
+                - gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): \
+                    3D ground truth bboxes
+                - gt_labels_3d (np.ndarray): Labels of ground truths.
+                - gt_names (list[str]): Class names of ground truths.
+        """
+        info = self.data_infos[index]
+        # filter out bbox containing no points
+        if self.use_valid_flag:
+            mask = info['valid_flag']
+        else:
+            mask = info['num_lidar_pts'] > 0
+        gt_bboxes_3d = info['gt_boxes'][mask]
+        gt_names_3d = info['gt_names'][mask]
+        gt_labels_3d = []
+        for cat in gt_names_3d:
+            if cat in self.CLASSES:
+                gt_labels_3d.append(self.CLASSES.index(cat))
+            else:
+                gt_labels_3d.append(-1)
+        gt_labels_3d = np.array(gt_labels_3d)
+
+        if self.with_velocity:
+            gt_velocity = info['gt_velocity'][mask]
+            nan_mask = np.isnan(gt_velocity[:, 0])
+            gt_velocity[nan_mask] = [0.0, 0.0]
+            gt_bboxes_3d = np.concatenate([gt_bboxes_3d, gt_velocity], axis=-1)
+
+        # the nuscenes box center is [0.5, 0.5, 0.5], we change it to be
+        # the same as KITTI (0.5, 0.5, 0)
+        gt_bboxes_3d = LiDARInstance3DBoxes(
+            gt_bboxes_3d,
+            box_dim=gt_bboxes_3d.shape[-1],
+            origin=(0.5, 0.5, 0.5)).convert_to(self.box_mode_3d)
+
+        anns_results = dict(
+            gt_bboxes_3d=gt_bboxes_3d,
+            gt_labels_3d=gt_labels_3d,
+            gt_names=gt_names_3d)
+        return anns_results
+
+    def _format_bbox(self, results, jsonfile_prefix=None):
+        """Convert the results to the standard format.
+
+        Args:
+            results (list[dict]): Testing results of the dataset.
+            jsonfile_prefix (str): The prefix of the output jsonfile.
+                You can specify the output directory/filename by
+                modifying the jsonfile_prefix. Default: None.
+
+        Returns:
+            str: Path of the output json file.
+        """
+        nusc_annos = {}
+        mapped_class_names = self.CLASSES
+
+        print('Start to convert detection format...')
+        for sample_id, det in enumerate(mmcv.track_iter_progress(results)):
+            annos = []
+            boxes = output_to_nusc_box(det)
+            sample_token = self.data_infos[sample_id]['token']
+            boxes = lidar_nusc_box_to_global(self.data_infos[sample_id], boxes,
+                                             mapped_class_names,
+                                             self.eval_detection_configs,
+                                             self.eval_version)
+            for i, box in enumerate(boxes):
+                name = mapped_class_names[box.label]
+                if np.sqrt(box.velocity[0]**2 + box.velocity[1]**2) > 0.2:
+                    if name in [
+                            'car',
+                            'construction_vehicle',
+                            'bus',
+                            'truck',
+                            'trailer',
+                    ]:
+                        attr = 'vehicle.moving'
+                    elif name in ['bicycle', 'motorcycle']:
+                        attr = 'cycle.with_rider'
+                    else:
+                        attr = NuScenesDataset.DefaultAttribute[name]
+                else:
+                    if name in ['pedestrian']:
+                        attr = 'pedestrian.standing'
+                    elif name in ['bus']:
+                        attr = 'vehicle.stopped'
+                    else:
+                        attr = NuScenesDataset.DefaultAttribute[name]
+
+                nusc_anno = dict(
+                    sample_token=sample_token,
+                    translation=box.center.tolist(),
+                    size=box.wlh.tolist(),
+                    rotation=box.orientation.elements.tolist(),
+                    velocity=box.velocity[:2].tolist(),
+                    detection_name=name,
+                    detection_score=box.score,
+                    attribute_name=attr)
+                annos.append(nusc_anno)
+            nusc_annos[sample_token] = annos
+        nusc_submissions = {
+            'meta': self.modality,
+            'results': nusc_annos,
+        }
+
+        mmcv.mkdir_or_exist(jsonfile_prefix)
+        res_path = osp.join(jsonfile_prefix, 'results_nusc.json')
+        print('Results writes to', res_path)
+        mmcv.dump(nusc_submissions, res_path)
+        return res_path
+
+    def _evaluate_single(self,
+                         result_path,
+                         logger=None,
+                         metric='bbox',
+                         result_name='pts_bbox'):
+        """Evaluation for a single model in nuScenes protocol.
+
+        Args:
+            result_path (str): Path of the result file.
+            logger (logging.Logger | str | None): Logger used for printing
+                related information during evaluation. Default: None.
+            metric (str): Metric name used for evaluation. Default: 'bbox'.
+            result_name (str): Result name in the metric prefix.
+                Default: 'pts_bbox'.
+
+        Returns:
+            dict: Dictionary of evaluation details.
+        """
+        from nuscenes import NuScenes
+        from nuscenes.eval.detection.evaluate import NuScenesEval
+
+        output_dir = osp.join(*osp.split(result_path)[:-1])
+        nusc = NuScenes(
+            version=self.version, dataroot=self.data_root, verbose=False)
+        eval_set_map = {
+            'v1.0-mini': 'mini_val',
+            'v1.0-trainval': 'val',
+        }
+        nusc_eval = NuScenesEval(
+            nusc,
+            config=self.eval_detection_configs,
+            result_path=result_path,
+            eval_set=eval_set_map[self.version],
+            output_dir=output_dir,
+            verbose=False)
+        nusc_eval.main(render_curves=False)
+
+        # record metrics
+        metrics = mmcv.load(osp.join(output_dir, 'metrics_summary.json'))
+        detail = dict()
+        metric_prefix = f'{result_name}_NuScenes'
+        for name in self.CLASSES:
+            for k, v in metrics['label_aps'][name].items():
+                val = float('{:.4f}'.format(v))
+                detail['{}/{}_AP_dist_{}'.format(metric_prefix, name, k)] = val
+            for k, v in metrics['label_tp_errors'][name].items():
+                val = float('{:.4f}'.format(v))
+                detail['{}/{}_{}'.format(metric_prefix, name, k)] = val
+
+        detail['{}/NDS'.format(metric_prefix)] = metrics['nd_score']
+        detail['{}/mAP'.format(metric_prefix)] = metrics['mean_ap']
+        return detail
+
+    def format_results(self, results, jsonfile_prefix=None):
+        """Format the results to json (standard format for COCO evaluation).
+
+        Args:
+            results (list[dict]): Testing results of the dataset.
+            jsonfile_prefix (str | None): The prefix of json files. It includes
+                the file path and the prefix of filename, e.g., "a/b/prefix".
+                If not specified, a temp file will be created. Default: None.
+
+        Returns:
+            tuple: Returns (result_files, tmp_dir), where `result_files` is a \
+                dict containing the json filepaths, `tmp_dir` is the temporal \
+                directory created for saving json files when \
+                `jsonfile_prefix` is not specified.
+        """
+        assert isinstance(results, list), 'results must be a list'
+        assert len(results) == len(self), (
+            'The length of results is not equal to the dataset len: {} != {}'.
+            format(len(results), len(self)))
+
+        if jsonfile_prefix is None:
+            tmp_dir = tempfile.TemporaryDirectory()
+            jsonfile_prefix = osp.join(tmp_dir.name, 'results')
+        else:
+            tmp_dir = None
+
+        if not isinstance(results[0], dict):
+            result_files = self._format_bbox(results, jsonfile_prefix)
+        else:
+            result_files = dict()
+            for name in results[0]:
+                print(f'\nFormating bboxes of {name}')
+                results_ = [out[name] for out in results]
+                tmp_file_ = osp.join(jsonfile_prefix, name)
+                result_files.update(
+                    {name: self._format_bbox(results_, tmp_file_)})
+        return result_files, tmp_dir
+
+    def evaluate(self,
+                 results,
+                 metric='bbox',
+                 logger=None,
+                 jsonfile_prefix=None,
+                 result_names=['pts_bbox'],
+                 show=False,
+                 out_dir=None):
+        """Evaluation in nuScenes protocol.
+
+        Args:
+            results (list[dict]): Testing results of the dataset.
+            metric (str | list[str]): Metrics to be evaluated.
+            logger (logging.Logger | str | None): Logger used for printing
+                related information during evaluation. Default: None.
+            jsonfile_prefix (str | None): The prefix of json files. It includes
+                the file path and the prefix of filename, e.g., "a/b/prefix".
+                If not specified, a temp file will be created. Default: None.
+            show (bool): Whether to visualize.
+                Default: False.
+            out_dir (str): Path to save the visualization results.
+                Default: None.
+
+        Returns:
+            dict[str, float]: Results of each evaluation metric.
+        """
+        result_files, tmp_dir = self.format_results(results, jsonfile_prefix)
+
+        if isinstance(result_files, dict):
+            results_dict = dict()
+            for name in result_names:
+                print('Evaluating bboxes of {}'.format(name))
+                ret_dict = self._evaluate_single(result_files[name])
+            results_dict.update(ret_dict)
+        elif isinstance(result_files, str):
+            results_dict = self._evaluate_single(result_files)
+
+        if tmp_dir is not None:
+            tmp_dir.cleanup()
+
+        if show:
+            self.show(results, out_dir)
+        return results_dict
+
+    def show(self, results, out_dir):
+        """Results visualization.
+
+        Args:
+            results (list[dict]): List of bounding boxes results.
+            out_dir (str): Output directory of visualization result.
+        """
+        for i, result in enumerate(results):
+            example = self.prepare_test_data(i)
+            points = example['points'][0]._data.numpy()
+            data_info = self.data_infos[i]
+            pts_path = data_info['lidar_path']
+            file_name = osp.split(pts_path)[-1].split('.')[0]
+            # for now we convert points into depth mode
+            points = Coord3DMode.convert_point(points, Coord3DMode.LIDAR,
+                                               Coord3DMode.DEPTH)
+            inds = result['pts_bbox']['scores_3d'] > 0.1
+            gt_bboxes = self.get_ann_info(i)['gt_bboxes_3d'].tensor
+            gt_bboxes = Box3DMode.convert(gt_bboxes, Box3DMode.LIDAR,
+                                          Box3DMode.DEPTH)
+            pred_bboxes = result['pts_bbox']['boxes_3d'][inds].tensor.numpy()
+            pred_bboxes = Box3DMode.convert(pred_bboxes, Box3DMode.LIDAR,
+                                            Box3DMode.DEPTH)
+            show_result(points, gt_bboxes, pred_bboxes, out_dir, file_name)
+
+
+def output_to_nusc_box(detection):
+    """Convert the output to the box class in the nuScenes.
+
+    Args:
+        detection (dict): Detection results.
+
+            - boxes_3d (:obj:`BaseInstance3DBoxes`): Detection bbox.
+            - scores_3d (torch.Tensor): Detection scores.
+            - labels_3d (torch.Tensor): Predicted box labels.
+
+    Returns:
+        list[:obj:`NuScenesBox`]: List of standard NuScenesBoxes.
+    """
+    box3d = detection['boxes_3d']
+    scores = detection['scores_3d'].numpy()
+    labels = detection['labels_3d'].numpy()
+
+    box_gravity_center = box3d.gravity_center.numpy()
+    box_dims = box3d.dims.numpy()
+    box_yaw = box3d.yaw.numpy()
+    # TODO: check whether this is necessary
+    # with dir_offset & dir_limit in the head
+    box_yaw = -box_yaw - np.pi / 2
+
+    box_list = []
+    for i in range(len(box3d)):
+        quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box_yaw[i])
+        velocity = (*box3d.tensor[i, 7:9], 0.0)
+        # velo_val = np.linalg.norm(box3d[i, 7:9])
+        # velo_ori = box3d[i, 6]
+        # velocity = (
+        # velo_val * np.cos(velo_ori), velo_val * np.sin(velo_ori), 0.0)
+        box = NuScenesBox(
+            box_gravity_center[i],
+            box_dims[i],
+            quat,
+            label=labels[i],
+            score=scores[i],
+            velocity=velocity)
+        box_list.append(box)
+    return box_list
+
+
+def lidar_nusc_box_to_global(info,
+                             boxes,
+                             classes,
+                             eval_configs,
+                             eval_version='detection_cvpr_2019'):
+    """Convert the box from ego to global coordinate.
+
+    Args:
+        info (dict): Info for a specific sample data, including the
+            calibration information.
+        boxes (list[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
+        classes (list[str]): Mapped classes in the evaluation.
+        eval_configs (object): Evaluation configuration object.
+        eval_version (str): Evaluation version.
+            Default: 'detection_cvpr_2019'
+
+    Returns:
+        list: List of standard NuScenesBoxes in the global
+            coordinate.
+    """
+    box_list = []
+    for box in boxes:
+        # Move box to ego vehicle coord system
+        box.rotate(pyquaternion.Quaternion(info['lidar2ego_rotation']))
+        box.translate(np.array(info['lidar2ego_translation']))
+        # filter det in ego.
+        cls_range_map = eval_configs.class_range
+        radius = np.linalg.norm(box.center[:2], 2)
+        det_range = cls_range_map[classes[box.label]]
+        if radius > det_range:
+            continue
+        # Move box to global coord system
+        box.rotate(pyquaternion.Quaternion(info['ego2global_rotation']))
+        box.translate(np.array(info['ego2global_translation']))
+        box_list.append(box)
+    return box_list
diff --git a/mmdet3d/datasets/nuscenes_dataset_with_instancetoken.py b/mmdet3d/datasets/nuscenes_dataset_with_instancetoken.py
new file mode 100644
index 0000000..7032f63
--- /dev/null
+++ b/mmdet3d/datasets/nuscenes_dataset_with_instancetoken.py
@@ -0,0 +1,665 @@
+import mmcv
+import numpy as np
+import pyquaternion
+import tempfile
+from nuscenes.utils.data_classes import Box as NuScenesBox
+from os import path as osp
+
+from mmdet.datasets import DATASETS
+from ..core import show_result
+from ..core.bbox import Box3DMode, Coord3DMode, LiDARInstance3DBoxes
+from .custom_3d import Custom3DDataset
+import orjson
+import json
+import time
+
+@DATASETS.register_module()
+class NuScenesDatasetInstanceToken(Custom3DDataset):
+    r"""NuScenes Dataset.
+
+    This class serves as the API for experiments on the NuScenes Dataset.
+
+    Please refer to `NuScenes Dataset <https://www.nuscenes.org/download>`_
+    for data downloading.
+
+    Args:
+        ann_file (str): Path of annotation file.
+        pipeline (list[dict], optional): Pipeline used for data processing.
+            Defaults to None.
+        data_root (str): Path of dataset root.
+        classes (tuple[str], optional): Classes used in the dataset.
+            Defaults to None.
+        load_interval (int, optional): Interval of loading the dataset. It is
+            used to uniformly sample the dataset. Defaults to 1.
+        with_velocity (bool, optional): Whether include velocity prediction
+            into the experiments. Defaults to True.
+        modality (dict, optional): Modality to specify the sensor data used
+            as input. Defaults to None.
+        box_type_3d (str, optional): Type of 3D box of this dataset.
+            Based on the `box_type_3d`, the dataset will encapsulate the box
+            to its original format then converted them to `box_type_3d`.
+            Defaults to 'LiDAR' in this dataset. Available options includes.
+            - 'LiDAR': Box in LiDAR coordinates.
+            - 'Depth': Box in depth coordinates, usually for indoor dataset.
+            - 'Camera': Box in camera coordinates.
+        filter_empty_gt (bool, optional): Whether to filter empty GT.
+            Defaults to True.
+        test_mode (bool, optional): Whether the dataset is in test mode.
+            Defaults to False.
+        eval_version (bool, optional): Configuration version of evaluation.
+            Defaults to  'detection_cvpr_2019'.
+        use_valid_flag (bool): Whether to use `use_valid_flag` key in the info
+            file as mask to filter gt_boxes and gt_names. Defaults to False.
+    """
+    NameMapping = {
+        'movable_object.barrier': 'barrier',
+        'vehicle.bicycle': 'bicycle',
+        'vehicle.bus.bendy': 'bus',
+        'vehicle.bus.rigid': 'bus',
+        'vehicle.car': 'car',
+        'vehicle.construction': 'construction_vehicle',
+        'vehicle.motorcycle': 'motorcycle',
+        'human.pedestrian.adult': 'pedestrian',
+        'human.pedestrian.child': 'pedestrian',
+        'human.pedestrian.construction_worker': 'pedestrian',
+        'human.pedestrian.police_officer': 'pedestrian',
+        'movable_object.trafficcone': 'traffic_cone',
+        'vehicle.trailer': 'trailer',
+        'vehicle.truck': 'truck'
+    }
+    DefaultAttribute = {
+        'car': 'vehicle.parked',
+        'pedestrian': 'pedestrian.moving',
+        'trailer': 'vehicle.parked',
+        'truck': 'vehicle.parked',
+        'bus': 'vehicle.moving',
+        'motorcycle': 'cycle.without_rider',
+        'construction_vehicle': 'vehicle.parked',
+        'bicycle': 'cycle.without_rider',
+        'barrier': '',
+        'traffic_cone': '',
+    }
+    AttrMapping = {
+        'cycle.with_rider': 0,
+        'cycle.without_rider': 1,
+        'pedestrian.moving': 2,
+        'pedestrian.standing': 3,
+        'pedestrian.sitting_lying_down': 4,
+        'vehicle.moving': 5,
+        'vehicle.parked': 6,
+        'vehicle.stopped': 7,
+    }
+    AttrMapping_rev = [
+        'cycle.with_rider',
+        'cycle.without_rider',
+        'pedestrian.moving',
+        'pedestrian.standing',
+        'pedestrian.sitting_lying_down',
+        'vehicle.moving',
+        'vehicle.parked',
+        'vehicle.stopped',
+    ]
+    CLASSES = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',
+               'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
+               'barrier')
+
+    def __init__(self,
+                 ann_file,
+                 num_views=6,
+                 pipeline=None,
+                 data_root=None,
+                 classes=None,
+                 load_interval=1,
+                 with_velocity=True,
+                 modality=None,
+                 box_type_3d='LiDAR',
+                 filter_empty_gt=True,
+                 test_mode=False,
+                 eval_version='detection_cvpr_2019',
+                 use_valid_flag=False):
+        self.load_interval = load_interval
+        self.use_valid_flag = use_valid_flag
+        super().__init__(
+            data_root=data_root,
+            ann_file=ann_file,
+            pipeline=pipeline,
+            classes=classes,
+            modality=modality,
+            box_type_3d=box_type_3d,
+            filter_empty_gt=filter_empty_gt,
+            test_mode=test_mode)
+
+        self.num_views = num_views
+        assert self.num_views <= 6
+        self.with_velocity = with_velocity
+        self.eval_version = eval_version
+        from nuscenes.eval.detection.config import config_factory
+        self.eval_detection_configs = config_factory(self.eval_version)
+        if self.modality is None:
+            self.modality = dict(
+                use_camera=False,
+                use_lidar=True,
+                use_radar=False,
+                use_map=False,
+                use_external=False,
+            )
+        from nuscenes import NuScenes
+        self.nusc = NuScenes(
+            version=self.version, dataroot=self.data_root, verbose=True)
+
+    def get_cat_ids(self, idx):
+        """Get category distribution of single scene.
+
+        Args:
+            idx (int): Index of the data_info.
+
+        Returns:
+            dict[list]: for each category, if the current scene
+                contains such boxes, store a list containing idx,
+                otherwise, store empty list.
+        """
+        info = self.data_infos[idx]
+        if self.use_valid_flag:
+            mask = info['valid_flag']
+            gt_names = set(info['gt_names'][mask])
+        else:
+            gt_names = set(info['gt_names'])
+
+        cat_ids = []
+        for name in gt_names:
+            if name in self.CLASSES:
+                cat_ids.append(self.cat2id[name])
+        return cat_ids
+
+    def load_annotations(self, ann_file):
+        """Load annotations from ann_file.
+
+        Args:
+            ann_file (str): Path of the annotation file.
+
+        Returns:
+            list[dict]: List of annotations sorted by timestamps.
+        """
+        data = mmcv.load(ann_file)
+        data_infos = list(sorted(data['infos'], key=lambda e: e['timestamp']))
+        data_infos = data_infos[::self.load_interval]
+        self.metadata = data['metadata']
+        self.version = self.metadata['version']
+        return data_infos
+
+    def get_data_info(self, index):
+        """Get data info according to the given index.
+
+        Args:
+            index (int): Index of the sample data to get.
+
+        Returns:
+            dict: Data information that will be passed to the data \
+                preprocessing pipelines. It includes the following keys:
+
+                - sample_idx (str): Sample index.
+                - pts_filename (str): Filename of point clouds.
+                - sweeps (list[dict]): Infos of sweeps.
+                - timestamp (float): Sample timestamp.
+                - img_filename (str, optional): Image filename.
+                - lidar2img (list[np.ndarray], optional): Transformations \
+                    from lidar to different cameras.
+                - ann_info (dict): Annotation info.
+        """
+        info = self.data_infos[index]
+        # standard protocal modified from SECOND.Pytorch
+        input_dict = dict(
+            sample_idx=info['token'],
+            pts_filename=info['lidar_path'],
+            sweeps=info['sweeps'],
+            timestamp=info['timestamp'] / 1e6,
+        )
+
+        input_dict.update(
+            dict(
+                lidar2ego_rotation=info['lidar2ego_rotation'],
+                lidar2ego_translation=info['lidar2ego_translation'],
+                ego2global_rotation=info['ego2global_rotation'],
+                ego2global_translation=info['ego2global_translation'],
+            ))
+
+
+
+        cam_orders = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT']
+        if self.modality['use_camera']:
+            image_paths = []
+            lidar2img_rts = []
+            lidar2cam_rts = []
+            intrinsics = []
+            # for cam_type, cam_info in info['cams'].items():
+            for cam_type in cam_orders:
+                cam_info = info['cams'][cam_type]
+                image_paths.append(cam_info['data_path'])
+                # obtain lidar to image transformation matrix
+                lidar2cam_r = np.linalg.inv(cam_info['sensor2lidar_rotation'])
+                lidar2cam_t = cam_info[
+                    'sensor2lidar_translation'] @ lidar2cam_r.T
+                lidar2cam_rt = np.eye(4)
+                lidar2cam_rt[:3, :3] = lidar2cam_r.T
+                lidar2cam_rt[3, :3] = -lidar2cam_t
+                intrinsic = cam_info['cam_intrinsic']
+                viewpad = np.eye(4)
+                viewpad[:intrinsic.shape[0], :intrinsic.shape[1]] = intrinsic
+                lidar2img_rt = (viewpad @ lidar2cam_rt.T)
+                lidar2img_rts.append(lidar2img_rt)
+                lidar2cam_rts.append(lidar2cam_rt)
+                intrinsics.append(viewpad)
+
+            input_dict.update(
+                dict(
+                    img_filename=image_paths,
+                    lidar2img=lidar2img_rts,
+                    lidar2cam=lidar2cam_rts,
+                    intrinsic=intrinsics
+                ))
+
+        if not self.test_mode:
+            annos = self.get_ann_info(index)
+            input_dict['ann_info'] = annos
+        # zzj : get gt_info when testing
+        else:
+            annos = self.get_ann_info(index)
+            input_dict['ann_info'] = annos
+
+        return input_dict
+
+    def get_ann_info(self, index):
+        """Get annotation info according to the given index.
+
+        Args:
+            index (int): Index of the annotation data to get.
+
+        Returns:
+            dict: Annotation information consists of the following keys:
+
+                - gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): \
+                    3D ground truth bboxes
+                - gt_labels_3d (np.ndarray): Labels of ground truths.
+                - gt_names (list[str]): Class names of ground truths.
+        """
+        info = self.data_infos[index]
+        # filter out bbox containing no points
+        if self.use_valid_flag:
+            mask = info['valid_flag']
+        else:
+            mask = info['num_lidar_pts'] > 0
+        
+        gt_bboxes_3d = info['gt_boxes'][mask]
+        gt_names_3d = info['gt_names'][mask]
+        
+        # zzj api add instance_token
+        anns_token_list = self.nusc.get('sample', info['token'])['anns']
+        instance_token_list = []
+        for i in range(len(anns_token_list)):
+            anns_token = anns_token_list[i]
+            if mask[i]:
+                instance_anno = self.nusc.get('sample_annotation', anns_token)
+                instance_token = instance_anno['instance_token']
+                instance_token_list.append(instance_token)
+
+        gt_labels_3d = []
+        for cat in gt_names_3d:
+            if cat in self.CLASSES:
+                gt_labels_3d.append(self.CLASSES.index(cat))
+            else:
+                gt_labels_3d.append(-1)
+        gt_labels_3d = np.array(gt_labels_3d)
+
+        if self.with_velocity:
+            gt_velocity = info['gt_velocity'][mask]
+            nan_mask = np.isnan(gt_velocity[:, 0])
+            gt_velocity[nan_mask] = [0.0, 0.0]
+            gt_bboxes_3d = np.concatenate([gt_bboxes_3d, gt_velocity], axis=-1)
+
+        # the nuscenes box center is [0.5, 0.5, 0.5], we change it to be
+        # the same as KITTI (0.5, 0.5, 0)
+        gt_bboxes_3d = LiDARInstance3DBoxes(
+            gt_bboxes_3d,
+            box_dim=gt_bboxes_3d.shape[-1],
+            origin=(0.5, 0.5, 0.5)).convert_to(self.box_mode_3d)
+
+        # zzj api add
+        # anns_results = dict(
+        #     gt_bboxes_3d=gt_bboxes_3d,
+        #     gt_labels_3d=gt_labels_3d,
+        #     gt_names=gt_names_3d)
+        anns_results = dict(
+            gt_bboxes_3d=gt_bboxes_3d,
+            gt_labels_3d=gt_labels_3d,
+            gt_names=gt_names_3d,
+            instance_tokens=instance_token_list
+            )
+        return anns_results
+
+    def _format_bbox(self, results, jsonfile_prefix=None):
+        """Convert the results to the standard format.
+
+        Args:
+            results (list[dict]): Testing results of the dataset.
+            jsonfile_prefix (str): The prefix of the output jsonfile.
+                You can specify the output directory/filename by
+                modifying the jsonfile_prefix. Default: None.
+
+        Returns:
+            str: Path of the output json file.
+        """
+        nusc_annos = {}
+        mapped_class_names = self.CLASSES
+
+        print('Start to convert detection format...')
+        for sample_id, det in enumerate(mmcv.track_iter_progress(results)):
+            annos = []
+            boxes = output_to_nusc_box(det)
+            sample_token = self.data_infos[sample_id]['token']
+            boxes = lidar_nusc_box_to_global(self.data_infos[sample_id], boxes,
+                                             mapped_class_names,
+                                             self.eval_detection_configs,
+                                             self.eval_version)
+            for i, box in enumerate(boxes):
+                name = mapped_class_names[box.label]
+                if np.sqrt(box.velocity[0]**2 + box.velocity[1]**2) > 0.2:
+                    if name in [
+                            'car',
+                            'construction_vehicle',
+                            'bus',
+                            'truck',
+                            'trailer',
+                    ]:
+                        attr = 'vehicle.moving'
+                    elif name in ['bicycle', 'motorcycle']:
+                        attr = 'cycle.with_rider'
+                    else:
+                        attr = NuScenesDataset.DefaultAttribute[name]
+                else:
+                    if name in ['pedestrian']:
+                        attr = 'pedestrian.standing'
+                    elif name in ['bus']:
+                        attr = 'vehicle.stopped'
+                    else:
+                        attr = NuScenesDataset.DefaultAttribute[name]
+
+                nusc_anno = dict(
+                    sample_token=sample_token,
+                    translation=box.center.tolist(),
+                    size=box.wlh.tolist(),
+                    rotation=box.orientation.elements.tolist(),
+                    velocity=box.velocity[:2].tolist(),
+                    detection_name=name,
+                    detection_score=box.score,
+                    attribute_name=attr)
+                annos.append(nusc_anno)
+            nusc_annos[sample_token] = annos
+        nusc_submissions = {
+            'meta': self.modality,
+            'results': nusc_annos,
+        }
+
+        mmcv.mkdir_or_exist(jsonfile_prefix)
+        res_path = osp.join(jsonfile_prefix, 'results_nusc.json')
+        print('Results writes to', res_path,'by orjson')
+        
+        
+        
+        # #生成json文件
+        # start = time.time()
+        # with open(res_path,"w") as f:
+        #     json.dump(nusc_submissions, f)
+        #     print("by json", time.time()-start)
+ 
+
+        start = time.time()
+        # 您通常可以通过write()函数轻松地将字节内容保存到文件中。 以二进制的方式写入：
+        with open(res_path, "wb") as f:    
+            f.write(orjson.dumps(nusc_submissions))
+            print("by orjson in", time.time()-start,'s')
+        
+        
+        
+        # mmcv.dump(nusc_submissions, res_path)
+        return res_path
+
+    def _evaluate_single(self,
+                         result_path,
+                         logger=None,
+                         metric='bbox',
+                         result_name='pts_bbox'):
+        """Evaluation for a single model in nuScenes protocol.
+
+        Args:
+            result_path (str): Path of the result file.
+            logger (logging.Logger | str | None): Logger used for printing
+                related information during evaluation. Default: None.
+            metric (str): Metric name used for evaluation. Default: 'bbox'.
+            result_name (str): Result name in the metric prefix.
+                Default: 'pts_bbox'.
+
+        Returns:
+            dict: Dictionary of evaluation details.
+        """
+        from nuscenes import NuScenes
+        from nuscenes.eval.detection.evaluate import NuScenesEval
+
+        output_dir = osp.join(*osp.split(result_path)[:-1])
+        nusc = NuScenes(
+            version=self.version, dataroot=self.data_root, verbose=True)
+        eval_set_map = {
+            'v1.0-mini': 'mini_val',
+            'v1.0-trainval': 'val',
+        }
+        nusc_eval = NuScenesEval(
+            nusc,
+            config=self.eval_detection_configs,
+            result_path=result_path,
+            eval_set=eval_set_map[self.version],
+            output_dir=output_dir,
+            verbose=False)
+        nusc_eval.main(render_curves=False)
+
+        # record metrics
+        metrics = mmcv.load(osp.join(output_dir, 'metrics_summary.json'))
+        detail = dict()
+        metric_prefix = f'{result_name}_NuScenes'
+        for name in self.CLASSES:
+            for k, v in metrics['label_aps'][name].items():
+                val = float('{:.4f}'.format(v))
+                detail['{}/{}_AP_dist_{}'.format(metric_prefix, name, k)] = val
+            for k, v in metrics['label_tp_errors'][name].items():
+                val = float('{:.4f}'.format(v))
+                detail['{}/{}_{}'.format(metric_prefix, name, k)] = val
+
+        detail['{}/NDS'.format(metric_prefix)] = metrics['nd_score']
+        detail['{}/mAP'.format(metric_prefix)] = metrics['mean_ap']
+        return detail
+
+    def format_results(self, results, jsonfile_prefix=None):
+        """Format the results to json (standard format for COCO evaluation).
+
+        Args:
+            results (list[dict]): Testing results of the dataset.
+            jsonfile_prefix (str | None): The prefix of json files. It includes
+                the file path and the prefix of filename, e.g., "a/b/prefix".
+                If not specified, a temp file will be created. Default: None.
+
+        Returns:
+            tuple: Returns (result_files, tmp_dir), where `result_files` is a \
+                dict containing the json filepaths, `tmp_dir` is the temporal \
+                directory created for saving json files when \
+                `jsonfile_prefix` is not specified.
+        """
+        assert isinstance(results, list), 'results must be a list'
+        assert len(results) == len(self), (
+            'The length of results is not equal to the dataset len: {} != {}'.
+            format(len(results), len(self)))
+
+        if jsonfile_prefix is None:
+            tmp_dir = tempfile.TemporaryDirectory()
+            jsonfile_prefix = osp.join(tmp_dir.name, 'results')
+        else:
+            tmp_dir = None
+
+        if not isinstance(results[0], dict):
+            result_files = self._format_bbox(results, jsonfile_prefix)
+        else:
+            result_files = dict()
+            for name in results[0]:
+                print(f'\nFormating bboxes of {name}')
+                results_ = [out[name] for out in results]
+                tmp_file_ = osp.join(jsonfile_prefix, name)
+                result_files.update(
+                    {name: self._format_bbox(results_, tmp_file_)})
+        return result_files, tmp_dir
+
+    def evaluate(self,
+                 results,
+                 metric='bbox',
+                 logger=None,
+                 jsonfile_prefix=None,
+                 result_names=['pts_bbox'],
+                 show=False,
+                 out_dir=None):
+        """Evaluation in nuScenes protocol.
+
+        Args:
+            results (list[dict]): Testing results of the dataset.
+            metric (str | list[str]): Metrics to be evaluated.
+            logger (logging.Logger | str | None): Logger used for printing
+                related information during evaluation. Default: None.
+            jsonfile_prefix (str | None): The prefix of json files. It includes
+                the file path and the prefix of filename, e.g., "a/b/prefix".
+                If not specified, a temp file will be created. Default: None.
+            show (bool): Whether to visualize.
+                Default: False.
+            out_dir (str): Path to save the visualization results.
+                Default: None.
+
+        Returns:
+            dict[str, float]: Results of each evaluation metric.
+        """
+        result_files, tmp_dir = self.format_results(results, jsonfile_prefix)
+
+        if isinstance(result_files, dict):
+            results_dict = dict()
+            for name in result_names:
+                print('Evaluating bboxes of {}'.format(name))
+                ret_dict = self._evaluate_single(result_files[name])
+            results_dict.update(ret_dict)
+        elif isinstance(result_files, str):
+            results_dict = self._evaluate_single(result_files)
+
+        if tmp_dir is not None:
+            tmp_dir.cleanup()
+
+        if show:
+            self.show(results, out_dir)
+        return results_dict
+
+    def show(self, results, out_dir):
+        """Results visualization.
+
+        Args:
+            results (list[dict]): List of bounding boxes results.
+            out_dir (str): Output directory of visualization result.
+        """
+        for i, result in enumerate(results):
+            example = self.prepare_test_data(i)
+            points = example['points'][0]._data.numpy()
+            data_info = self.data_infos[i]
+            pts_path = data_info['lidar_path']
+            file_name = osp.split(pts_path)[-1].split('.')[0]
+            # for now we convert points into depth mode
+            points = Coord3DMode.convert_point(points, Coord3DMode.LIDAR,
+                                               Coord3DMode.DEPTH)
+            inds = result['pts_bbox']['scores_3d'] > 0.1
+            gt_bboxes = self.get_ann_info(i)['gt_bboxes_3d'].tensor
+            gt_bboxes = Box3DMode.convert(gt_bboxes, Box3DMode.LIDAR,
+                                          Box3DMode.DEPTH)
+            pred_bboxes = result['pts_bbox']['boxes_3d'][inds].tensor.numpy()
+            pred_bboxes = Box3DMode.convert(pred_bboxes, Box3DMode.LIDAR,
+                                            Box3DMode.DEPTH)
+            show_result(points, gt_bboxes, pred_bboxes, out_dir, file_name)
+
+
+def output_to_nusc_box(detection):
+    """Convert the output to the box class in the nuScenes.
+
+    Args:
+        detection (dict): Detection results.
+
+            - boxes_3d (:obj:`BaseInstance3DBoxes`): Detection bbox.
+            - scores_3d (torch.Tensor): Detection scores.
+            - labels_3d (torch.Tensor): Predicted box labels.
+
+    Returns:
+        list[:obj:`NuScenesBox`]: List of standard NuScenesBoxes.
+    """
+    box3d = detection['boxes_3d']
+    scores = detection['scores_3d'].numpy()
+    labels = detection['labels_3d'].numpy()
+
+    box_gravity_center = box3d.gravity_center.numpy()
+    box_dims = box3d.dims.numpy()
+    box_yaw = box3d.yaw.numpy()
+    # TODO: check whether this is necessary
+    # with dir_offset & dir_limit in the head
+    box_yaw = -box_yaw - np.pi / 2
+
+    box_list = []
+    for i in range(len(box3d)):
+        quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box_yaw[i])
+        velocity = (*box3d.tensor[i, 7:9], 0.0)
+        # velo_val = np.linalg.norm(box3d[i, 7:9])
+        # velo_ori = box3d[i, 6]
+        # velocity = (
+        # velo_val * np.cos(velo_ori), velo_val * np.sin(velo_ori), 0.0)
+        box = NuScenesBox(
+            box_gravity_center[i],
+            box_dims[i],
+            quat,
+            label=labels[i],
+            score=scores[i],
+            velocity=velocity)
+        box_list.append(box)
+    return box_list
+
+
+def lidar_nusc_box_to_global(info,
+                             boxes,
+                             classes,
+                             eval_configs,
+                             eval_version='detection_cvpr_2019'):
+    """Convert the box from ego to global coordinate.
+
+    Args:
+        info (dict): Info for a specific sample data, including the
+            calibration information.
+        boxes (list[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
+        classes (list[str]): Mapped classes in the evaluation.
+        eval_configs (object): Evaluation configuration object.
+        eval_version (str): Evaluation version.
+            Default: 'detection_cvpr_2019'
+
+    Returns:
+        list: List of standard NuScenesBoxes in the global
+            coordinate.
+    """
+    box_list = []
+    for box in boxes:
+        # Move box to ego vehicle coord system
+        box.rotate(pyquaternion.Quaternion(info['lidar2ego_rotation']))
+        box.translate(np.array(info['lidar2ego_translation']))
+        # filter det in ego.
+        cls_range_map = eval_configs.class_range
+        radius = np.linalg.norm(box.center[:2], 2)
+        det_range = cls_range_map[classes[box.label]]
+        if radius > det_range:
+            continue
+        # Move box to global coord system
+        box.rotate(pyquaternion.Quaternion(info['ego2global_rotation']))
+        box.translate(np.array(info['ego2global_translation']))
+        box_list.append(box)
+    return box_list
diff --git a/mmdet3d/datasets/pipelines/loading.py b/mmdet3d/datasets/pipelines/loading.py
index 8c08478..888a685 100644
--- a/mmdet3d/datasets/pipelines/loading.py
+++ b/mmdet3d/datasets/pipelines/loading.py
@@ -481,6 +481,8 @@ class LoadMultiViewImageFromFiles(object):
             std=np.ones(num_channels, dtype=np.float32),
             to_rgb=False)
         results['img_fields'] = ['img']
+        # directly use readed img, avoid read img twice
+        results['img_org'] = img
         return results
 
     def __repr__(self):
@@ -885,6 +887,9 @@ class LoadAnnotations3D(LoadAnnotations):
             dict: The dict containing loaded 3D bounding box annotations.
         """
         results['gt_bboxes_3d'] = results['ann_info']['gt_bboxes_3d']
+        # zzj : add for instance_token anno
+        if 'instance_tokens' in results['ann_info']:
+            results['instance_tokens'] = results['ann_info']['instance_tokens']
         results['bbox3d_fields'].append('gt_bboxes_3d')
         return results
 
diff --git a/mmdet3d/models/detectors/transfusion.py b/mmdet3d/models/detectors/transfusion.py
index bf5686d..f62964d 100644
--- a/mmdet3d/models/detectors/transfusion.py
+++ b/mmdet3d/models/detectors/transfusion.py
@@ -71,7 +71,7 @@ class TransFusionDetector(MVXTwoStageDetector):
             x = self.pts_neck(x)
         return x
 
-    @torch.no_grad()
+    # @torch.no_grad()
     @force_fp32()
     def voxelize(self, points):
         """Apply dynamic voxelization to points.
diff --git a/mmdet3d/models/middle_encoders/sparse_encoder.py b/mmdet3d/models/middle_encoders/sparse_encoder.py
index c49d47d..90a2265 100644
--- a/mmdet3d/models/middle_encoders/sparse_encoder.py
+++ b/mmdet3d/models/middle_encoders/sparse_encoder.py
@@ -1,4 +1,4 @@
-from mmcv.runner import auto_fp16
+from mmcv.runner import auto_fp16, force_fp32
 from torch import nn as nn
 
 from mmdet3d.ops import SparseBasicBlock, make_sparse_convmodule
@@ -92,7 +92,8 @@ class SparseEncoder(nn.Module):
             indice_key='spconv_down2',
             conv_type='SparseConv3d')
 
-    @auto_fp16(apply_to=('voxel_features', ))
+    # @auto_fp16(apply_to=('voxel_features', ))
+    @force_fp32(apply_to=('voxel_features'))
     def forward(self, voxel_features, coors, batch_size):
         """Forward of SparseEncoder.
 
@@ -106,7 +107,7 @@ class SparseEncoder(nn.Module):
             dict: Backbone features.
         """
         coors = coors.int()
-        input_sp_tensor = spconv.SparseConvTensor(voxel_features, coors,
+        input_sp_tensor = spconv.SparseConvTensor(voxel_features, coors, # voxel_features 是 16 的 正常是32的
                                                   self.sparse_shape,
                                                   batch_size)
         x = self.conv_input(input_sp_tensor)
diff --git a/mmdet3d/ops/voxel/src/scatter_points_cuda.cu b/mmdet3d/ops/voxel/src/scatter_points_cuda.cu
index 8d888b0..2ed1869 100644
--- a/mmdet3d/ops/voxel/src/scatter_points_cuda.cu
+++ b/mmdet3d/ops/voxel/src/scatter_points_cuda.cu
@@ -77,61 +77,17 @@ __device__ __forceinline__ static void reduceAdd(double *address, double val) {
 }
 #endif
 
-template <typename T_int>
-__global__ void coors_id_kernel(const T_int *coors, const T_int *dim,
-                                int64_t *coors_id, const int num_input,
-                                const int NDim) {
-  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
-       x += gridDim.x * blockDim.x) {
-    const T_int *coor_x = coors + x * NDim;
-    auto coor_id = 0;
-    for (int i = 0; i < NDim && coor_id != -1; i++) {
-      coor_id *= dim[i];
-      auto t = static_cast<int64_t>(coor_x[i]);
-      coor_id = (t < 0) ? -1 : coor_id + t;
-    }
-    coors_id[x] = coor_id;
-  }
-}
-
-template <typename T_int>
-__global__ void coors_map_init_kernel(const int64_t *coors_id,
-                                      const T_int *coors_id_argsort,
-                                      int32_t *coors_map, const int num_input) {
-  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
-       x += gridDim.x * blockDim.x) {
-    auto here = coors_id[coors_id_argsort[x]];
-    if (x == 0) {
-      if (here == -1) {  // there is invalid points
-        coors_map[0] = -1;
-      } else {
-        coors_map[0] = 0;
-      }
-      continue;
-    }
-    auto left = coors_id[coors_id_argsort[x - 1]];
-    coors_map[x] = (left < here) ? 1 : 0;
-  }
-}
-
-template <typename T, typename T_int>
-__global__ void feats_reduce_kernel(
-    const T *feats, const T_int *coors, int32_t *coors_map,
-    int32_t *reduce_count,  // shall be 0 at initialization
-    T *reduced_feats,       // shall be 0 at initialization
-    T_int *out_coors, const int num_input, const int num_feats, const int NDim,
-    const reduce_t reduce_type) {
+template <typename T>
+__global__ void
+feats_reduce_kernel(const T *feats, const int32_t *coors_map,
+                    T *reduced_feats, // shall be 0 at initialization
+                    const int num_input, const int num_feats,
+                    const reduce_t reduce_type) {
   for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
        x += gridDim.x * blockDim.x) {
     int32_t reduce_to = coors_map[x];
     if (reduce_to == -1) continue;
 
-    const T_int *coors_offset = coors + x * NDim;
-    T_int *out_coors_offset = out_coors + reduce_to * NDim;
-    for (int i = 0; i < NDim; i++) {
-      out_coors_offset[i] = coors_offset[i];
-    }
-
     const T *feats_offset = feats + x * num_feats;
     T *reduced_feats_offset = reduced_feats + reduce_to * num_feats;
     if (reduce_type == reduce_t::MAX) {
@@ -139,9 +95,6 @@ __global__ void feats_reduce_kernel(
         reduceMax(&reduced_feats_offset[i], feats_offset[i]);
       }
     } else {
-      if (reduce_type == reduce_t::MEAN) {
-        atomicAdd(&reduce_count[reduce_to], static_cast<int32_t>(1));
-      }
       for (int i = 0; i < num_feats; i++) {
         reduceAdd(&reduced_feats_offset[i], feats_offset[i]);
       }
@@ -233,78 +186,53 @@ std::vector<at::Tensor> dynamic_point_to_voxel_forward_gpu(
   CHECK_INPUT(feats);
   CHECK_INPUT(coors);
 
-  const int NDim = coors.size(1);
   const int num_input = feats.size(0);
   const int num_feats = feats.size(1);
 
-  auto coors_id = at::empty({num_input}, coors.options().dtype(torch::kInt64));
-  auto coor_space_dim = std::get<0>(coors.max(0)) + 1;
-  auto coors_map_sorted =
-      at::empty({num_input}, coors.options().dtype(torch::kInt32));
-  auto coors_map = at::empty({num_input}, coors.options().dtype(torch::kInt32));
-  auto num_coors = at::zeros({1}, coors.options().dtype(torch::kInt32));
-
-  AT_DISPATCH_INTEGRAL_TYPES(
-      coors.scalar_type(), "coors_id_kernel", ([&] {
-        dim3 blocks(std::min(at::cuda::ATenCeilDiv(num_input, threadsPerBlock),
-                             maxGridDim));
-        dim3 threads(threadsPerBlock);
-        coors_id_kernel<<<blocks, threads>>>(
-            coors.data_ptr<scalar_t>(), coor_space_dim.data_ptr<scalar_t>(),
-            coors_id.data_ptr<int64_t>(), num_input, NDim);
-      }));
-  AT_CUDA_CHECK(cudaGetLastError());
+  if (num_input == 0)
+    return {feats.clone().detach(),
+            coors.clone().detach(),
+            coors.new_empty({0}, torch::kInt32),
+            coors.new_empty({0}, torch::kInt32)};
 
-  auto coors_id_argsort = coors_id.argsort();
-
-  AT_DISPATCH_INTEGRAL_TYPES(
-      coors_id_argsort.scalar_type(), "coors_map_init_kernel", ([&] {
-        dim3 blocks(std::min(at::cuda::ATenCeilDiv(num_input, threadsPerBlock),
-                             maxGridDim));
-        dim3 threads(threadsPerBlock);
-        coors_map_init_kernel<<<blocks, threads>>>(
-            coors_id.data_ptr<int64_t>(), coors_id_argsort.data_ptr<scalar_t>(),
-            coors_map_sorted.data_ptr<int32_t>(), num_input);
-      }));
-  AT_CUDA_CHECK(cudaGetLastError());
+  at::Tensor out_coors;
+  at::Tensor coors_map;
+  at::Tensor reduce_count;
+
+  auto coors_clean = coors.masked_fill(coors.lt(0).any(-1, true), -1);
+
+  std::tie(out_coors, coors_map, reduce_count) =
+      at::unique_dim(coors_clean, 0, true, true, true);
 
-  coors_map_sorted = coors_map_sorted.cumsum(0, torch::kInt32);
-  coors_map.index_put_(coors_id_argsort, coors_map_sorted);
+  if (out_coors.index({0, 0}).lt(0).item<bool>()) {
+    // the first element of out_coors (-1,-1,-1) and should be removed
+    out_coors = out_coors.slice(0, 1);
+    reduce_count = reduce_count.slice(0, 1);
+    coors_map = coors_map - 1;
+  }
+
+  coors_map = coors_map.to(torch::kInt32);
+  reduce_count = reduce_count.to(torch::kInt32);
 
-  const int num_coors_cpu =
-      coors_map_sorted[-1].cpu().data_ptr<int32_t>()[0] + 1;
-  auto out_coors = at::empty({num_coors_cpu, NDim}, coors.options());
-  auto reduced_feats = at::empty({num_coors_cpu, num_feats}, feats.options());
-  auto reduce_count =
-      at::zeros({num_coors_cpu}, coors.options().dtype(torch::kInt32));
+  auto reduced_feats =
+      at::empty({out_coors.size(0), num_feats}, feats.options());
 
   AT_DISPATCH_FLOATING_TYPES(
       feats.scalar_type(), "feats_reduce_kernel", ([&] {
-        using F_t = scalar_t;
-        AT_DISPATCH_INTEGRAL_TYPES(
-            coors.scalar_type(), "feats_reduce_kernel", ([&] {
-              using I_t = scalar_t;
-
-              if (reduce_type == reduce_t::MAX)
-                reduced_feats.fill_(-std::numeric_limits<F_t>::infinity());
-              else
-                reduced_feats.fill_(static_cast<F_t>(0));
-
-              dim3 blocks(
-                  std::min(at::cuda::ATenCeilDiv(num_input, threadsPerBlock),
-                           maxGridDim));
-              dim3 threads(threadsPerBlock);
-              feats_reduce_kernel<<<blocks, threads>>>(
-                  feats.data_ptr<F_t>(), coors.data_ptr<I_t>(),
-                  coors_map.data_ptr<int32_t>(),
-                  reduce_count.data_ptr<int32_t>(),
-                  reduced_feats.data_ptr<F_t>(), out_coors.data_ptr<I_t>(),
-                  num_input, num_feats, NDim, reduce_type);
-              if (reduce_type == reduce_t::MEAN)
-                reduced_feats /=
-                    reduce_count.unsqueeze(-1).to(reduced_feats.dtype());
-            }));
-      }));
+    if (reduce_type == reduce_t::MAX)
+      reduced_feats.fill_(-std::numeric_limits<scalar_t>::infinity());
+    else
+      reduced_feats.fill_(static_cast<scalar_t>(0));
+
+    dim3 blocks(std::min(at::cuda::ATenCeilDiv(num_input, threadsPerBlock),
+                         maxGridDim));
+    dim3 threads(threadsPerBlock);
+    feats_reduce_kernel<<<blocks, threads>>>(
+        feats.data_ptr<scalar_t>(), coors_map.data_ptr<int32_t>(),
+        reduced_feats.data_ptr<scalar_t>(), num_input, num_feats, reduce_type);
+    if (reduce_type == reduce_t::MEAN)
+      reduced_feats /= reduce_count.unsqueeze(-1).to(reduced_feats.dtype());
+  }));
   AT_CUDA_CHECK(cudaGetLastError());
 
   return {reduced_feats, out_coors, coors_map, reduce_count};
@@ -331,6 +259,8 @@ void dynamic_point_to_voxel_backward_gpu(at::Tensor &grad_feats,
   grad_feats.fill_(0);
   // copy voxel grad to points
 
+  if (num_input == 0 || num_reduced == 0) return;
+
   if (reduce_type == reduce_t::MEAN || reduce_type == reduce_t::SUM) {
     AT_DISPATCH_FLOATING_TYPES(
         grad_reduced_feats.scalar_type(), "add_reduce_traceback_grad_kernel",
diff --git a/mmdet3d/ops/voxel/src/voxelization.h b/mmdet3d/ops/voxel/src/voxelization.h
index 47df1d3..1e96c4e 100644
--- a/mmdet3d/ops/voxel/src/voxelization.h
+++ b/mmdet3d/ops/voxel/src/voxelization.h
@@ -27,8 +27,20 @@ int hard_voxelize_gpu(const at::Tensor &points, at::Tensor &voxels,
                       const std::vector<float> voxel_size,
                       const std::vector<float> coors_range,
                       const int max_points, const int max_voxels,
+                      // zzj api 20220224
+                      // : add input valuables for return
+                      at::Tensor &point_to_pointidx,
+                      at::Tensor &point_to_voxelidx,
+                      at::Tensor &coor_to_voxelidx,
                       const int NDim = 3);
 
+int nondisterministic_hard_voxelize_gpu(const at::Tensor &points, at::Tensor &voxels,
+                                        at::Tensor &coors, at::Tensor &num_points_per_voxel,
+                                        const std::vector<float> voxel_size,
+                                        const std::vector<float> coors_range,
+                                        const int max_points, const int max_voxels,
+                                        const int NDim = 3);
+
 void dynamic_voxelize_gpu(const at::Tensor &points, at::Tensor &coors,
                           const std::vector<float> voxel_size,
                           const std::vector<float> coors_range,
@@ -53,12 +65,27 @@ inline int hard_voxelize(const at::Tensor &points, at::Tensor &voxels,
                          const std::vector<float> voxel_size,
                          const std::vector<float> coors_range,
                          const int max_points, const int max_voxels,
-                         const int NDim = 3) {
+                         // zzj api 20220224
+                         // : add input valuables for return
+                         at::Tensor &point_to_pointidx,
+                         at::Tensor &point_to_voxelidx,
+                         at::Tensor &coor_to_voxelidx,
+                         const int NDim = 3, const bool deterministic = true) {
   if (points.device().is_cuda()) {
 #ifdef WITH_CUDA
-    return hard_voxelize_gpu(points, voxels, coors, num_points_per_voxel,
-                             voxel_size, coors_range, max_points, max_voxels,
-                             NDim);
+    if (deterministic) {
+      return hard_voxelize_gpu(points, voxels, coors, num_points_per_voxel,
+                               voxel_size, coors_range, max_points, max_voxels,
+                               // zzj api 20220224
+                               // : add input valuables for return
+                               point_to_pointidx,
+                               point_to_voxelidx,
+                               coor_to_voxelidx,
+                               NDim);
+    }
+    return nondisterministic_hard_voxelize_gpu(points, voxels, coors, num_points_per_voxel,
+                                               voxel_size, coors_range, max_points, max_voxels,
+                                               NDim);
 #else
     AT_ERROR("Not compiled with GPU support");
 #endif
diff --git a/mmdet3d/ops/voxel/src/voxelization_cpu.cpp b/mmdet3d/ops/voxel/src/voxelization_cpu.cpp
index 8a8091a..6bcec40 100644
--- a/mmdet3d/ops/voxel/src/voxelization_cpu.cpp
+++ b/mmdet3d/ops/voxel/src/voxelization_cpu.cpp
@@ -14,7 +14,8 @@ void dynamic_voxelize_kernel(const torch::TensorAccessor<T, 2> points,
                              const int NDim) {
   const int ndim_minus_1 = NDim - 1;
   bool failed = false;
-  int coor[NDim];
+  // int coor[NDim];
+  int* coor = new int[NDim]();
   int c;
 
   for (int i = 0; i < num_points; ++i) {
@@ -37,6 +38,7 @@ void dynamic_voxelize_kernel(const torch::TensorAccessor<T, 2> points,
     }
   }
 
+  delete[] coor;
   return;
 }
 
@@ -75,7 +77,7 @@ void hard_voxelize_kernel(const torch::TensorAccessor<T, 2> points,
     // record voxel
     if (voxelidx == -1) {
       voxelidx = voxel_num;
-      if (max_voxels != -1 && voxel_num >= max_voxels) break;
+      if (max_voxels != -1 && voxel_num >= max_voxels) continue;
       voxel_num += 1;
 
       coor_to_voxelidx[coor[i][0]][coor[i][1]][coor[i][2]] = voxelidx;
diff --git a/mmdet3d/ops/voxel/src/voxelization_cuda.cu b/mmdet3d/ops/voxel/src/voxelization_cuda.cu
index 4d98821..a69ecc7 100644
--- a/mmdet3d/ops/voxel/src/voxelization_cuda.cu
+++ b/mmdet3d/ops/voxel/src/voxelization_cuda.cu
@@ -164,7 +164,7 @@ __global__ void determin_voxel_num(
     } else if (point_pos_in_voxel == 0) {
       // record new voxel
       int voxelidx = voxel_num[0];
-      if (voxel_num[0] >= max_voxels) break;
+      if (voxel_num[0] >= max_voxels) continue;
       voxel_num[0] += 1;
       coor_to_voxelidx[i] = voxelidx;
       num_points_per_voxel[voxelidx] = 1;
@@ -179,6 +179,53 @@ __global__ void determin_voxel_num(
   }
 }
 
+__global__ void nondisterministic_get_assign_pos(
+    const int nthreads, const int32_t *coors_map, int32_t *pts_id,
+    int32_t *coors_count, int32_t *reduce_count, int32_t *coors_order) {
+  CUDA_1D_KERNEL_LOOP(thread_idx, nthreads) {
+    int coors_idx = coors_map[thread_idx];
+    if (coors_idx > -1) {
+      int32_t coors_pts_pos = atomicAdd(&reduce_count[coors_idx], 1);
+      pts_id[thread_idx] = coors_pts_pos;
+      if (coors_pts_pos == 0) {
+        coors_order[coors_idx] = atomicAdd(coors_count, 1);
+      }
+    }
+  }
+}
+
+template<typename T>
+__global__ void nondisterministic_assign_point_voxel(
+    const int nthreads, const T *points, const int32_t *coors_map,
+    const int32_t *pts_id, const int32_t *coors_in,
+    const int32_t *reduce_count, const int32_t *coors_order,
+    T *voxels, int32_t *coors, int32_t *pts_count, const int max_voxels,
+    const int max_points, const int num_features, const int NDim) {
+  CUDA_1D_KERNEL_LOOP(thread_idx, nthreads) {
+    int coors_idx = coors_map[thread_idx];
+    int coors_pts_pos = pts_id[thread_idx];
+    if (coors_idx > -1) {
+      int coors_pos = coors_order[coors_idx];
+      if (coors_pos < max_voxels && coors_pts_pos < max_points) {
+        auto voxels_offset =
+            voxels + (coors_pos * max_points + coors_pts_pos) * num_features;
+        auto points_offset = points + thread_idx * num_features;
+        for (int k = 0; k < num_features; k++) {
+          voxels_offset[k] = points_offset[k];
+        }
+        if (coors_pts_pos == 0) {
+          pts_count[coors_pos] = min(reduce_count[coors_idx], max_points);
+          auto coors_offset = coors + coors_pos * NDim;
+          auto coors_in_offset = coors_in + coors_idx * NDim;
+          for (int k = 0; k < NDim; k++) {
+            coors_offset[k] = coors_in_offset[k];
+          }
+        }
+      }
+    }
+  }
+}
+
 namespace voxelization {
 
 int hard_voxelize_gpu(const at::Tensor& points, at::Tensor& voxels,
@@ -186,6 +233,11 @@ int hard_voxelize_gpu(const at::Tensor& points, at::Tensor& voxels,
                       const std::vector<float> voxel_size,
                       const std::vector<float> coors_range,
                       const int max_points, const int max_voxels,
+                      // zzj api 20220224
+                      // : add for return
+                      at::Tensor &point_to_pointidx,
+                      at::Tensor &point_to_voxelidx,
+                      at::Tensor &coor_to_voxelidx,
                       const int NDim = 3) {
   // current version tooks about 0.04s for one frame on cpu
   // check device
@@ -233,16 +285,20 @@ int hard_voxelize_gpu(const at::Tensor& points, at::Tensor& voxels,
 
   // 2. map point to the idx of the corresponding voxel, find duplicate coor
   // create some temporary variables
-  auto point_to_pointidx = -at::ones(
-      {
-          num_points,
-      },
-      points.options().dtype(at::kInt));
-  auto point_to_voxelidx = -at::ones(
-      {
-          num_points,
-      },
-      points.options().dtype(at::kInt));
+
+  // zzj api 20220224
+  // : use outside coming values defined in python 
+  // not defined in C 
+  // auto point_to_pointidx = -at::ones(
+  //     {
+  //         num_points,
+  //     },
+  //     points.options().dtype(at::kInt));
+  // auto point_to_voxelidx = -at::ones(
+  //     {
+  //         num_points,
+  //     },
+  //     points.options().dtype(at::kInt));
 
   dim3 map_grid(std::min(at::cuda::ATenCeilDiv(num_points, 512), 4096));
   dim3 map_block(512);
@@ -260,11 +316,16 @@ int hard_voxelize_gpu(const at::Tensor& points, at::Tensor& voxels,
 
   // 3. determin voxel num and voxel's coor index
   // make the logic in the CUDA device could accelerate about 10 times
-  auto coor_to_voxelidx = -at::ones(
-      {
-          num_points,
-      },
-      points.options().dtype(at::kInt));
+  
+  // zzj api 20220224
+  // : use outside coming values defined in python 
+  // not defined in C 
+  // auto coor_to_voxelidx = -at::ones(
+  //     {
+  //         num_points,
+  //     },
+  //     points.options().dtype(at::kInt));
+
   auto voxel_num = at::zeros(
       {
           1,
@@ -325,6 +386,116 @@ int hard_voxelize_gpu(const at::Tensor& points, at::Tensor& voxels,
   return voxel_num_int;
 }
 
+int nondisterministic_hard_voxelize_gpu(
+    const at::Tensor &points, at::Tensor &voxels,
+    at::Tensor &coors, at::Tensor &num_points_per_voxel,
+    const std::vector<float> voxel_size,
+    const std::vector<float> coors_range,
+    const int max_points, const int max_voxels,
+    const int NDim = 3) {
+
+  CHECK_INPUT(points);
+
+  at::cuda::CUDAGuard device_guard(points.device());
+
+  const int num_points = points.size(0);
+  const int num_features = points.size(1);
+
+  if (num_points == 0)
+    return 0;
+
+  const float voxel_x = voxel_size[0];
+  const float voxel_y = voxel_size[1];
+  const float voxel_z = voxel_size[2];
+  const float coors_x_min = coors_range[0];
+  const float coors_y_min = coors_range[1];
+  const float coors_z_min = coors_range[2];
+  const float coors_x_max = coors_range[3];
+  const float coors_y_max = coors_range[4];
+  const float coors_z_max = coors_range[5];
+
+  const int grid_x = round((coors_x_max - coors_x_min) / voxel_x);
+  const int grid_y = round((coors_y_max - coors_y_min) / voxel_y);
+  const int grid_z = round((coors_z_max - coors_z_min) / voxel_z);
+
+  // map points to voxel coors
+  at::Tensor temp_coors =
+      at::zeros({num_points, NDim}, points.options().dtype(torch::kInt32));
+
+  dim3 grid(std::min(at::cuda::ATenCeilDiv(num_points, 512), 4096));
+  dim3 block(512);
+
+  // 1. link point to corresponding voxel coors
+  AT_DISPATCH_ALL_TYPES(
+      points.scalar_type(), "hard_voxelize_kernel", ([&] {
+    dynamic_voxelize_kernel<scalar_t, int>
+    <<<grid, block, 0, at::cuda::getCurrentCUDAStream()>>>(
+        points.contiguous().data_ptr<scalar_t>(),
+        temp_coors.contiguous().data_ptr<int>(), voxel_x, voxel_y,
+        voxel_z, coors_x_min, coors_y_min, coors_z_min, coors_x_max,
+        coors_y_max, coors_z_max, grid_x, grid_y, grid_z, num_points,
+        num_features, NDim);
+  }));
+
+  at::Tensor coors_map;
+  at::Tensor coors_count;
+  at::Tensor coors_order;
+  at::Tensor reduce_count;
+  at::Tensor pts_id;
+
+  auto coors_clean = temp_coors.masked_fill(temp_coors.lt(0).any(-1, true), -1);
+
+  std::tie(temp_coors, coors_map, reduce_count) =
+      at::unique_dim(coors_clean, 0, true, true, false);
+
+  if (temp_coors.index({0, 0}).lt(0).item<bool>()) {
+    // the first element of temp_coors is (-1,-1,-1) and should be removed
+    temp_coors = temp_coors.slice(0, 1);
+    coors_map = coors_map - 1;
+  }
+
+  int num_coors = temp_coors.size(0);
+  temp_coors = temp_coors.to(torch::kInt32);
+  coors_map = coors_map.to(torch::kInt32);
+
+  coors_count = coors_map.new_zeros(1);
+  coors_order = coors_map.new_empty(num_coors);
+  reduce_count = coors_map.new_zeros(num_coors);
+  pts_id = coors_map.new_zeros(num_points);
+
+  dim3 cp_grid(std::min(at::cuda::ATenCeilDiv(num_points, 512), 4096));
+  dim3 cp_block(512);
+  AT_DISPATCH_ALL_TYPES(points.scalar_type(), "get_assign_pos", ([&] {
+    nondisterministic_get_assign_pos<<<cp_grid, cp_block, 0,
+    at::cuda::getCurrentCUDAStream()>>>(
+        num_points,
+        coors_map.contiguous().data_ptr<int32_t>(),
+        pts_id.contiguous().data_ptr<int32_t>(),
+        coors_count.contiguous().data_ptr<int32_t>(),
+        reduce_count.contiguous().data_ptr<int32_t>(),
+        coors_order.contiguous().data_ptr<int32_t>());
+  }));
+
+  AT_DISPATCH_ALL_TYPES(
+      points.scalar_type(), "assign_point_to_voxel", ([&] {
+    nondisterministic_assign_point_voxel<scalar_t>
+    <<<cp_grid, cp_block, 0, at::cuda::getCurrentCUDAStream()>>>(
+        num_points, points.contiguous().data_ptr<scalar_t>(),
+        coors_map.contiguous().data_ptr<int32_t>(),
+        pts_id.contiguous().data_ptr<int32_t>(),
+        temp_coors.contiguous().data_ptr<int32_t>(),
+        reduce_count.contiguous().data_ptr<int32_t>(),
+        coors_order.contiguous().data_ptr<int32_t>(),
+        voxels.contiguous().data_ptr<scalar_t>(),
+        coors.contiguous().data_ptr<int32_t>(),
+        num_points_per_voxel.contiguous().data_ptr<int32_t>(),
+        max_voxels, max_points,
+        num_features, NDim);
+  }));
+  AT_CUDA_CHECK(cudaGetLastError());
+  return max_voxels < num_coors ? max_voxels : num_coors;
+}
+
 void dynamic_voxelize_gpu(const at::Tensor& points, at::Tensor& coors,
                           const std::vector<float> voxel_size,
                           const std::vector<float> coors_range,
diff --git a/mmdet3d/ops/voxel/src_backup/scatter_points_cpu.cpp b/mmdet3d/ops/voxel/src_backup/scatter_points_cpu.cpp
new file mode 100644
index 0000000..c22b8ae
--- /dev/null
+++ b/mmdet3d/ops/voxel/src_backup/scatter_points_cpu.cpp
@@ -0,0 +1,122 @@
+#include <ATen/TensorUtils.h>
+#include <torch/extension.h>
+// #include "voxelization.h"
+
+namespace {
+
+template <typename T_int>
+void determin_max_points_kernel(
+    torch::TensorAccessor<T_int, 2> coor,
+    torch::TensorAccessor<T_int, 1> point_to_voxelidx,
+    torch::TensorAccessor<T_int, 1> num_points_per_voxel,
+    torch::TensorAccessor<T_int, 3> coor_to_voxelidx, int& voxel_num,
+    int& max_points, const int num_points) {
+  int voxelidx, num;
+  for (int i = 0; i < num_points; ++i) {
+    if (coor[i][0] == -1) continue;
+    voxelidx = coor_to_voxelidx[coor[i][0]][coor[i][1]][coor[i][2]];
+
+    // record voxel
+    if (voxelidx == -1) {
+      voxelidx = voxel_num;
+      voxel_num += 1;
+      coor_to_voxelidx[coor[i][0]][coor[i][1]][coor[i][2]] = voxelidx;
+    }
+
+    // put points into voxel
+    num = num_points_per_voxel[voxelidx];
+    point_to_voxelidx[i] = num;
+    num_points_per_voxel[voxelidx] += 1;
+
+    // update max points per voxel
+    max_points = std::max(max_points, num + 1);
+  }
+
+  return;
+}
+
+template <typename T, typename T_int>
+void scatter_point_to_voxel_kernel(
+    const torch::TensorAccessor<T, 2> points,
+    torch::TensorAccessor<T_int, 2> coor,
+    torch::TensorAccessor<T_int, 1> point_to_voxelidx,
+    torch::TensorAccessor<T_int, 3> coor_to_voxelidx,
+    torch::TensorAccessor<T, 3> voxels,
+    torch::TensorAccessor<T_int, 2> voxel_coors, const int num_features,
+    const int num_points, const int NDim) {
+  for (int i = 0; i < num_points; ++i) {
+    int num = point_to_voxelidx[i];
+    int voxelidx = coor_to_voxelidx[coor[i][0]][coor[i][1]][coor[i][2]];
+    for (int k = 0; k < num_features; ++k) {
+      voxels[voxelidx][num][k] = points[i][k];
+    }
+    for (int k = 0; k < NDim; ++k) {
+      voxel_coors[voxelidx][k] = coor[i][k];
+    }
+  }
+}
+
+}  // namespace
+
+namespace voxelization {
+
+std::vector<at::Tensor> dynamic_point_to_voxel_cpu(
+    const at::Tensor& points, const at::Tensor& voxel_mapping,
+    const std::vector<float> voxel_size, const std::vector<float> coors_range) {
+  // current version tooks about 0.02s_0.03s for one frame on cpu
+  // check device
+  AT_ASSERTM(points.device().is_cpu(), "points must be a CPU tensor");
+
+  const int NDim = voxel_mapping.size(1);
+  const int num_points = points.size(0);
+  const int num_features = points.size(1);
+
+  std::vector<int> grid_size(NDim);
+  for (int i = 0; i < NDim; ++i) {
+    grid_size[i] =
+        round((coors_range[NDim + i] - coors_range[i]) / voxel_size[i]);
+  }
+
+  at::Tensor num_points_per_voxel = at::zeros(
+      {
+          num_points,
+      },
+      voxel_mapping.options());
+  at::Tensor coor_to_voxelidx = -at::ones(
+      {grid_size[2], grid_size[1], grid_size[0]}, voxel_mapping.options());
+  at::Tensor point_to_voxelidx = -at::ones(
+      {
+          num_points,
+      },
+      voxel_mapping.options());
+
+  int voxel_num = 0;
+  int max_points = 0;
+  AT_DISPATCH_ALL_TYPES(voxel_mapping.scalar_type(), "determin_max_point", [&] {
+    determin_max_points_kernel<scalar_t>(
+        voxel_mapping.accessor<scalar_t, 2>(),
+        point_to_voxelidx.accessor<scalar_t, 1>(),
+        num_points_per_voxel.accessor<scalar_t, 1>(),
+        coor_to_voxelidx.accessor<scalar_t, 3>(), voxel_num, max_points,
+        num_points);
+  });
+
+  at::Tensor voxels =
+      at::zeros({voxel_num, max_points, num_features}, points.options());
+  at::Tensor voxel_coors =
+      at::zeros({voxel_num, NDim}, points.options().dtype(at::kInt));
+
+  AT_DISPATCH_ALL_TYPES(points.scalar_type(), "scatter_point_to_voxel", [&] {
+    scatter_point_to_voxel_kernel<scalar_t, int>(
+        points.accessor<scalar_t, 2>(), voxel_mapping.accessor<int, 2>(),
+        point_to_voxelidx.accessor<int, 1>(),
+        coor_to_voxelidx.accessor<int, 3>(), voxels.accessor<scalar_t, 3>(),
+        voxel_coors.accessor<int, 2>(), num_features, num_points, NDim);
+  });
+
+  at::Tensor num_points_per_voxel_out =
+      num_points_per_voxel.slice(/*dim=*/0, /*start=*/0, /*end=*/voxel_num);
+  return {voxels, voxel_coors, num_points_per_voxel_out};
+}
+
+}  // namespace voxelization
diff --git a/mmdet3d/ops/voxel/src_backup/scatter_points_cuda.cu b/mmdet3d/ops/voxel/src_backup/scatter_points_cuda.cu
new file mode 100644
index 0000000..8d888b0
--- /dev/null
+++ b/mmdet3d/ops/voxel/src_backup/scatter_points_cuda.cu
@@ -0,0 +1,380 @@
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <torch/types.h>
+
+#include <ATen/cuda/CUDAApplyUtils.cuh>
+
+typedef enum { SUM = 0, MEAN = 1, MAX = 2 } reduce_t;
+
+#define CHECK_CUDA(x) \
+  TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
+#define CHECK_CONTIGUOUS(x) \
+  TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
+#define CHECK_INPUT(x) \
+  CHECK_CUDA(x);       \
+  CHECK_CONTIGUOUS(x)
+
+namespace {
+int const threadsPerBlock = 512;
+int const maxGridDim = 50000;
+}  // namespace
+
+__device__ __forceinline__ static void reduceMax(float *address, float val) {
+  int *address_as_i = reinterpret_cast<int *>(address);
+  int old = *address_as_i, assumed;
+  do {
+    assumed = old;
+    old = atomicCAS(address_as_i, assumed,
+                    __float_as_int(fmaxf(val, __int_as_float(assumed))));
+  } while (assumed != old || __int_as_float(old) < val);
+}
+
+__device__ __forceinline__ static void reduceMax(double *address, double val) {
+  unsigned long long *address_as_ull =
+      reinterpret_cast<unsigned long long *>(address);
+  unsigned long long old = *address_as_ull, assumed;
+  do {
+    assumed = old;
+    old = atomicCAS(
+        address_as_ull, assumed,
+        __double_as_longlong(fmax(val, __longlong_as_double(assumed))));
+  } while (assumed != old || __longlong_as_double(old) < val);
+}
+
+// get rid of meaningless warnings when compiling host code
+#ifdef __CUDA_ARCH__
+__device__ __forceinline__ static void reduceAdd(float *address, float val) {
+#if (__CUDA_ARCH__ < 200)
+#warning \
+    "compute capability lower than 2.x. fall back to use CAS version of atomicAdd for float32"
+  int *address_as_i = reinterpret_cast<int *>(address);
+  int old = *address_as_i, assumed;
+  do {
+    assumed = old;
+    old = atomicCAS(address_as_i, assumed,
+                    __float_as_int(val + __int_as_float(assumed)));
+  } while (assumed != old);
+#else
+  atomicAdd(address, val);
+#endif
+}
+
+__device__ __forceinline__ static void reduceAdd(double *address, double val) {
+#if (__CUDA_ARCH__ < 600)
+#warning \
+    "compute capability lower than 6.x. fall back to use CAS version of atomicAdd for float64"
+  unsigned long long *address_as_ull =
+      reinterpret_cast<unsigned long long *>(address);
+  unsigned long long old = *address_as_ull, assumed;
+  do {
+    assumed = old;
+    old = atomicCAS(address_as_ull, assumed,
+                    __double_as_longlong(val + __longlong_as_double(assumed)));
+  } while (assumed != old);
+#else
+  atomicAdd(address, val);
+#endif
+}
+#endif
+
+template <typename T_int>
+__global__ void coors_id_kernel(const T_int *coors, const T_int *dim,
+                                int64_t *coors_id, const int num_input,
+                                const int NDim) {
+  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
+       x += gridDim.x * blockDim.x) {
+    const T_int *coor_x = coors + x * NDim;
+    auto coor_id = 0;
+    for (int i = 0; i < NDim && coor_id != -1; i++) {
+      coor_id *= dim[i];
+      auto t = static_cast<int64_t>(coor_x[i]);
+      coor_id = (t < 0) ? -1 : coor_id + t;
+    }
+    coors_id[x] = coor_id;
+  }
+}
+
+template <typename T_int>
+__global__ void coors_map_init_kernel(const int64_t *coors_id,
+                                      const T_int *coors_id_argsort,
+                                      int32_t *coors_map, const int num_input) {
+  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
+       x += gridDim.x * blockDim.x) {
+    auto here = coors_id[coors_id_argsort[x]];
+    if (x == 0) {
+      if (here == -1) {  // there is invalid points
+        coors_map[0] = -1;
+      } else {
+        coors_map[0] = 0;
+      }
+      continue;
+    }
+    auto left = coors_id[coors_id_argsort[x - 1]];
+    coors_map[x] = (left < here) ? 1 : 0;
+  }
+}
+
+template <typename T, typename T_int>
+__global__ void feats_reduce_kernel(
+    const T *feats, const T_int *coors, int32_t *coors_map,
+    int32_t *reduce_count,  // shall be 0 at initialization
+    T *reduced_feats,       // shall be 0 at initialization
+    T_int *out_coors, const int num_input, const int num_feats, const int NDim,
+    const reduce_t reduce_type) {
+  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
+       x += gridDim.x * blockDim.x) {
+    int32_t reduce_to = coors_map[x];
+    if (reduce_to == -1) continue;
+
+    const T_int *coors_offset = coors + x * NDim;
+    T_int *out_coors_offset = out_coors + reduce_to * NDim;
+    for (int i = 0; i < NDim; i++) {
+      out_coors_offset[i] = coors_offset[i];
+    }
+
+    const T *feats_offset = feats + x * num_feats;
+    T *reduced_feats_offset = reduced_feats + reduce_to * num_feats;
+    if (reduce_type == reduce_t::MAX) {
+      for (int i = 0; i < num_feats; i++) {
+        reduceMax(&reduced_feats_offset[i], feats_offset[i]);
+      }
+    } else {
+      if (reduce_type == reduce_t::MEAN) {
+        atomicAdd(&reduce_count[reduce_to], static_cast<int32_t>(1));
+      }
+      for (int i = 0; i < num_feats; i++) {
+        reduceAdd(&reduced_feats_offset[i], feats_offset[i]);
+      }
+    }
+  }
+}
+
+template <typename T>
+__global__ void add_reduce_traceback_grad_kernel(
+    T *grad_feats, const T *grad_reduced_feats, const int32_t *coors_map,
+    const int32_t *reduce_count, const int num_input, const int num_feats,
+    const reduce_t reduce_type) {
+  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
+       x += gridDim.x * blockDim.x) {
+    int32_t reduce_to = coors_map[x];
+    if (reduce_to == -1) {
+      continue;
+    }
+
+    const int input_offset = x * num_feats;
+    T *grad_feats_offset = grad_feats + input_offset;
+    const int reduced_offset = reduce_to * num_feats;
+    const T *grad_reduced_feats_offset = grad_reduced_feats + reduced_offset;
+
+    if (reduce_type == reduce_t::SUM) {
+      for (int i = 0; i < num_feats; i++) {
+        grad_feats_offset[i] = grad_reduced_feats_offset[i];
+      }
+    } else if (reduce_type == reduce_t::MEAN) {
+      for (int i = 0; i < num_feats; i++) {
+        grad_feats_offset[i] = grad_reduced_feats_offset[i] /
+                               static_cast<T>(reduce_count[reduce_to]);
+      }
+    }
+  }
+}
+
+template <typename T>
+__global__ void max_reduce_traceback_scatter_idx_kernel(
+    const T *feats, const T *reduced_feats, int32_t *reduce_from,
+    const int32_t *coors_map, const int num_input, const int num_feats) {
+  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_input;
+       x += gridDim.x * blockDim.x) {
+    int32_t reduce_to = coors_map[x];
+
+    const int input_offset = x * num_feats;
+    const T *feats_offset = feats + input_offset;
+
+    if (reduce_to == -1) {
+      continue;
+    }
+
+    const int reduced_offset = reduce_to * num_feats;
+    const T *reduced_feats_offset = reduced_feats + reduced_offset;
+    int32_t *reduce_from_offset = reduce_from + reduced_offset;
+
+    for (int i = 0; i < num_feats; i++) {
+      if (feats_offset[i] == reduced_feats_offset[i]) {
+        atomicMin(&reduce_from_offset[i], static_cast<int32_t>(x));
+      }
+    }
+  }
+}
+
+template <typename T>
+__global__ void max_reduce_scatter_grad_kernel(T *grad_feats,
+                                               const T *grad_reduced_feats,
+                                               const int32_t *reduce_from,
+                                               const int num_reduced,
+                                               const int num_feats) {
+  for (int x = blockIdx.x * blockDim.x + threadIdx.x; x < num_reduced;
+       x += gridDim.x * blockDim.x) {
+    const int reduced_offset = x * num_feats;
+    const int32_t *scatter_to_offset = reduce_from + reduced_offset;
+    const T *grad_reduced_feats_offset = grad_reduced_feats + reduced_offset;
+
+    for (int i = 0; i < num_feats; i++) {
+      grad_feats[scatter_to_offset[i] * num_feats + i] =
+          grad_reduced_feats_offset[i];
+    }
+  }
+}
+
+namespace voxelization {
+
+std::vector<at::Tensor> dynamic_point_to_voxel_forward_gpu(
+    const at::Tensor &feats, const at::Tensor &coors,
+    const reduce_t reduce_type) {
+  CHECK_INPUT(feats);
+  CHECK_INPUT(coors);
+
+  const int NDim = coors.size(1);
+  const int num_input = feats.size(0);
+  const int num_feats = feats.size(1);
+
+  auto coors_id = at::empty({num_input}, coors.options().dtype(torch::kInt64));
+  auto coor_space_dim = std::get<0>(coors.max(0)) + 1;
+  auto coors_map_sorted =
+      at::empty({num_input}, coors.options().dtype(torch::kInt32));
+  auto coors_map = at::empty({num_input}, coors.options().dtype(torch::kInt32));
+  auto num_coors = at::zeros({1}, coors.options().dtype(torch::kInt32));
+
+  AT_DISPATCH_INTEGRAL_TYPES(
+      coors.scalar_type(), "coors_id_kernel", ([&] {
+        dim3 blocks(std::min(at::cuda::ATenCeilDiv(num_input, threadsPerBlock),
+                             maxGridDim));
+        dim3 threads(threadsPerBlock);
+        coors_id_kernel<<<blocks, threads>>>(
+            coors.data_ptr<scalar_t>(), coor_space_dim.data_ptr<scalar_t>(),
+            coors_id.data_ptr<int64_t>(), num_input, NDim);
+      }));
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  auto coors_id_argsort = coors_id.argsort();
+
+  AT_DISPATCH_INTEGRAL_TYPES(
+      coors_id_argsort.scalar_type(), "coors_map_init_kernel", ([&] {
+        dim3 blocks(std::min(at::cuda::ATenCeilDiv(num_input, threadsPerBlock),
+                             maxGridDim));
+        dim3 threads(threadsPerBlock);
+        coors_map_init_kernel<<<blocks, threads>>>(
+            coors_id.data_ptr<int64_t>(), coors_id_argsort.data_ptr<scalar_t>(),
+            coors_map_sorted.data_ptr<int32_t>(), num_input);
+      }));
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  coors_map_sorted = coors_map_sorted.cumsum(0, torch::kInt32);
+  coors_map.index_put_(coors_id_argsort, coors_map_sorted);
+
+  const int num_coors_cpu =
+      coors_map_sorted[-1].cpu().data_ptr<int32_t>()[0] + 1;
+  auto out_coors = at::empty({num_coors_cpu, NDim}, coors.options());
+  auto reduced_feats = at::empty({num_coors_cpu, num_feats}, feats.options());
+  auto reduce_count =
+      at::zeros({num_coors_cpu}, coors.options().dtype(torch::kInt32));
+
+  AT_DISPATCH_FLOATING_TYPES(
+      feats.scalar_type(), "feats_reduce_kernel", ([&] {
+        using F_t = scalar_t;
+        AT_DISPATCH_INTEGRAL_TYPES(
+            coors.scalar_type(), "feats_reduce_kernel", ([&] {
+              using I_t = scalar_t;
+
+              if (reduce_type == reduce_t::MAX)
+                reduced_feats.fill_(-std::numeric_limits<F_t>::infinity());
+              else
+                reduced_feats.fill_(static_cast<F_t>(0));
+
+              dim3 blocks(
+                  std::min(at::cuda::ATenCeilDiv(num_input, threadsPerBlock),
+                           maxGridDim));
+              dim3 threads(threadsPerBlock);
+              feats_reduce_kernel<<<blocks, threads>>>(
+                  feats.data_ptr<F_t>(), coors.data_ptr<I_t>(),
+                  coors_map.data_ptr<int32_t>(),
+                  reduce_count.data_ptr<int32_t>(),
+                  reduced_feats.data_ptr<F_t>(), out_coors.data_ptr<I_t>(),
+                  num_input, num_feats, NDim, reduce_type);
+              if (reduce_type == reduce_t::MEAN)
+                reduced_feats /=
+                    reduce_count.unsqueeze(-1).to(reduced_feats.dtype());
+            }));
+      }));
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  return {reduced_feats, out_coors, coors_map, reduce_count};
+}
+
+void dynamic_point_to_voxel_backward_gpu(at::Tensor &grad_feats,
+                                         const at::Tensor &grad_reduced_feats,
+                                         const at::Tensor &feats,
+                                         const at::Tensor &reduced_feats,
+                                         const at::Tensor &coors_map,
+                                         const at::Tensor &reduce_count,
+                                         const reduce_t reduce_type) {
+  CHECK_INPUT(grad_feats);
+  CHECK_INPUT(grad_reduced_feats);
+  CHECK_INPUT(feats);
+  CHECK_INPUT(reduced_feats);
+  CHECK_INPUT(coors_map);
+  CHECK_INPUT(reduce_count);
+
+  const int num_input = feats.size(0);
+  const int num_reduced = reduced_feats.size(0);
+  const int num_feats = feats.size(1);
+
+  grad_feats.fill_(0);
+  // copy voxel grad to points
+
+  if (reduce_type == reduce_t::MEAN || reduce_type == reduce_t::SUM) {
+    AT_DISPATCH_FLOATING_TYPES(
+        grad_reduced_feats.scalar_type(), "add_reduce_traceback_grad_kernel",
+        ([&] {
+          dim3 blocks(std::min(
+              at::cuda::ATenCeilDiv(num_input, threadsPerBlock), maxGridDim));
+          dim3 threads(threadsPerBlock);
+          add_reduce_traceback_grad_kernel<<<blocks, threads>>>(
+              grad_feats.data_ptr<scalar_t>(),
+              grad_reduced_feats.data_ptr<scalar_t>(),
+              coors_map.data_ptr<int32_t>(), reduce_count.data_ptr<int32_t>(),
+              num_input, num_feats, reduce_type);
+        }));
+    AT_CUDA_CHECK(cudaGetLastError());
+  } else {
+    auto reduce_from = at::full({num_reduced, num_feats}, num_input,
+                                coors_map.options().dtype(torch::kInt32));
+    AT_DISPATCH_FLOATING_TYPES(
+        grad_reduced_feats.scalar_type(),
+        "max_reduce_traceback_scatter_idx_kernel", ([&] {
+          dim3 blocks(std::min(
+              at::cuda::ATenCeilDiv(num_input, threadsPerBlock), maxGridDim));
+          dim3 threads(threadsPerBlock);
+          max_reduce_traceback_scatter_idx_kernel<<<blocks, threads>>>(
+              feats.data_ptr<scalar_t>(), reduced_feats.data_ptr<scalar_t>(),
+              reduce_from.data_ptr<int32_t>(), coors_map.data_ptr<int32_t>(),
+              num_input, num_feats);
+        }));
+    AT_CUDA_CHECK(cudaGetLastError());
+
+    AT_DISPATCH_FLOATING_TYPES(
+        grad_reduced_feats.scalar_type(),
+        "max_reduce_traceback_scatter_idx_kernel", ([&] {
+          dim3 blocks(std::min(
+              at::cuda::ATenCeilDiv(num_reduced, threadsPerBlock), maxGridDim));
+          dim3 threads(threadsPerBlock);
+          max_reduce_scatter_grad_kernel<<<blocks, threads>>>(
+              grad_feats.data_ptr<scalar_t>(),
+              grad_reduced_feats.data_ptr<scalar_t>(),
+              reduce_from.data_ptr<int32_t>(), num_reduced, num_feats);
+        }));
+    AT_CUDA_CHECK(cudaGetLastError());
+  }
+  return;
+}
+
+}  // namespace voxelization
diff --git a/mmdet3d/ops/voxel/src_backup/voxelization.cpp b/mmdet3d/ops/voxel/src_backup/voxelization.cpp
new file mode 100644
index 0000000..f83348e
--- /dev/null
+++ b/mmdet3d/ops/voxel/src_backup/voxelization.cpp
@@ -0,0 +1,13 @@
+#include <torch/extension.h>
+#include "voxelization.h"
+
+namespace voxelization {
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  m.def("hard_voxelize", &hard_voxelize, "hard voxelize");
+  m.def("dynamic_voxelize", &dynamic_voxelize, "dynamic voxelization");
+  m.def("dynamic_point_to_voxel_forward", &dynamic_point_to_voxel_forward, "dynamic point to voxel forward");
+  m.def("dynamic_point_to_voxel_backward", &dynamic_point_to_voxel_backward, "dynamic point to voxel backward");
+}
+
+} // namespace voxelization
diff --git a/mmdet3d/ops/voxel/src_backup/voxelization.h b/mmdet3d/ops/voxel/src_backup/voxelization.h
new file mode 100644
index 0000000..47df1d3
--- /dev/null
+++ b/mmdet3d/ops/voxel/src_backup/voxelization.h
@@ -0,0 +1,130 @@
+#pragma once
+#include <torch/extension.h>
+
+typedef enum { SUM = 0, MEAN = 1, MAX = 2 } reduce_t;
+
+namespace voxelization {
+
+int hard_voxelize_cpu(const at::Tensor &points, at::Tensor &voxels,
+                      at::Tensor &coors, at::Tensor &num_points_per_voxel,
+                      const std::vector<float> voxel_size,
+                      const std::vector<float> coors_range,
+                      const int max_points, const int max_voxels,
+                      const int NDim = 3);
+
+void dynamic_voxelize_cpu(const at::Tensor &points, at::Tensor &coors,
+                          const std::vector<float> voxel_size,
+                          const std::vector<float> coors_range,
+                          const int NDim = 3);
+
+std::vector<at::Tensor> dynamic_point_to_voxel_cpu(
+    const at::Tensor &points, const at::Tensor &voxel_mapping,
+    const std::vector<float> voxel_size, const std::vector<float> coors_range);
+
+#ifdef WITH_CUDA
+int hard_voxelize_gpu(const at::Tensor &points, at::Tensor &voxels,
+                      at::Tensor &coors, at::Tensor &num_points_per_voxel,
+                      const std::vector<float> voxel_size,
+                      const std::vector<float> coors_range,
+                      const int max_points, const int max_voxels,
+                      const int NDim = 3);
+
+void dynamic_voxelize_gpu(const at::Tensor &points, at::Tensor &coors,
+                          const std::vector<float> voxel_size,
+                          const std::vector<float> coors_range,
+                          const int NDim = 3);
+
+std::vector<torch::Tensor> dynamic_point_to_voxel_forward_gpu(const torch::Tensor &feats,
+                                                              const torch::Tensor &coors,
+                                                              const reduce_t reduce_type);
+
+void dynamic_point_to_voxel_backward_gpu(torch::Tensor &grad_feats,
+                                         const torch::Tensor &grad_reduced_feats,
+                                         const torch::Tensor &feats,
+                                         const torch::Tensor &reduced_feats,
+                                         const torch::Tensor &coors_idx,
+                                         const torch::Tensor &reduce_count,
+                                         const reduce_t reduce_type);
+#endif
+
+// Interface for Python
+inline int hard_voxelize(const at::Tensor &points, at::Tensor &voxels,
+                         at::Tensor &coors, at::Tensor &num_points_per_voxel,
+                         const std::vector<float> voxel_size,
+                         const std::vector<float> coors_range,
+                         const int max_points, const int max_voxels,
+                         const int NDim = 3) {
+  if (points.device().is_cuda()) {
+#ifdef WITH_CUDA
+    return hard_voxelize_gpu(points, voxels, coors, num_points_per_voxel,
+                             voxel_size, coors_range, max_points, max_voxels,
+                             NDim);
+#else
+    AT_ERROR("Not compiled with GPU support");
+#endif
+  }
+  return hard_voxelize_cpu(points, voxels, coors, num_points_per_voxel,
+                           voxel_size, coors_range, max_points, max_voxels,
+                           NDim);
+}
+
+inline void dynamic_voxelize(const at::Tensor &points, at::Tensor &coors,
+                             const std::vector<float> voxel_size,
+                             const std::vector<float> coors_range,
+                             const int NDim = 3) {
+  if (points.device().is_cuda()) {
+#ifdef WITH_CUDA
+    return dynamic_voxelize_gpu(points, coors, voxel_size, coors_range, NDim);
+#else
+    AT_ERROR("Not compiled with GPU support");
+#endif
+  }
+  return dynamic_voxelize_cpu(points, coors, voxel_size, coors_range, NDim);
+}
+
+inline reduce_t convert_reduce_type(const std::string &reduce_type) {
+  if (reduce_type == "max")
+    return reduce_t::MAX;
+  else if (reduce_type == "sum")
+    return reduce_t::SUM;
+  else if (reduce_type == "mean")
+    return reduce_t::MEAN;
+  else TORCH_CHECK(false, "do not support reduce type " + reduce_type)
+  return reduce_t::SUM;
+}
+
+inline std::vector<torch::Tensor> dynamic_point_to_voxel_forward(const torch::Tensor &feats,
+                                                                 const torch::Tensor &coors,
+                                                                 const std::string &reduce_type) {
+  if (feats.device().is_cuda()) {
+#ifdef WITH_CUDA
+    return dynamic_point_to_voxel_forward_gpu(feats, coors, convert_reduce_type(reduce_type));
+#else
+    TORCH_CHECK(false, "Not compiled with GPU support");
+#endif
+  }
+  TORCH_CHECK(false, "do not support cpu yet");
+  return std::vector<torch::Tensor>();
+}
+
+inline void dynamic_point_to_voxel_backward(torch::Tensor &grad_feats,
+                                            const torch::Tensor &grad_reduced_feats,
+                                            const torch::Tensor &feats,
+                                            const torch::Tensor &reduced_feats,
+                                            const torch::Tensor &coors_idx,
+                                            const torch::Tensor &reduce_count,
+                                            const std::string &reduce_type) {
+  if (grad_feats.device().is_cuda()) {
+#ifdef WITH_CUDA
+    dynamic_point_to_voxel_backward_gpu(
+        grad_feats, grad_reduced_feats, feats, reduced_feats, coors_idx, reduce_count,
+        convert_reduce_type(reduce_type));
+    return;
+#else
+    TORCH_CHECK(false, "Not compiled with GPU support");
+#endif
+  }
+  TORCH_CHECK(false, "do not support cpu yet");
+}
+
+}  // namespace voxelization
diff --git a/mmdet3d/ops/voxel/src_backup/voxelization_cpu.cpp b/mmdet3d/ops/voxel/src_backup/voxelization_cpu.cpp
new file mode 100644
index 0000000..8a8091a
--- /dev/null
+++ b/mmdet3d/ops/voxel/src_backup/voxelization_cpu.cpp
@@ -0,0 +1,171 @@
+#include <ATen/TensorUtils.h>
+#include <torch/extension.h>
+// #include "voxelization.h"
+
+namespace {
+
+template <typename T, typename T_int>
+void dynamic_voxelize_kernel(const torch::TensorAccessor<T, 2> points,
+                             torch::TensorAccessor<T_int, 2> coors,
+                             const std::vector<float> voxel_size,
+                             const std::vector<float> coors_range,
+                             const std::vector<int> grid_size,
+                             const int num_points, const int num_features,
+                             const int NDim) {
+  const int ndim_minus_1 = NDim - 1;
+  bool failed = false;
+  int coor[NDim];
+  int c;
+
+  for (int i = 0; i < num_points; ++i) {
+    failed = false;
+    for (int j = 0; j < NDim; ++j) {
+      c = floor((points[i][j] - coors_range[j]) / voxel_size[j]);
+      // necessary to rm points out of range
+      if ((c < 0 || c >= grid_size[j])) {
+        failed = true;
+        break;
+      }
+      coor[ndim_minus_1 - j] = c;
+    }
+
+    for (int k = 0; k < NDim; ++k) {
+      if (failed)
+        coors[i][k] = -1;
+      else
+        coors[i][k] = coor[k];
+    }
+  }
+
+  return;
+}
+
+template <typename T, typename T_int>
+void hard_voxelize_kernel(const torch::TensorAccessor<T, 2> points,
+                          torch::TensorAccessor<T, 3> voxels,
+                          torch::TensorAccessor<T_int, 2> coors,
+                          torch::TensorAccessor<T_int, 1> num_points_per_voxel,
+                          torch::TensorAccessor<T_int, 3> coor_to_voxelidx,
+                          int& voxel_num, const std::vector<float> voxel_size,
+                          const std::vector<float> coors_range,
+                          const std::vector<int> grid_size,
+                          const int max_points, const int max_voxels,
+                          const int num_points, const int num_features,
+                          const int NDim) {
+  // declare a temp coors
+  at::Tensor temp_coors = at::zeros(
+      {num_points, NDim}, at::TensorOptions().dtype(at::kInt).device(at::kCPU));
+
+  // First use dynamic voxelization to get coors,
+  // then check max points/voxels constraints
+  dynamic_voxelize_kernel<T, int>(points, temp_coors.accessor<int, 2>(),
+                                  voxel_size, coors_range, grid_size,
+                                  num_points, num_features, NDim);
+
+  int voxelidx, num;
+  auto coor = temp_coors.accessor<int, 2>();
+
+  for (int i = 0; i < num_points; ++i) {
+    // T_int* coor = temp_coors.data_ptr<int>() + i * NDim;
+
+    if (coor[i][0] == -1) continue;
+
+    voxelidx = coor_to_voxelidx[coor[i][0]][coor[i][1]][coor[i][2]];
+
+    // record voxel
+    if (voxelidx == -1) {
+      voxelidx = voxel_num;
+      if (max_voxels != -1 && voxel_num >= max_voxels) break;
+      voxel_num += 1;
+
+      coor_to_voxelidx[coor[i][0]][coor[i][1]][coor[i][2]] = voxelidx;
+
+      for (int k = 0; k < NDim; ++k) {
+        coors[voxelidx][k] = coor[i][k];
+      }
+    }
+
+    // put points into voxel
+    num = num_points_per_voxel[voxelidx];
+    if (max_points == -1 || num < max_points) {
+      for (int k = 0; k < num_features; ++k) {
+        voxels[voxelidx][num][k] = points[i][k];
+      }
+      num_points_per_voxel[voxelidx] += 1;
+    }
+  }
+
+  return;
+}
+
+}  // namespace
+
+namespace voxelization {
+
+int hard_voxelize_cpu(const at::Tensor& points, at::Tensor& voxels,
+                      at::Tensor& coors, at::Tensor& num_points_per_voxel,
+                      const std::vector<float> voxel_size,
+                      const std::vector<float> coors_range,
+                      const int max_points, const int max_voxels,
+                      const int NDim = 3) {
+  // current version tooks about 0.02s_0.03s for one frame on cpu
+  // check device
+  AT_ASSERTM(points.device().is_cpu(), "points must be a CPU tensor");
+
+  std::vector<int> grid_size(NDim);
+  const int num_points = points.size(0);
+  const int num_features = points.size(1);
+
+  for (int i = 0; i < NDim; ++i) {
+    grid_size[i] =
+        round((coors_range[NDim + i] - coors_range[i]) / voxel_size[i]);
+  }
+
+  // coors, num_points_per_voxel, coor_to_voxelidx are int Tensor
+  // printf("cpu coor_to_voxelidx size: [%d, %d, %d]\n", grid_size[2],
+  // grid_size[1], grid_size[0]);
+  at::Tensor coor_to_voxelidx =
+      -at::ones({grid_size[2], grid_size[1], grid_size[0]}, coors.options());
+
+  int voxel_num = 0;
+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
+      points.scalar_type(), "hard_voxelize_forward", [&] {
+        hard_voxelize_kernel<scalar_t, int>(
+            points.accessor<scalar_t, 2>(), voxels.accessor<scalar_t, 3>(),
+            coors.accessor<int, 2>(), num_points_per_voxel.accessor<int, 1>(),
+            coor_to_voxelidx.accessor<int, 3>(), voxel_num, voxel_size,
+            coors_range, grid_size, max_points, max_voxels, num_points,
+            num_features, NDim);
+      });
+
+  return voxel_num;
+}
+
+void dynamic_voxelize_cpu(const at::Tensor& points, at::Tensor& coors,
+                          const std::vector<float> voxel_size,
+                          const std::vector<float> coors_range,
+                          const int NDim = 3) {
+  // check device
+  AT_ASSERTM(points.device().is_cpu(), "points must be a CPU tensor");
+
+  std::vector<int> grid_size(NDim);
+  const int num_points = points.size(0);
+  const int num_features = points.size(1);
+
+  for (int i = 0; i < NDim; ++i) {
+    grid_size[i] =
+        round((coors_range[NDim + i] - coors_range[i]) / voxel_size[i]);
+  }
+
+  // coors, num_points_per_voxel, coor_to_voxelidx are int Tensor
+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
+      points.scalar_type(), "hard_voxelize_forward", [&] {
+        dynamic_voxelize_kernel<scalar_t, int>(
+            points.accessor<scalar_t, 2>(), coors.accessor<int, 2>(),
+            voxel_size, coors_range, grid_size, num_points, num_features, NDim);
+      });
+
+  return;
+}
+
+}  // namespace voxelization
diff --git a/mmdet3d/ops/voxel/src_backup/voxelization_cuda.cu b/mmdet3d/ops/voxel/src_backup/voxelization_cuda.cu
new file mode 100644
index 0000000..4d98821
--- /dev/null
+++ b/mmdet3d/ops/voxel/src_backup/voxelization_cuda.cu
@@ -0,0 +1,373 @@
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <c10/cuda/CUDAGuard.h>
+#include <torch/types.h>
+
+#include <ATen/cuda/CUDAApplyUtils.cuh>
+
+#define CHECK_CUDA(x) \
+  TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
+#define CHECK_CONTIGUOUS(x) \
+  TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
+#define CHECK_INPUT(x) \
+  CHECK_CUDA(x);       \
+  CHECK_CONTIGUOUS(x)
+
+namespace {
+int const threadsPerBlock = sizeof(unsigned long long) * 8;
+}
+
+#define CUDA_1D_KERNEL_LOOP(i, n)                            \
+  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; \
+       i += blockDim.x * gridDim.x)
+
+template <typename T, typename T_int>
+__global__ void dynamic_voxelize_kernel(
+    const T* points, T_int* coors, const float voxel_x, const float voxel_y,
+    const float voxel_z, const float coors_x_min, const float coors_y_min,
+    const float coors_z_min, const float coors_x_max, const float coors_y_max,
+    const float coors_z_max, const int grid_x, const int grid_y,
+    const int grid_z, const int num_points, const int num_features,
+    const int NDim) {
+  //   const int index = blockIdx.x * threadsPerBlock + threadIdx.x;
+  CUDA_1D_KERNEL_LOOP(index, num_points) {
+    // To save some computation
+    auto points_offset = points + index * num_features;
+    auto coors_offset = coors + index * NDim;
+    int c_x = floor((points_offset[0] - coors_x_min) / voxel_x);
+    if (c_x < 0 || c_x >= grid_x) {
+      coors_offset[0] = -1;
+      return;
+    }
+
+    int c_y = floor((points_offset[1] - coors_y_min) / voxel_y);
+    if (c_y < 0 || c_y >= grid_y) {
+      coors_offset[0] = -1;
+      coors_offset[1] = -1;
+      return;
+    }
+
+    int c_z = floor((points_offset[2] - coors_z_min) / voxel_z);
+    if (c_z < 0 || c_z >= grid_z) {
+      coors_offset[0] = -1;
+      coors_offset[1] = -1;
+      coors_offset[2] = -1;
+    } else {
+      coors_offset[0] = c_z;
+      coors_offset[1] = c_y;
+      coors_offset[2] = c_x;
+    }
+  }
+}
+
+template <typename T, typename T_int>
+__global__ void assign_point_to_voxel(const int nthreads, const T* points,
+                                      T_int* point_to_voxelidx,
+                                      T_int* coor_to_voxelidx, T* voxels,
+                                      const int max_points,
+                                      const int num_features,
+                                      const int num_points, const int NDim) {
+  CUDA_1D_KERNEL_LOOP(thread_idx, nthreads) {
+    // const int index = blockIdx.x * threadsPerBlock + threadIdx.x;
+    int index = thread_idx / num_features;
+
+    int num = point_to_voxelidx[index];
+    int voxelidx = coor_to_voxelidx[index];
+    if (num > -1 && voxelidx > -1) {
+      auto voxels_offset =
+          voxels + voxelidx * max_points * num_features + num * num_features;
+
+      int k = thread_idx % num_features;
+      voxels_offset[k] = points[thread_idx];
+    }
+  }
+}
+
+template <typename T, typename T_int>
+__global__ void assign_voxel_coors(const int nthreads, T_int* coor,
+                                   T_int* point_to_voxelidx,
+                                   T_int* coor_to_voxelidx, T_int* voxel_coors,
+                                   const int num_points, const int NDim) {
+  CUDA_1D_KERNEL_LOOP(thread_idx, nthreads) {
+    // const int index = blockIdx.x * threadsPerBlock + threadIdx.x;
+    // if (index >= num_points) return;
+    int index = thread_idx / NDim;
+    int num = point_to_voxelidx[index];
+    int voxelidx = coor_to_voxelidx[index];
+    if (num == 0 && voxelidx > -1) {
+      auto coors_offset = voxel_coors + voxelidx * NDim;
+      int k = thread_idx % NDim;
+      coors_offset[k] = coor[thread_idx];
+    }
+  }
+}
+
+template <typename T_int>
+__global__ void point_to_voxelidx_kernel(const T_int* coor,
+                                         T_int* point_to_voxelidx,
+                                         T_int* point_to_pointidx,
+                                         const int max_points,
+                                         const int max_voxels,
+                                         const int num_points, const int NDim) {
+  CUDA_1D_KERNEL_LOOP(index, num_points) {
+    auto coor_offset = coor + index * NDim;
+    // skip invalid points
+    if ((index >= num_points) || (coor_offset[0] == -1)) return;
+
+    int num = 0;
+    int coor_x = coor_offset[0];
+    int coor_y = coor_offset[1];
+    int coor_z = coor_offset[2];
+    // only calculate the coors before this coor[index]
+    for (int i = 0; i < index; ++i) {
+      auto prev_coor = coor + i * NDim;
+      if (prev_coor[0] == -1) continue;
+
+      // Find all previous points that have the same coors
+      // if find the same coor, record it
+      if ((prev_coor[0] == coor_x) && (prev_coor[1] == coor_y) &&
+          (prev_coor[2] == coor_z)) {
+        num++;
+        if (num == 1) {
+          // point to the same coor that first show up
+          point_to_pointidx[index] = i;
+        } else if (num >= max_points) {
+          // out of boundary
+          return;
+        }
+      }
+    }
+    if (num == 0) {
+      point_to_pointidx[index] = index;
+    }
+    if (num < max_points) {
+      point_to_voxelidx[index] = num;
+    }
+  }
+}
+
+template <typename T_int>
+__global__ void determin_voxel_num(
+    // const T_int* coor,
+    T_int* num_points_per_voxel, T_int* point_to_voxelidx,
+    T_int* point_to_pointidx, T_int* coor_to_voxelidx, T_int* voxel_num,
+    const int max_points, const int max_voxels, const int num_points) {
+  // only calculate the coors before this coor[index]
+  for (int i = 0; i < num_points; ++i) {
+    // if (coor[i][0] == -1)
+    //    continue;
+    int point_pos_in_voxel = point_to_voxelidx[i];
+    // record voxel
+    if (point_pos_in_voxel == -1) {
+      // out of max_points or invalid point
+      continue;
+    } else if (point_pos_in_voxel == 0) {
+      // record new voxel
+      int voxelidx = voxel_num[0];
+      if (voxel_num[0] >= max_voxels) break;
+      voxel_num[0] += 1;
+      coor_to_voxelidx[i] = voxelidx;
+      num_points_per_voxel[voxelidx] = 1;
+    } else {
+      int point_idx = point_to_pointidx[i];
+      int voxelidx = coor_to_voxelidx[point_idx];
+      if (voxelidx != -1) {
+        coor_to_voxelidx[i] = voxelidx;
+        num_points_per_voxel[voxelidx] += 1;
+      }
+    }
+  }
+}
+
+namespace voxelization {
+
+int hard_voxelize_gpu(const at::Tensor& points, at::Tensor& voxels,
+                      at::Tensor& coors, at::Tensor& num_points_per_voxel,
+                      const std::vector<float> voxel_size,
+                      const std::vector<float> coors_range,
+                      const int max_points, const int max_voxels,
+                      const int NDim = 3) {
+  // current version tooks about 0.04s for one frame on cpu
+  // check device
+  CHECK_INPUT(points);
+
+  at::cuda::CUDAGuard device_guard(points.device());
+
+  const int num_points = points.size(0);
+  const int num_features = points.size(1);
+
+  const float voxel_x = voxel_size[0];
+  const float voxel_y = voxel_size[1];
+  const float voxel_z = voxel_size[2];
+  const float coors_x_min = coors_range[0];
+  const float coors_y_min = coors_range[1];
+  const float coors_z_min = coors_range[2];
+  const float coors_x_max = coors_range[3];
+  const float coors_y_max = coors_range[4];
+  const float coors_z_max = coors_range[5];
+
+  const int grid_x = round((coors_x_max - coors_x_min) / voxel_x);
+  const int grid_y = round((coors_y_max - coors_y_min) / voxel_y);
+  const int grid_z = round((coors_z_max - coors_z_min) / voxel_z);
+
+  // map points to voxel coors
+  at::Tensor temp_coors =
+      at::zeros({num_points, NDim}, points.options().dtype(at::kInt));
+
+  dim3 grid(std::min(at::cuda::ATenCeilDiv(num_points, 512), 4096));
+  dim3 block(512);
+
+  // 1. link point to corresponding voxel coors
+  AT_DISPATCH_ALL_TYPES(
+      points.scalar_type(), "hard_voxelize_kernel", ([&] {
+        dynamic_voxelize_kernel<scalar_t, int>
+            <<<grid, block, 0, at::cuda::getCurrentCUDAStream()>>>(
+                points.contiguous().data_ptr<scalar_t>(),
+                temp_coors.contiguous().data_ptr<int>(), voxel_x, voxel_y,
+                voxel_z, coors_x_min, coors_y_min, coors_z_min, coors_x_max,
+                coors_y_max, coors_z_max, grid_x, grid_y, grid_z, num_points,
+                num_features, NDim);
+      }));
+  cudaDeviceSynchronize();
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  // 2. map point to the idx of the corresponding voxel, find duplicate coor
+  // create some temporary variables
+  auto point_to_pointidx = -at::ones(
+      {
+          num_points,
+      },
+      points.options().dtype(at::kInt));
+  auto point_to_voxelidx = -at::ones(
+      {
+          num_points,
+      },
+      points.options().dtype(at::kInt));
+
+  dim3 map_grid(std::min(at::cuda::ATenCeilDiv(num_points, 512), 4096));
+  dim3 map_block(512);
+  AT_DISPATCH_ALL_TYPES(
+      temp_coors.scalar_type(), "determin_duplicate", ([&] {
+        point_to_voxelidx_kernel<int>
+            <<<map_grid, map_block, 0, at::cuda::getCurrentCUDAStream()>>>(
+                temp_coors.contiguous().data_ptr<int>(),
+                point_to_voxelidx.contiguous().data_ptr<int>(),
+                point_to_pointidx.contiguous().data_ptr<int>(), max_points,
+                max_voxels, num_points, NDim);
+      }));
+  cudaDeviceSynchronize();
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  // 3. determin voxel num and voxel's coor index
+  // make the logic in the CUDA device could accelerate about 10 times
+  auto coor_to_voxelidx = -at::ones(
+      {
+          num_points,
+      },
+      points.options().dtype(at::kInt));
+  auto voxel_num = at::zeros(
+      {
+          1,
+      },
+      points.options().dtype(at::kInt));  // must be zero from the begining
+
+  AT_DISPATCH_ALL_TYPES(
+      temp_coors.scalar_type(), "determin_duplicate", ([&] {
+        determin_voxel_num<int><<<1, 1, 0, at::cuda::getCurrentCUDAStream()>>>(
+            num_points_per_voxel.contiguous().data_ptr<int>(),
+            point_to_voxelidx.contiguous().data_ptr<int>(),
+            point_to_pointidx.contiguous().data_ptr<int>(),
+            coor_to_voxelidx.contiguous().data_ptr<int>(),
+            voxel_num.contiguous().data_ptr<int>(), max_points, max_voxels,
+            num_points);
+      }));
+  cudaDeviceSynchronize();
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  // 4. copy point features to voxels
+  // Step 4 & 5 could be parallel
+  auto pts_output_size = num_points * num_features;
+  dim3 cp_grid(std::min(at::cuda::ATenCeilDiv(pts_output_size, 512), 4096));
+  dim3 cp_block(512);
+  AT_DISPATCH_ALL_TYPES(
+      points.scalar_type(), "assign_point_to_voxel", ([&] {
+        assign_point_to_voxel<float, int>
+            <<<cp_grid, cp_block, 0, at::cuda::getCurrentCUDAStream()>>>(
+                pts_output_size, points.contiguous().data_ptr<float>(),
+                point_to_voxelidx.contiguous().data_ptr<int>(),
+                coor_to_voxelidx.contiguous().data_ptr<int>(),
+                voxels.contiguous().data_ptr<float>(), max_points, num_features,
+                num_points, NDim);
+      }));
+  //   cudaDeviceSynchronize();
+  //   AT_CUDA_CHECK(cudaGetLastError());
+
+  // 5. copy coors of each voxels
+  auto coors_output_size = num_points * NDim;
+  dim3 coors_cp_grid(
+      std::min(at::cuda::ATenCeilDiv(coors_output_size, 512), 4096));
+  dim3 coors_cp_block(512);
+  AT_DISPATCH_ALL_TYPES(
+      points.scalar_type(), "assign_point_to_voxel", ([&] {
+        assign_voxel_coors<float, int><<<coors_cp_grid, coors_cp_block, 0,
+                                         at::cuda::getCurrentCUDAStream()>>>(
+            coors_output_size, temp_coors.contiguous().data_ptr<int>(),
+            point_to_voxelidx.contiguous().data_ptr<int>(),
+            coor_to_voxelidx.contiguous().data_ptr<int>(),
+            coors.contiguous().data_ptr<int>(), num_points, NDim);
+      }));
+  cudaDeviceSynchronize();
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  auto voxel_num_cpu = voxel_num.to(at::kCPU);
+  int voxel_num_int = voxel_num_cpu.data_ptr<int>()[0];
+
+  return voxel_num_int;
+}
+
+void dynamic_voxelize_gpu(const at::Tensor& points, at::Tensor& coors,
+                          const std::vector<float> voxel_size,
+                          const std::vector<float> coors_range,
+                          const int NDim = 3) {
+  // current version tooks about 0.04s for one frame on cpu
+  // check device
+  CHECK_INPUT(points);
+
+  at::cuda::CUDAGuard device_guard(points.device());
+
+  const int num_points = points.size(0);
+  const int num_features = points.size(1);
+
+  const float voxel_x = voxel_size[0];
+  const float voxel_y = voxel_size[1];
+  const float voxel_z = voxel_size[2];
+  const float coors_x_min = coors_range[0];
+  const float coors_y_min = coors_range[1];
+  const float coors_z_min = coors_range[2];
+  const float coors_x_max = coors_range[3];
+  const float coors_y_max = coors_range[4];
+  const float coors_z_max = coors_range[5];
+
+  const int grid_x = round((coors_x_max - coors_x_min) / voxel_x);
+  const int grid_y = round((coors_y_max - coors_y_min) / voxel_y);
+  const int grid_z = round((coors_z_max - coors_z_min) / voxel_z);
+
+  const int col_blocks = at::cuda::ATenCeilDiv(num_points, threadsPerBlock);
+  dim3 blocks(col_blocks);
+  dim3 threads(threadsPerBlock);
+  cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  AT_DISPATCH_ALL_TYPES(points.scalar_type(), "dynamic_voxelize_kernel", [&] {
+    dynamic_voxelize_kernel<scalar_t, int><<<blocks, threads, 0, stream>>>(
+        points.contiguous().data_ptr<scalar_t>(),
+        coors.contiguous().data_ptr<int>(), voxel_x, voxel_y, voxel_z,
+        coors_x_min, coors_y_min, coors_z_min, coors_x_max, coors_y_max,
+        coors_z_max, grid_x, grid_y, grid_z, num_points, num_features, NDim);
+  });
+  cudaDeviceSynchronize();
+  AT_CUDA_CHECK(cudaGetLastError());
+
+  return;
+}
+
+}  // namespace voxelization
diff --git a/mmdet3d/ops/voxel/voxelize.py b/mmdet3d/ops/voxel/voxelize.py
index e578448..1534e70 100644
--- a/mmdet3d/ops/voxel/voxelize.py
+++ b/mmdet3d/ops/voxel/voxelize.py
@@ -7,15 +7,28 @@ from torch.nn.modules.utils import _pair
 from .voxel_layer import dynamic_voxelize, hard_voxelize
 
 
-class _Voxelization(Function):
+# class _Voxelization(Function):
 
-    @staticmethod
-    def forward(ctx,
+#     @staticmethod
+#     def forward(ctx,
+#                 points,
+#                 voxel_size,
+#                 coors_range,
+#                 max_points=35,
+#                 max_voxels=20000,
+#                 deterministic=True):
+# zzj api 20220224
+# : change class _Voxelization(Function) which only define forward 
+# to def _Voxelization
+# enables the auto backward . 
+def _Voxelization(
+                # ctx,
                 points,
                 voxel_size,
                 coors_range,
                 max_points=35,
-                max_voxels=20000):
+                max_voxels=20000,
+                deterministic=True):
         """convert kitti points(N, >=3) to voxels.
 
         Args:
@@ -30,6 +43,16 @@ class _Voxelization(Function):
             max_voxels: int. indicate maximum voxels this function create.
                 for second, 20000 is a good choice. Users should shuffle points
                 before call this function because max_voxels may drop points.
+            deterministic: bool. whether to invoke the non-deterministic
+                version of hard-voxelization implementations. non-deterministic
+                version is considerablly fast but is not deterministic. only
+                affects hard voxelization. default True. for more information
+                of this argument and the implementation insights, please refer
+                to the following links:
+                https://github.com/open-mmlab/mmdetection3d/issues/894
+                https://github.com/open-mmlab/mmdetection3d/pull/904
+                it is an experimental feature and we will appreciate it if
+                you could share with us the failing cases.
 
         Returns:
             voxels: [M, max_points, ndim] float tensor. only contain points
@@ -48,17 +71,70 @@ class _Voxelization(Function):
             coors = points.new_zeros(size=(max_voxels, 3), dtype=torch.int)
             num_points_per_voxel = points.new_zeros(
                 size=(max_voxels, ), dtype=torch.int)
+            
+
+            # ==================== old =====================
+            # voxel_num = hard_voxelize(points, voxels, coors,
+            #                           num_points_per_voxel, voxel_size,
+            #                           coors_range, max_points, max_voxels, 3,
+            #                           deterministic)
+            # # select the valid voxels
+            # voxels_out = voxels[:voxel_num]
+            # coors_out = coors[:voxel_num]
+            # num_points_per_voxel_out = num_points_per_voxel[:voxel_num]
+            # return voxels_out, coors_out, num_points_per_voxel_out
+
+            # ==================== new =====================
+            # zzj api 20220224
+            # change .cu code and recomplie to get some idx we need 
+            # only in c code.
+            point_to_pointidx = points.new_zeros(size=(points.size(0),)).fill_(-1).int()
+            point_to_voxelidx = points.new_zeros(size=(points.size(0),)).fill_(-1).int()
+            coor_to_voxelidx = points.new_zeros(size=(points.size(0),)).fill_(-1).int()
+
+            # 这个函数的作用是把point填到一个一个voxel里面，
+            # 多余point丢掉，不足的补零
             voxel_num = hard_voxelize(points, voxels, coors,
                                       num_points_per_voxel, voxel_size,
-                                      coors_range, max_points, max_voxels, 3)
-            # select the valid voxels
-            voxels_out = voxels[:voxel_num]
+                                      coors_range, max_points, max_voxels,
+                                      point_to_pointidx, # 当前点对应的voxel（坐标）中，第一个出现的点的序号
+                                      point_to_voxelidx, # 当前点对应的voxel中，点的数目
+                                      coor_to_voxelidx,  # 当前点的voxel（坐标），对应的voxel的序号
+                                      3,
+                                      deterministic)
+
+            valid_index = (point_to_voxelidx>-1)*(coor_to_voxelidx>-1)
+            point_to_voxelidx_valid = point_to_voxelidx[valid_index]
+            coor_to_voxelidx_valid = coor_to_voxelidx[valid_index]
+            points_valid = points[valid_index]
+
+            voxelidx_tensor = coor_to_voxelidx_valid
+            num_tensor = point_to_voxelidx_valid
+            # 需要一个按位置填入的函数
+            # 可以建立一个单词本，把空的地方按照000单词填入，看看行不行
+            # 那么首先就要确定输出的tensor大小，序号tensor的大小：(max_voxels, max_points)
+            # 在这里做文章就够了吧，根据之前已有的index把point填进去的时候，多填几次就行
+            # 关键是判断朝上填还是朝下填，在xyz三个轴上
+
+            voxels_new = torch.cuda.FloatTensor(
+                size=(max_voxels, max_points, points.size(1))).fill_(0)
+            voxels_new_line = voxels_new.view(-1,points.size(1))
+            voxels_new_line[(voxelidx_tensor*max_points+num_tensor).long()] = points_valid
+            voxels_new = voxels_new_line.view(max_voxels, max_points, points.size(1))
+
+            voxels_out = voxels_new[:voxel_num]
+            # voxels_out = voxels[:voxel_num]
             coors_out = coors[:voxel_num]
             num_points_per_voxel_out = num_points_per_voxel[:voxel_num]
             return voxels_out, coors_out, num_points_per_voxel_out
+            
 
 
-voxelization = _Voxelization.apply
+# zzj api 20220224
+# to make def _Voxelization() works 
+# change this code
+# voxelization = _Voxelization.apply
+voxelization = _Voxelization
 
 
 class Voxelization(nn.Module):
@@ -67,7 +143,8 @@ class Voxelization(nn.Module):
                  voxel_size,
                  point_cloud_range,
                  max_num_points,
-                 max_voxels=20000):
+                 max_voxels=20000,
+                 deterministic=True):
         super(Voxelization, self).__init__()
         """
         Args:
@@ -77,6 +154,16 @@ class Voxelization(nn.Module):
             max_num_points (int): max number of points per voxel
             max_voxels (tuple or int): max number of voxels in
                 (training, testing) time
+            deterministic: bool. whether to invoke the non-deterministic
+                version of hard-voxelization implementations. non-deterministic
+                version is considerablly fast but is not deterministic. only
+                affects hard voxelization. default True. for more information
+                of this argument and the implementation insights, please refer
+                to the following links:
+                https://github.com/open-mmlab/mmdetection3d/issues/894
+                https://github.com/open-mmlab/mmdetection3d/pull/904
+                it is an experimental feature and we will appreciate it if
+                you could share with us the failing cases.
         """
         self.voxel_size = voxel_size
         self.point_cloud_range = point_cloud_range
@@ -85,6 +172,7 @@ class Voxelization(nn.Module):
             self.max_voxels = max_voxels
         else:
             self.max_voxels = _pair(max_voxels)
+        self.deterministic = deterministic
 
         point_cloud_range = torch.tensor(
             point_cloud_range, dtype=torch.float32)
@@ -110,7 +198,8 @@ class Voxelization(nn.Module):
             max_voxels = self.max_voxels[1]
 
         return voxelization(input, self.voxel_size, self.point_cloud_range,
-                            self.max_num_points, max_voxels)
+                            self.max_num_points, max_voxels,
+                            self.deterministic)
 
     def __repr__(self):
         tmpstr = self.__class__.__name__ + '('
@@ -118,5 +207,6 @@ class Voxelization(nn.Module):
         tmpstr += ', point_cloud_range=' + str(self.point_cloud_range)
         tmpstr += ', max_num_points=' + str(self.max_num_points)
         tmpstr += ', max_voxels=' + str(self.max_voxels)
+        tmpstr += ', deterministic=' + str(self.deterministic)
         tmpstr += ')'
         return tmpstr
diff --git a/mmdet3d/ops/voxel/voxelize_backup.py b/mmdet3d/ops/voxel/voxelize_backup.py
new file mode 100644
index 0000000..e578448
--- /dev/null
+++ b/mmdet3d/ops/voxel/voxelize_backup.py
@@ -0,0 +1,122 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
+import torch
+from torch import nn
+from torch.autograd import Function
+from torch.nn.modules.utils import _pair
+
+from .voxel_layer import dynamic_voxelize, hard_voxelize
+
+
+class _Voxelization(Function):
+
+    @staticmethod
+    def forward(ctx,
+                points,
+                voxel_size,
+                coors_range,
+                max_points=35,
+                max_voxels=20000):
+        """convert kitti points(N, >=3) to voxels.
+
+        Args:
+            points: [N, ndim] float tensor. points[:, :3] contain xyz points
+                and points[:, 3:] contain other information like reflectivity
+            voxel_size: [3] list/tuple or array, float. xyz, indicate voxel
+                size
+            coors_range: [6] list/tuple or array, float. indicate voxel
+                range. format: xyzxyz, minmax
+            max_points: int. indicate maximum points contained in a voxel. if
+                max_points=-1, it means using dynamic_voxelize
+            max_voxels: int. indicate maximum voxels this function create.
+                for second, 20000 is a good choice. Users should shuffle points
+                before call this function because max_voxels may drop points.
+
+        Returns:
+            voxels: [M, max_points, ndim] float tensor. only contain points
+                    and returned when max_points != -1.
+            coordinates: [M, 3] int32 tensor, always returned.
+            num_points_per_voxel: [M] int32 tensor. Only returned when
+                max_points != -1.
+        """
+        if max_points == -1 or max_voxels == -1:
+            coors = points.new_zeros(size=(points.size(0), 3), dtype=torch.int)
+            dynamic_voxelize(points, coors, voxel_size, coors_range, 3)
+            return coors
+        else:
+            voxels = points.new_zeros(
+                size=(max_voxels, max_points, points.size(1)))
+            coors = points.new_zeros(size=(max_voxels, 3), dtype=torch.int)
+            num_points_per_voxel = points.new_zeros(
+                size=(max_voxels, ), dtype=torch.int)
+            voxel_num = hard_voxelize(points, voxels, coors,
+                                      num_points_per_voxel, voxel_size,
+                                      coors_range, max_points, max_voxels, 3)
+            # select the valid voxels
+            voxels_out = voxels[:voxel_num]
+            coors_out = coors[:voxel_num]
+            num_points_per_voxel_out = num_points_per_voxel[:voxel_num]
+            return voxels_out, coors_out, num_points_per_voxel_out
+
+
+voxelization = _Voxelization.apply
+
+
+class Voxelization(nn.Module):
+
+    def __init__(self,
+                 voxel_size,
+                 point_cloud_range,
+                 max_num_points,
+                 max_voxels=20000):
+        super(Voxelization, self).__init__()
+        """
+        Args:
+            voxel_size (list): list [x, y, z] size of three dimension
+            point_cloud_range (list):
+                [x_min, y_min, z_min, x_max, y_max, z_max]
+            max_num_points (int): max number of points per voxel
+            max_voxels (tuple or int): max number of voxels in
+                (training, testing) time
+        """
+        self.voxel_size = voxel_size
+        self.point_cloud_range = point_cloud_range
+        self.max_num_points = max_num_points
+        if isinstance(max_voxels, tuple):
+            self.max_voxels = max_voxels
+        else:
+            self.max_voxels = _pair(max_voxels)
+
+        point_cloud_range = torch.tensor(
+            point_cloud_range, dtype=torch.float32)
+        # [0, -40, -3, 70.4, 40, 1]
+        voxel_size = torch.tensor(voxel_size, dtype=torch.float32)
+        grid_size = (point_cloud_range[3:] -
+                     point_cloud_range[:3]) / voxel_size
+        grid_size = torch.round(grid_size).long()
+        input_feat_shape = grid_size[:2]
+        self.grid_size = grid_size
+        # the origin shape is as [x-len, y-len, z-len]
+        # [w, h, d] -> [d, h, w]
+        self.pcd_shape = [*input_feat_shape, 1][::-1]
+
+    def forward(self, input):
+        """
+        Args:
+            input: NC points
+        """
+        if self.training:
+            max_voxels = self.max_voxels[0]
+        else:
+            max_voxels = self.max_voxels[1]
+
+        return voxelization(input, self.voxel_size, self.point_cloud_range,
+                            self.max_num_points, max_voxels)
+
+    def __repr__(self):
+        tmpstr = self.__class__.__name__ + '('
+        tmpstr += 'voxel_size=' + str(self.voxel_size)
+        tmpstr += ', point_cloud_range=' + str(self.point_cloud_range)
+        tmpstr += ', max_num_points=' + str(self.max_num_points)
+        tmpstr += ', max_voxels=' + str(self.max_voxels)
+        tmpstr += ')'
+        return tmpstr
diff --git a/tools/test_fgsm_img_launcher.py b/tools/test_fgsm_img_launcher.py
new file mode 100644
index 0000000..bbd5038
--- /dev/null
+++ b/tools/test_fgsm_img_launcher.py
@@ -0,0 +1,219 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis_common.test_fgsm_img import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('scattered_result_prefix', help='save scattered_result file dir')
+    parser.add_argument('eps255', help='eps of fgsm in 0-255')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'), train_cfg=cfg.get('train_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader,
+                                  args.scattered_result_prefix,
+                                  args.eps255,
+                                  )
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_patch_class_launcher.py b/tools/test_patch_class_launcher.py
new file mode 100644
index 0000000..2c0abed
--- /dev/null
+++ b/tools/test_patch_class_launcher.py
@@ -0,0 +1,221 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis_common.test_patch_class import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('patch_save_prefix', help='save patch_save file dir')
+    parser.add_argument('area_rate_str', help='area rate of patch')
+    parser.add_argument('optim_lr', help='optim_lr of attack')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'), train_cfg=cfg.get('train_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader,
+                                  args.patch_save_prefix,
+                                  args.area_rate_str,
+                                  args.optim_lr
+                                  )
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_patch_instance_launcher.py b/tools/test_patch_instance_launcher.py
new file mode 100644
index 0000000..a1a7937
--- /dev/null
+++ b/tools/test_patch_instance_launcher.py
@@ -0,0 +1,221 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis_common.test_patch_instance import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('scattered_result_prefix', help='save scattered_result file dir')
+    parser.add_argument('mask_code', help='mask area of instance patch')
+    parser.add_argument('step', help='step of attack')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'), train_cfg=cfg.get('train_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader,
+                                  args.scattered_result_prefix,
+                                  args.mask_code,
+                                  args.step
+                                  )
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_patch_overlap_launcher.py b/tools/test_patch_overlap_launcher.py
new file mode 100644
index 0000000..b3f78e3
--- /dev/null
+++ b/tools/test_patch_overlap_launcher.py
@@ -0,0 +1,224 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis_common.test_patch_overlap import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('scattered_result_prefix', help='save scattered_result file dir')
+    parser.add_argument('area_rate_str', help='area rate of patch')
+    parser.add_argument('optim_lr', help='optim_lr of attack')
+    parser.add_argument('optim_step', help='optim_lr of attack')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'), train_cfg=cfg.get('train_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader,
+                                  args.scattered_result_prefix,
+                                  args.area_rate_str,
+                                  args.optim_lr,
+                                  args.optim_step
+                                  )
+    
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_patch_temporal_launcher.py b/tools/test_patch_temporal_launcher.py
new file mode 100644
index 0000000..935487c
--- /dev/null
+++ b/tools/test_patch_temporal_launcher.py
@@ -0,0 +1,228 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis_common.test_patch_temporal import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('scattered_result_prefix', help='save scattered_result file dir')
+    parser.add_argument('area_rate_str', help='area rate of patch')
+    parser.add_argument('optim_lr', help='optim_lr of attack')
+    parser.add_argument('optim_step', help='optim_lr of attack')
+    parser.add_argument('--index-min', type=int, default=0)      # for multi-gpu split dataset
+    parser.add_argument('--index-max', type=int, default=100000) # for multi-gpu split dataset
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'), train_cfg=cfg.get('train_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader,
+                                  args.scattered_result_prefix,
+                                  args.area_rate_str,
+                                  args.optim_lr,
+                                  args.optim_step,
+                                  args.index_min,
+                                  args.index_max,
+                                  )
+    
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_pgd_img_launcher.py b/tools/test_pgd_img_launcher.py
new file mode 100644
index 0000000..44958cb
--- /dev/null
+++ b/tools/test_pgd_img_launcher.py
@@ -0,0 +1,221 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis_common.test_pgd_img import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('scattered_result_prefix', help='save scattered_result file dir')
+    parser.add_argument('eps255', help='eps of pgd in 0-255')
+    parser.add_argument('step', help='step of pgd')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'), train_cfg=cfg.get('train_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader,
+                                  args.scattered_result_prefix,
+                                  args.eps255,
+                                  args.step
+                                  )
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_pgd_imgpoint_launcher.py b/tools/test_pgd_imgpoint_launcher.py
new file mode 100644
index 0000000..d8cbc83
--- /dev/null
+++ b/tools/test_pgd_imgpoint_launcher.py
@@ -0,0 +1,223 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis_common.test_pgd_imgpoint import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('scattered_result_prefix', help='save scattered_result file prefix')
+    parser.add_argument('img_eps255', help='eps of pgd in img in 0-255')
+    parser.add_argument('point_eps_m', help='eps of pgd in point in meter')
+    parser.add_argument('step', help='step of pgd')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'), train_cfg=cfg.get('train_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader,
+                                  args.scattered_result_prefix,
+                                  args.img_eps255,
+                                  args.point_eps_m,
+                                  args.step
+                                  )
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_save_instance_patch_mask_launcher.py b/tools/test_save_instance_patch_mask_launcher.py
new file mode 100644
index 0000000..06c5d56
--- /dev/null
+++ b/tools/test_save_instance_patch_mask_launcher.py
@@ -0,0 +1,217 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis.test_save_instance_patch_mask import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('mask_save_dir', help='instance mask save dir')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    if args.fuse_conv_bn:
+        model = fuse_conv_bn(model)
+    # old versions did not save class info in checkpoints, this walkaround is
+    # for backward compatibility
+    if 'CLASSES' in checkpoint.get('meta', {}):
+        model.CLASSES = checkpoint['meta']['CLASSES']
+    else:
+        model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        model = MMDataParallel(model, device_ids=[0])
+        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir,
+                            mask_save_dir=args.mask_save_dir
+        )
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_save_patch_info_launcher.py b/tools/test_save_patch_info_launcher.py
new file mode 100644
index 0000000..8cb5e64
--- /dev/null
+++ b/tools/test_save_patch_info_launcher.py
@@ -0,0 +1,218 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis.test_save_patch_info import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('patch_info_2d3d_dir', help='patch info save dir')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # assert args.out or args.eval or args.format_only or args.show \
+    #     or args.show_dir, \
+    #     ('Please specify at least one operation (save/eval/format/show the '
+    #      'results / save the results) with the argument "--out", "--eval"'
+    #      ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    # cfg.model.train_cfg = None
+    # model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))
+    # fp16_cfg = cfg.get('fp16', None)
+    # if fp16_cfg is not None:
+    #     wrap_fp16_model(model)
+    # checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    # if args.fuse_conv_bn:
+    #     model = fuse_conv_bn(model)
+    # # old versions did not save class info in checkpoints, this walkaround is
+    # # for backward compatibility
+    # if 'CLASSES' in checkpoint.get('meta', {}):
+    #     model.CLASSES = checkpoint['meta']['CLASSES']
+    # else:
+    #     model.CLASSES = dataset.CLASSES
+
+    if not distributed:
+        # model = MMDataParallel(model, device_ids=[0])
+        model = None
+        outputs = single_gpu_test(model, data_loader, 
+                                  args.patch_info_2d3d_dir
+        )
+    else:
+        model = MMDistributedDataParallel(
+            model.cuda(),
+            device_ids=[torch.cuda.current_device()],
+            broadcast_buffers=False)
+        outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+                                 args.gpu_collect)
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/test_scatterd_eval.py b/tools/test_scatterd_eval.py
new file mode 100644
index 0000000..28bc78b
--- /dev/null
+++ b/tools/test_scatterd_eval.py
@@ -0,0 +1,223 @@
+import argparse
+import mmcv
+import os
+import torch
+import warnings
+from mmcv import Config, DictAction
+from mmcv.cnn import fuse_conv_bn
+from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
+                         wrap_fp16_model)
+
+from mmdet3d.apis import single_gpu_test
+from mmdet3d.datasets import build_dataloader, build_dataset
+from mmdet3d.models import build_detector
+from mmdet.apis import multi_gpu_test, set_random_seed
+from mmdet.datasets import replace_ImageToTensor
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description='MMDet test (and eval) a model')
+    parser.add_argument('config', help='test config file path')
+    # parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('scattered_result_dir', help='save scattered_result file dir')
+    parser.add_argument('--out', help='output result file in pickle format')
+    parser.add_argument(
+        '--fuse-conv-bn',
+        action='store_true',
+        help='Whether to fuse conv and bn, this will slightly increase'
+        'the inference speed')
+    parser.add_argument(
+        '--format-only',
+        action='store_true',
+        help='Format the output results without perform evaluation. It is'
+        'useful when you want to format the result to a specific format and '
+        'submit it to the test server')
+    parser.add_argument(
+        '--eval',
+        type=str,
+        nargs='+',
+        help='evaluation metrics, which depends on the dataset, e.g., "bbox",'
+        ' "segm", "proposal" for COCO, and "mAP", "recall" for PASCAL VOC')
+    parser.add_argument('--show', action='store_true', help='show results')
+    parser.add_argument(
+        '--show-dir', help='directory where results will be saved')
+    parser.add_argument(
+        '--gpu-collect',
+        action='store_true',
+        help='whether to use gpu to collect results.')
+    parser.add_argument(
+        '--tmpdir',
+        help='tmp directory used for collecting results from multiple '
+        'workers, available when gpu-collect is not specified')
+    parser.add_argument('--seed', type=int, default=0, help='random seed')
+    parser.add_argument(
+        '--deterministic',
+        action='store_true',
+        help='whether to set deterministic options for CUDNN backend.')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function (deprecate), '
+        'change to --eval-options instead.')
+    parser.add_argument(
+        '--eval-options',
+        nargs='+',
+        action=DictAction,
+        help='custom options for evaluation, the key-value pair in xxx=yyy '
+        'format will be kwargs for dataset.evaluate() function')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    parser.add_argument('--local_rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+
+    if args.options and args.eval_options:
+        raise ValueError(
+            '--options and --eval-options cannot be both specified, '
+            '--options is deprecated in favor of --eval-options')
+    if args.options:
+        warnings.warn('--options is deprecated in favor of --eval-options')
+        args.eval_options = args.options
+    return args
+
+
+def main():
+    args = parse_args()
+
+    assert args.out or args.eval or args.format_only or args.show \
+        or args.show_dir, \
+        ('Please specify at least one operation (save/eval/format/show the '
+         'results / save the results) with the argument "--out", "--eval"'
+         ', "--format-only", "--show" or "--show-dir"')
+
+    if args.eval and args.format_only:
+        raise ValueError('--eval and --format_only cannot be both specified')
+
+    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
+        raise ValueError('The output file must be a pkl file.')
+
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    # import modules from string list.
+    if cfg.get('custom_imports', None):
+        from mmcv.utils import import_modules_from_strings
+        import_modules_from_strings(**cfg['custom_imports'])
+    # set cudnn_benchmark
+    if cfg.get('cudnn_benchmark', False):
+        torch.backends.cudnn.benchmark = True
+
+    cfg.model.pretrained = None
+    # in case the test dataset is concatenated
+    samples_per_gpu = 1
+    if isinstance(cfg.data.test, dict):
+        cfg.data.test.test_mode = True
+        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)
+        if samples_per_gpu > 1:
+            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
+            cfg.data.test.pipeline = replace_ImageToTensor(
+                cfg.data.test.pipeline)
+    elif isinstance(cfg.data.test, list):
+        for ds_cfg in cfg.data.test:
+            ds_cfg.test_mode = True
+        samples_per_gpu = max(
+            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])
+        if samples_per_gpu > 1:
+            for ds_cfg in cfg.data.test:
+                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)
+
+    # init distributed env first, since logger depends on the dist info.
+    if args.launcher == 'none':
+        distributed = False
+    else:
+        distributed = True
+        init_dist(args.launcher, **cfg.dist_params)
+
+    # set random seeds
+    if args.seed is not None:
+        set_random_seed(args.seed, deterministic=args.deterministic)
+
+    # build the dataloader
+    dataset = build_dataset(cfg.data.test)
+    data_loader = build_dataloader(
+        dataset,
+        samples_per_gpu=samples_per_gpu,
+        workers_per_gpu=cfg.data.workers_per_gpu,
+        dist=distributed,
+        shuffle=False)
+
+    # build the model and load checkpoint
+    cfg.model.train_cfg = None
+    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))
+    fp16_cfg = cfg.get('fp16', None)
+    if fp16_cfg is not None:
+        wrap_fp16_model(model)
+    # checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+    # if args.fuse_conv_bn:
+    #     model = fuse_conv_bn(model)
+    # # old versions did not save class info in checkpoints, this walkaround is
+    # # for backward compatibility
+    # if 'CLASSES' in checkpoint.get('meta', {}):
+    #     model.CLASSES = checkpoint['meta']['CLASSES']
+    # else:
+    #     model.CLASSES = dataset.CLASSES
+
+    # if not distributed:
+    #     model = MMDataParallel(model, device_ids=[0])
+    #     outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)
+    # else:
+    #     model = MMDistributedDataParallel(
+    #         model.cuda(),
+    #         device_ids=[torch.cuda.current_device()],
+    #         broadcast_buffers=False)
+    #     outputs = multi_gpu_test(model, data_loader, args.tmpdir,
+    #                              args.gpu_collect)
+    
+    # 读取零散预测结果
+    output_dir = args.scattered_result_dir
+    outputs = []
+    for i in range(len(dataset)):
+        output_path = os.path.join(output_dir, str(i)+'.pkl')
+        output = mmcv.load(output_path)
+        outputs.append(output[0])
+
+    rank, _ = get_dist_info()
+    if rank == 0:
+        if args.out:
+            print(f'\nwriting results to {args.out}')
+            mmcv.dump(outputs, args.out)
+        kwargs = {} if args.eval_options is None else args.eval_options
+        if args.format_only:
+            dataset.format_results(outputs, **kwargs)
+        if args.eval:
+            eval_kwargs = cfg.get('evaluation', {}).copy()
+            # hard-code way to remove EvalHook args
+            for key in [
+                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
+                    'rule'
+            ]:
+                eval_kwargs.pop(key, None)
+            eval_kwargs.update(dict(metric=args.eval, **kwargs))
+            print(dataset.evaluate(outputs, **eval_kwargs))
+
+
+if __name__ == '__main__':
+    main()
